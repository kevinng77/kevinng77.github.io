const e=JSON.parse('{"key":"v-b1a6a7ea","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0BO.html","title":"Bayesian Optimization|贝叶斯优化","lang":"zh-CN","frontmatter":{"title":"Bayesian Optimization|贝叶斯优化","date":"2021-02-28T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["Bayesian Optimization","Deep Learning"],"mathjax":true,"toc":true,"comments":null,"description":"贝叶斯优化 Hyperparameters 超参的调整是 Deep Learning 较为耗时的一部分。比起 grid search 耗时耗力的暴力破解与 random search 的若有缘则相见。贝叶斯优化提供了一种相对有方向的搜索方案。 算法大致思路 相关图片","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0BO.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"Bayesian Optimization|贝叶斯优化"}],["meta",{"property":"og:description","content":"贝叶斯优化 Hyperparameters 超参的调整是 Deep Learning 较为耗时的一部分。比起 grid search 耗时耗力的暴力破解与 random search 的若有缘则相见。贝叶斯优化提供了一种相对有方向的搜索方案。 算法大致思路 相关图片"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"Bayesian Optimization"}],["meta",{"property":"article:tag","content":"Deep Learning"}],["meta",{"property":"article:published_time","content":"2021-02-28T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Bayesian Optimization|贝叶斯优化\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2021-02-28T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"算法大致思路","slug":"算法大致思路","link":"#算法大致思路","children":[{"level":3,"title":"SMBO 算法解释","slug":"smbo-算法解释","link":"#smbo-算法解释","children":[]},{"level":3,"title":"迭代中的相关算法","slug":"迭代中的相关算法","link":"#迭代中的相关算法","children":[]},{"level":3,"title":"Acquisition Function","slug":"acquisition-function","link":"#acquisition-function","children":[]}]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{},"readingTime":{"minutes":4.96,"words":1488},"filePathRelative":"posts/notes/articles/笔记BO.md","localizedDate":"2021年2月28日","excerpt":"<h1> 贝叶斯优化</h1>\\n<blockquote>\\n<p>Hyperparameters 超参的调整是 Deep Learning 较为耗时的一部分。比起 grid search 耗时耗力的暴力破解与 random search 的若有缘则相见。贝叶斯优化提供了一种相对有方向的搜索方案。</p>\\n</blockquote>\\n<h2> 算法大致思路</h2>\\n<figure><img src=\\"/assets/img/BO/image-20210228155822610.png\\" alt=\\"相关图片\\" height=\\"300\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>相关图片</figcaption></figure>","autoDesc":true}');export{e as data};
