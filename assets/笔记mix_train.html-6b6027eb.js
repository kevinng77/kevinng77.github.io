const e=JSON.parse('{"key":"v-12e8a556","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0mix_train.html","title":"混合精度训练","lang":"zh-CN","frontmatter":{"title":"混合精度训练","date":"2022-06-18T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP"],"mathjax":true,"description":"混合精度训练，短短的几行代码，在节省显存占用 40%+，训练速度翻倍的前提下，能够做到模型准确率几乎不减少！强烈推荐阅读这篇只有 9 页的文章：MIXED PRECISION TRAINING 。 快速开始 在 AMP （自动混合精度训练）提出的一开始，大家使用的都是 NVIDIA 的 apex 库实现。后来各大深度学习平台纷纷添加了自带的 AMP API，如 tf，torch，paddle 等。","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0mix_train.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"混合精度训练"}],["meta",{"property":"og:description","content":"混合精度训练，短短的几行代码，在节省显存占用 40%+，训练速度翻倍的前提下，能够做到模型准确率几乎不减少！强烈推荐阅读这篇只有 9 页的文章：MIXED PRECISION TRAINING 。 快速开始 在 AMP （自动混合精度训练）提出的一开始，大家使用的都是 NVIDIA 的 apex 库实现。后来各大深度学习平台纷纷添加了自带的 AMP API，如 tf，torch，paddle 等。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-02-22T13:54:51.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:published_time","content":"2022-06-18T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-02-22T13:54:51.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"混合精度训练\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2022-06-18T00:00:00.000Z\\",\\"dateModified\\":\\"2023-02-22T13:54:51.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"快速开始","slug":"快速开始","link":"#快速开始","children":[]},{"level":2,"title":"三个要点","slug":"三个要点","link":"#三个要点","children":[{"level":3,"title":"保存模型的 FP32 主权重副本","slug":"保存模型的-fp32-主权重副本","link":"#保存模型的-fp32-主权重副本","children":[]},{"level":3,"title":"loss-scaling","slug":"loss-scaling","link":"#loss-scaling","children":[]},{"level":3,"title":"改进算数方法","slug":"改进算数方法","link":"#改进算数方法","children":[]}]},{"level":2,"title":"训练要点","slug":"训练要点","link":"#训练要点","children":[{"level":3,"title":"O1 伪代码","slug":"o1-伪代码","link":"#o1-伪代码","children":[]}]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1676542179000,"updatedTime":1677074091000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":2}]},"readingTime":{"minutes":7.24,"words":2171},"filePathRelative":"posts/notes/articles/笔记mix_train.md","localizedDate":"2022年6月18日","excerpt":"<blockquote>\\n<p>混合精度训练，短短的几行代码，在节省显存占用 40%+，训练速度翻倍的前提下，能够做到模型准确率几乎不减少！强烈推荐阅读这篇只有 9 页的文章：<a href=\\"https://arxiv.org/pdf/1710.03740.pdf\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">MIXED PRECISION TRAINING</a> 。</p>\\n</blockquote>\\n<h2> 快速开始</h2>\\n<p>在 AMP （自动混合精度训练）提出的一开始，大家使用的都是 NVIDIA 的 apex 库实现。后来各大深度学习平台纷纷添加了自带的 AMP API，如 tf，torch，paddle 等。</p>","autoDesc":true}');export{e as data};
