import{_ as s,E as e,S as d,W as l,$ as n,a3 as t,Z as i,aS as p}from"./framework-d5c0d2cb.js";const o={},r=n("p",null,"本文对 Text generation inference + exllama 的 LLaMa 量化服务方案进行单卡 4090 部署测试。",-1),c={href:"https://zhuanlan.zhihu.com/p/645732302",target:"_blank",rel:"noopener noreferrer"},u=p(`<p>在上期中我们提到了 TGI 和 vllm 的对比测试，在使用 vllm 和 TGI 对 float16 模型进行部署后，我们能够在单卡 4090 上达到 3.5+ request/秒的吞吐量。</p><p>就在几天前 TGI 优化了 exllama，基于之前量化 LLaMa 的测试，exllama 能在控制精度损失的情况下，将模型的推理速度提升。</p><p>以下参考 TGI 的官方手册对采用 AUTOGPTQ 量化后的 LLaMa v2 gptq4 权重进行部署:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">--rm</span> <span class="token parameter variable">--name</span> tgi <span class="token punctuation">\\</span>
    <span class="token parameter variable">--runtime</span><span class="token operator">=</span>nvidia <span class="token punctuation">\\</span>
    <span class="token parameter variable">--gpus</span> all <span class="token punctuation">\\</span>
    <span class="token parameter variable">-p</span> <span class="token number">5001</span>:5001 <span class="token punctuation">\\</span>
    <span class="token parameter variable">-v</span> /home/kevin/models:/models <span class="token punctuation">\\</span>
    ghcr.io/huggingface/text-generation-inference:1.0.0 <span class="token punctuation">\\</span>
    --model-id /models/llama2-7b-chat-gptq-int4 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--hostname</span> <span class="token number">0.0</span>.0.0 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--port</span> <span class="token number">5001</span> <span class="token punctuation">\\</span>
    --max-concurrent-requests <span class="token number">256</span>  <span class="token punctuation">\\</span>
    <span class="token parameter variable">--quantize</span> gptq <span class="token punctuation">\\</span>
    --trust-remote-code <span class="token punctuation">\\</span>
    --max-batch-total-tokens <span class="token number">30000</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--sharded</span> <span class="token boolean">false</span> <span class="token punctuation">\\</span>
    --max-input-length <span class="token number">1024</span> <span class="token punctuation">\\</span>
    --validation-workers <span class="token number">4</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，<code>llama2-7b-chat-gptq-int4</code> 量化采用 AUTOGPTQ 提供的示例量化代码进行量化，量化数据集选择 wikitext：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># git clone AUTOGPTQ 仓库后进入 \`examples/quantization\` 文件夹</span>
<span class="token comment"># 修改以下 pretrained_model_dir 和 quantized_model_dir 选择用 Llama-2-7b-chat-hf 量化</span>
python basic_usage_wikitext2.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>当然 TGI 和 GPTQ-for-LLaMa 也提供了 llama 量化脚本，但是对 llama v2 进行 GPTQ 量化时，AUTOGPTQ 的损失总是比较小， <strong>量化后的模型输出也更稳定一些</strong> ，原因未知。</p><p>目前 TGI 版本（1.0.0）对本地加载 exllama 模型仍有不少问题，如果遇到了 weight gptq_bits not found 的话，在模型文件夹下加入一个 safetensor，其中储存好 gptq_bits 和 group 就行。具体 exllama 权重加载逻辑可以看 <code>utils.weight.Wight</code>。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> safetensors<span class="token punctuation">.</span>torch <span class="token keyword">import</span> save_file

tensors <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;gptq_bits&quot;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">&quot;gptq_groupsize&quot;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
save_file<span class="token punctuation">(</span>tensors<span class="token punctuation">,</span> <span class="token string">&quot;/home/kevin/models/llama2-7b-chat-gptq-int4/gptq_config.safetensors&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>目前 TGI 中对量化权重的处理方法不是很兼容 AUTOGPTQ 等 GPTQ 采用的数据储存方式，但有几个 PR 中已经在对此优化。</p><h2 id="tgi-exllama-测试" tabindex="-1"><a class="header-anchor" href="#tgi-exllama-测试" aria-hidden="true">#</a> TGI + EXLLAMA 测试</h2><p>部署后，发送单一的请求进行速度测试：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment">###</span>
POST http://127.0.0.1:5001/generate
Content-Type: application/json

<span class="token punctuation">{</span>
    <span class="token string">&quot;inputs&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Once a upon time,&quot;</span>,
    <span class="token string">&quot;parameters&quot;</span>:<span class="token punctuation">{</span><span class="token string">&quot;max_new_tokens&quot;</span>:100,<span class="token string">&quot;tempareture&quot;</span>:0.6<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>发送请求后 853ms 得到预测结果，平均 117.23 tokens/s。比 TGI + AUTOGPTQ （约 80 tokens/s）快。但还是不如 exllama 官方的推理速度（140+ tokens/s）。</p><p>通过 vllm 提供的 server benchmark 文件 <code>benchmark/benchmark_serving.py</code> 进行测试，</p><ul><li>测试数据集：ShareGPT_V3_unfiltered_cleaned_split.json</li><li>num prompt: 100 （随机从 ShareGPT 提供的用户和 GPT 对话数据当中，筛选 100 个问题进行测试）</li><li>默认设备为：单卡 4090 + inter i9-13900K。（采用 3070 测试的数据有标注）</li><li>request 间隔: 每个 request 发送的间隔。</li></ul><table><thead><tr><th></th><th>request 间隔（秒）</th><th>Throughtput (request/s)</th><th>average speed (tokens/s)</th><th>lowest speed (tokens/s)</th></tr></thead><tbody><tr><td>vllm</td><td>1</td><td>0.95</td><td>51</td><td>39.4</td></tr><tr><td>vllm</td><td>0.5</td><td>1.66</td><td>44.96</td><td>29.41</td></tr><tr><td>vllm</td><td>0.25</td><td>2.48</td><td>37.6</td><td>24.05</td></tr><tr><td>vllm</td><td>0.05</td><td>3.24</td><td>26.31</td><td>4.13</td></tr><tr><td>TGI float16</td><td>1</td><td>0.96</td><td>80.15</td><td>40.91</td></tr><tr><td>TGI float16</td><td>0.5</td><td>1.81</td><td>74.62</td><td>32.97</td></tr><tr><td>TGI float16</td><td>0.25</td><td>2.67</td><td>59.36</td><td>22.59</td></tr><tr><td>TGI float16</td><td>0.05</td><td>3.6</td><td>37.39</td><td>4.12</td></tr><tr><td>TGI EXLLAMA</td><td>1</td><td>1.01</td><td>131.87</td><td>70.47</td></tr><tr><td>TGI EXLLAMA</td><td>0.5</td><td>1.86</td><td>97.28</td><td>44.22</td></tr><tr><td>TGI EXLLAMA</td><td>0.25</td><td>2.91</td><td>59.70</td><td>16.66</td></tr><tr><td>TGI EXLLAMA</td><td>0.02</td><td>4.89</td><td>41.41</td><td>14.88</td></tr><tr><td>TGI Exllama (3070)</td><td>1</td><td>0.42</td><td>8.58</td><td>0.72</td></tr><tr><td>TGI Exllama (3070)</td><td>0.25</td><td>0.35</td><td>2.39</td><td>0.16</td></tr><tr><td>TGI Exllama (3070)</td><td>0.02</td><td>0.43</td><td>2.61</td><td>0.19</td></tr></tbody></table><p>TGI + exllama 使得我们能够在一些小显存上部署模型，如 3070 (8GB)。期待 TGI，vllm 等部署服务对量化部署 llm 的持续优化，让私有部署模型有更好的体验。</p>`,18);function m(v,k){const a=e("ExternalLinkIcon");return d(),l("div",null,[r,n("p",null,[t("上期内容："),n("a",c,[t("vllm vs TGI 部署 llama v2 7B 踩坑笔记"),i(a)])]),u])}const h=s(o,[["render",m],["__file","笔记tgi_exllama.html.vue"]]);export{h as default};
