const e=JSON.parse('{"key":"v-244b3d8c","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0ernie_layout.html","title":"ERNIE-layout 笔记","lang":"zh-CN","frontmatter":{"title":"ERNIE-layout 笔记","date":"2022-11-05T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP"],"mathjax":true,"description":"ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding 模型架构 总结：Ernie-layout 整体采用 Transformer Encoder 架构，特点在于： 借鉴了 DeBERTa 的解耦注意力，依靠额外的 Layout-Parser 来设计 position_ids。 同时对文档图片及文档中的文字进行编码，并设计了 4 种图文结合的预训练方式。 需要依靠额外的 OCR 工具来获得图片中的文字内容，及其对应位置信息。","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0ernie_layout.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"ERNIE-layout 笔记"}],["meta",{"property":"og:description","content":"ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding 模型架构 总结：Ernie-layout 整体采用 Transformer Encoder 架构，特点在于： 借鉴了 DeBERTa 的解耦注意力，依靠额外的 Layout-Parser 来设计 position_ids。 同时对文档图片及文档中的文字进行编码，并设计了 4 种图文结合的预训练方式。 需要依靠额外的 OCR 工具来获得图片中的文字内容，及其对应位置信息。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-02-16T10:09:39.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:published_time","content":"2022-11-05T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-02-16T10:09:39.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"ERNIE-layout 笔记\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2022-11-05T00:00:00.000Z\\",\\"dateModified\\":\\"2023-02-16T10:09:39.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"模型架构","slug":"模型架构","link":"#模型架构","children":[{"level":3,"title":"Embedding","slug":"embedding","link":"#embedding","children":[]},{"level":3,"title":"Encoder","slug":"encoder","link":"#encoder","children":[]},{"level":3,"title":"Decoder","slug":"decoder","link":"#decoder","children":[]}]},{"level":2,"title":"训练方式","slug":"训练方式","link":"#训练方式","children":[{"level":3,"title":"预训练","slug":"预训练","link":"#预训练","children":[]}]},{"level":2,"title":"使用案例","slug":"使用案例","link":"#使用案例","children":[{"level":3,"title":"关于 taskflow 使用案例","slug":"关于-taskflow-使用案例","link":"#关于-taskflow-使用案例","children":[]},{"level":3,"title":"心得","slug":"心得","link":"#心得","children":[]}]}],"git":{"createdTime":1676542179000,"updatedTime":1676542179000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":5.32,"words":1596},"filePathRelative":"posts/notes/articles/笔记ernie_layout.md","localizedDate":"2022年11月5日","excerpt":"<p>ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding</p>\\n<h2> 模型架构</h2>\\n<p>总结：Ernie-layout 整体采用 Transformer Encoder 架构，特点在于：</p>\\n<ul>\\n<li>借鉴了 DeBERTa 的解耦注意力，依靠额外的 <a href=\\"https://link.zhihu.com/?target=https%3A//github.com/Layout-Parser/%20layout-parser\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Layout-Parser</a> 来设计 position_ids。</li>\\n<li>同时对文档图片及文档中的文字进行编码，并设计了 4 种图文结合的预训练方式。</li>\\n<li>需要依靠额外的 OCR 工具来获得图片中的文字内容，及其对应位置信息。</li>\\n</ul>","autoDesc":true}');export{e as data};
