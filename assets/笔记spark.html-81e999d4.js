import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as p,o as r,c,a as s,b as a,d as e,e as t,f as l}from"./app-29ce8251.js";const i="/assets/img/spark/image-20220401122356005.png",d={},y=s("p",null,"内容包括：Spark 搭建本地模式，集群模式（StandAlon, YARN）搭建；认识 Spark 框架、运行逻辑；认识 PySpark 下 SparkCore、SparkSQL 操作。",-1),u={href:"https://spark.apache.org/docs/3.1.2/api/python/user_guide/index.html",target:"_blank",rel:"noopener noreferrer"},h=s("h1",{id:"概述",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#概述","aria-hidden":"true"},"#"),a(" 概述")],-1),v={href:"https://zhuanlan.zhihu.com/p/34436165",target:"_blank",rel:"noopener noreferrer"},m=l('<h3 id="spark-与-hadoop" tabindex="-1"><a class="header-anchor" href="#spark-与-hadoop" aria-hidden="true">#</a> <strong>Spark 与 hadoop</strong></h3><p>Spark 仅做计算，而 Hadoop 生态圈不仅有计算（MR）也有存储（HDFS）和资源管理调度（YARN），HDFS 和 YARN 仍是许多大数据体系的核心架构。在计算层面，Spark 相比较 MR（MapReduce）有巨大的性能优势，但至今仍有许多计算工具基于 MR 构架，比如非常成熟的 Hive</p><h3 id="spark-框架模型" tabindex="-1"><a class="header-anchor" href="#spark-框架模型" aria-hidden="true">#</a> spark 框架模型</h3><p><strong>Spark Core：</strong> Spark 的核心，Spark 核心功能均由 Spark Core 模块提供，是 Spark 运行的基础。Spark Core 以 RDD 为数据抽象，提供 Python、Java、Scala、R 语言的 API，可以编程进行海量离线数据批处理计算。</p><p><strong>SparkSQL：</strong> 基于 SparkCore 之上，提供结构化数据的处理模块。SparkSQL 支持以 SQL 语言对数据进行处理，SparkSQL 本身针对离线计算场景。同时基于 SparkSQL，Spark 提供了 StructuredStreaming 模块，可以以 SparkSQL 为基础，进行数据的流式计算。</p><p><strong>SparkStreaming：</strong> 以 SparkCore 为基础，提供数据的流式计算功能。</p><p><strong>MLlib：</strong> 以 SparkCore 为基础，进行机器学习计算，内置了大量的机器学习库和 API 算法等。方便用户以分布式计算的模式进行机器学习计算。</p><p><strong>GraphX：</strong> 以 SparkCore 为基础，进行图计算，提供了大量的图计算 API，方便用于以分布式计算模式进行图计算。</p><h3 id="spark-架构角色" tabindex="-1"><a class="header-anchor" href="#spark-架构角色" aria-hidden="true">#</a> Spark 架构角色</h3><p><strong>资源管理层面：</strong> 管理集群资源：Master 管理单个服务器资源：worker</p><p><strong>任务执行层面：</strong> 管理单个 spark 任务在运行的时候的工作：Driver 单个任务运行时的工作者： Executor</p><h1 id="快速开始" tabindex="-1"><a class="header-anchor" href="#快速开始" aria-hidden="true">#</a> 快速开始</h1>',12),b={href:"https://spark.apache.org/docs/latest/quick-start.html",target:"_blank",rel:"noopener noreferrer"},E=l(`<h2 id="本地-local-模式配置" tabindex="-1"><a class="header-anchor" href="#本地-local-模式配置" aria-hidden="true">#</a> 本地 local 模式配置</h2><p>Local 模式搭建方便，是学习 Spark 入门操作应用的首选模式。Local 模式的本质是启动一个 JVM Process 进程(一个进程里面有多个线程)，执行任务 Task。Local 模式可以使用<code>Local[N]</code> 或 <code>Local[*]</code> 限制模拟 Spark 集群环境的线程数量。N 为线程数，<code>*</code> 为 CPU 最大核心数。 在 local 模式下，全部四种 spark 角色都为 jvm 进程本身，且只能运行一个 Spark 程序。Local 模式可以做到开箱即用：解压从官网下载的 Spark 安装包：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">tar</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-zxvf</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-3.2.0-bin-hadoop3.2.tgz</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-C</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/export/server/</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>配置 Spark 需要调整以下 5 个环境变量：<code>SPARK_HOME</code>,<code>PYSPARK_PYTHON</code>,<code>JAVA_HOME</code>,<code>HADOOP_CONF_DIR</code>,<code>HADOOP_HOME</code></p><h3 id="spark-操作介绍" tabindex="-1"><a class="header-anchor" href="#spark-操作介绍" aria-hidden="true">#</a> Spark 操作介绍</h3><p>配置好后测试环境，分别运行：</p><p><code>bin/pyspark</code> 可以提供一个 <code>交互式</code>的 Python 解释器环境, 在这里面可以写普通 python 代码。添加 <code>--master local[*]</code> 参数控制使用线程数量。</p><p><code>bin/spark-shell</code> 提供交互式解析器环境，运行 scala 程序代码。</p><p><code>bin/spark-submit</code> 提交指定的 Spark 代码到 Spark 环境中运行。如 <code>bin/spark-submit /export/server/spark/examples/src/main/python/pi.py 10</code></p><h3 id="spark-端口" tabindex="-1"><a class="header-anchor" href="#spark-端口" aria-hidden="true">#</a> Spark 端口</h3><p><strong>4040:</strong> 一个运行的 Application 在运行的过程中临时绑定的端口，用以查看当前任务的状态，当前程序运行完成后,，4040 就会被注销。4040 被占用会顺延到 4041.4042 等</p><p><strong>8080</strong> : 默认是 StandAlone 下, Master 角色(进程)的 WEB 端口,用以查看当前 Master(集群)的状态</p><p><strong>18080/9870</strong> : 默认是历史服务器的端口, 回看某个程序的运行状态就可以通过历史服务器查看,历史服务器长期稳定运行,可供随时查看被记录的程序的运行过程。</p><h2 id="pyspark" tabindex="-1"><a class="header-anchor" href="#pyspark" aria-hidden="true">#</a> PySpark</h2>`,14),g={href:"https://spark.apache.org/docs/3.1.2/rdd-programming-guide.html",target:"_blank",rel:"noopener noreferrer"},k=s("code",null,"pip install pyspark",-1),D=l(`<h3 id="初体验-wordcount-实例" tabindex="-1"><a class="header-anchor" href="#初体验-wordcount-实例" aria-hidden="true">#</a> 初体验：WordCount 实例</h3><p><strong>初始化</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">conf </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> SparkConf().setAppName(appName).setMaster(master)</span></span>
<span class="line"><span style="color:#24292E;">sc </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> SparkContext(</span><span style="color:#E36209;">conf</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">conf)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>local 模式的话 <code>master=&#39;local[n]&#39;</code></p><p><strong>读取数据到 RDD 对象：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">wordsRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.textFile(</span><span style="color:#032F62;">&quot;hdfs://node1:8020/input/words.txt&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#6A737D;"># 本地文件读取：file:///home/data/word.txt</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>RDD 对象处理</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">flatMapRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> wordsRDD.flatMap(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> line: line.split(</span><span style="color:#032F62;">&quot; &quot;</span><span style="color:#24292E;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">mapRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> flatMapRDD.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: (x, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 以上步骤将在不同的集群节点上执行，参考下图。因此可以并行执行，加快效率。</span></span>
<span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> mapRDD.reduceByKey(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> a, b: a </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> b)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">res_rdd_col2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> resultRDD.collect()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+i+`" alt="image-20220401122356005" tabindex="0" loading="lazy"><figcaption>image-20220401122356005</figcaption></figure><p>将结果写入文件并通过 spark 储存于 HDFS：<code>resultRDD.saveAsTextFile(&quot;hdfs://node1:8020/words_new.txt&quot;)</code></p><p>提交代码到集群运行，需要修改初始化方式：<code>conf = SparkConf().setAppName(appName)</code></p><p>而后通过 <code>bin/spark-submit</code> 提交到集群运行：通过 <code>--py-files</code> 提交依赖文件，可以单个文件 <code>.py</code> 或者多个文件 <code>.zip</code>。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">/export/server/spark/bin/spark-submit</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">yarn</span><span style="color:#24292E;"> </span><span style="color:#032F62;">./test.py</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>该 python 代码底层由 java 实现（通过 py4j 交互)</p><h2 id="spark-core-api" tabindex="-1"><a class="header-anchor" href="#spark-core-api" aria-hidden="true">#</a> Spark Core API</h2><p>分布式框架中，需要由同意的数据结构对象来实现分布式计算所需功能，这个对象就是 <strong>RDD（Resilient Distributed Dataset）</strong> 。在初体验中 <code>wordsRDD = sc.textFile(&quot;hdfs://node1:8020/input/words.txt&quot;)</code> 读取的就是 RDD 对象。RDD 是通过 java 实现的抽象类、泛型类型。特征包括：</p><ul><li><strong>有分区（RDD is a list of partitions）</strong></li></ul><p>RDD 的分区是 RDD 数据存储的最小单位，如：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">7</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">8</span><span style="color:#24292E;">], </span><span style="color:#E36209;">numSlices</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">3</span><span style="color:#24292E;">).glom().collect()</span></span>
<span class="line"><span style="color:#6A737D;"># [[1, 2], [3, 4], [5, 6, 7, 8]] </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>分区为 3 时，RDD 内数据分为 3 份。</p><ul><li><strong>可并行计算，计算方法会作用到每一个分区上</strong></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">7</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">8</span><span style="color:#24292E;">], </span><span style="color:#E36209;">numSlices</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">3</span><span style="color:#24292E;">).map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: x</span><span style="color:#D73A49;">*</span><span style="color:#005CC5;">10</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">   </span><span style="color:#005CC5;">print</span><span style="color:#24292E;">(resultRDD.glom().collect())</span></span>
<span class="line"><span style="color:#6A737D;"># [[10, 20], [30, 40], [50, 60, 70, 80]]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>RDD 之间相互依赖、迭代的；</strong></li></ul><p>针对快速探索中的 wordcount 案例，每个操作步骤都是一次对 RDD 的迭代。一个新的 RDD 由上一步骤的 RDD 计算得来。</p><ul><li><strong>KV 型 RDD 可以由分区器；</strong></li></ul><p>对 key-value 型的 RDD，默认采用 Hash 分区，可以手动使用 <code>rdd.partitionBy</code> 设置分区。</p><ul><li><strong>分区的计算会尽量选择靠近数据所在地</strong></li></ul><p>目的是最大化性能，毕竟本地读取效率是大于网络读取的。</p><h3 id="创建-rdd" tabindex="-1"><a class="header-anchor" href="#创建-rdd" aria-hidden="true">#</a> 创建 RDD</h3><p>可以从文件加载：<code>sc.textFile(&quot;hdfs://node1:8020/input/words.txt&quot;)</code>，或者从本地的集合对象（如 list）创建：<code>sc.parallelize(set(), numSlices=3)</code>，从文件夹加载：<code>sc.wholeTextFiles(&quot;hdfs://node1:8020/input&quot;)</code></p><h3 id="rdd-算子" tabindex="-1"><a class="header-anchor" href="#rdd-算子" aria-hidden="true">#</a> RDD 算子</h3>`,31),f={href:"https://spark.apache.org/docs/3.1.2/rdd-programming-guide.html#",target:"_blank",rel:"noopener noreferrer"},C=l(`<h4 id="transformation-算子" tabindex="-1"><a class="header-anchor" href="#transformation-算子" aria-hidden="true">#</a> <strong>Transformation 算子</strong></h4><p>返回一个 RDD，如果没有 action 算子，那么这类算子是不工作的。</p><p><code>flatMap()</code>：对 rdd 进行 map 后接触嵌套。如：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#032F62;">&quot;hello hello&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;nihao nihao &quot;</span><span style="color:#24292E;">])</span></span>
<span class="line"><span style="color:#24292E;">resultRDD.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:x.split()).collect()</span></span>
<span class="line"><span style="color:#6A737D;"># [[&#39;hello&#39;, &#39;hello&#39;], [&#39;nihao&#39;, &#39;nihao&#39;]]</span></span>
<span class="line"><span style="color:#24292E;">mappedRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> resultRDD.flatMap(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:x.split())</span></span>
<span class="line"><span style="color:#6A737D;"># [&#39;hello&#39;, &#39;hello&#39;, &#39;nihao&#39;, &#39;nihao&#39;]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>reduceByKey()</code>：真能对 KV 型 RDD，对组内数据根据 key 分类并聚合。如：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 接 flatmap() 例子</span></span>
<span class="line"><span style="color:#24292E;">mappedRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> mappedRDD.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: (x, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">))</span></span>
<span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> mappedRDD.reduceByKey(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> a, b: a </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> b)</span></span>
<span class="line"><span style="color:#6A737D;"># [(&#39;nihao&#39;, 2), (&#39;hello&#39;, 2)]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>groupBy()</code>：将元素分组，分组后每个组是一个 KV，key 为分组 <code>func</code> 的返回值，value 为该组元素构成的迭代器。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">dataRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">7</span><span style="color:#24292E;">])</span></span>
<span class="line"><span style="color:#24292E;">mappedRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dataRDD.groupBy(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: </span><span style="color:#032F62;">&quot;even&quot;</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> x </span><span style="color:#D73A49;">%</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">else</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;odd&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> mappedRDD.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:(x[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">list</span><span style="color:#24292E;">(x[</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">])))</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(resultRDD.collect())</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>Filter(func)</code>：过滤元素，传入的函数方法需要返回布尔值。</p><p><code>distinct()</code>：数据去重，一般不需要传入参数。</p><p><code>union()</code>：合并两个 RDD 并返回，支持不同各类型。<code>union_rdd = rdd1.union(rdd2)</code></p><p><code>join()</code>：同 sql 的 join。支持左右拼接，<code>leftOuterJoin()</code>, <code>rightOuterJoin()</code></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">dataRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([(</span><span style="color:#032F62;">&quot;hello&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">), (</span><span style="color:#032F62;">&quot;good&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">)])</span></span>
<span class="line"><span style="color:#24292E;">dataRDD2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([(</span><span style="color:#032F62;">&quot;hello&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">10</span><span style="color:#24292E;">), (</span><span style="color:#032F62;">&quot;good&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">20</span><span style="color:#24292E;">), (</span><span style="color:#032F62;">&quot;world&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">30</span><span style="color:#24292E;">)])</span></span>
<span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dataRDD.join(dataRDD2)</span></span>
<span class="line"><span style="color:#6A737D;"># [(&#39;good&#39;, (2, 20)), (&#39;hello&#39;, (1, 10))]</span></span>
<span class="line"><span style="color:#6A737D;"># rightOuterJoin 后：[(&#39;good&#39;, (2, 20)), (&#39;hello&#39;, (1, 10)), (&#39;world&#39;, (None, 30))]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>intersection()</code>：返回交集<code>rdd1.intersection(rdd2)</code></p><p><code>glom()</code> ：将 RDD 数据根据分区进行嵌套。</p><p><code>groupByKey()</code>：针对 KV 型 RDD，根据 key 分组，但不聚合，类似 <code>grouBy()</code>。使用<code>groupByKey()</code> + 聚合的性能是远差与直接使用 <code>reduceByKey()</code> 的。</p><p><code>sortByKey()</code> 或 <code>sortBy(func, ascending=False, numPartitions=1)</code>：排序 <code>func</code> 为排序依据。 <strong>如果要全局有序，排序分区数需要设置为 1。</strong></p><p><code>mapPartition(func)</code>：<code>func</code> 输入可迭代对象，输出可迭代对象。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">func</span><span style="color:#24292E;">(iter_tool):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> x </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> iter_tool:</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">yield</span><span style="color:#24292E;"> x</span><span style="color:#D73A49;">+</span><span style="color:#005CC5;">1</span></span>
<span class="line"><span style="color:#24292E;">rdd1 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(rdd1.mapPartitions(func).glom().collect())</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>partitionBy(func)</code>：<code>func</code> 参数应该为元素 hash 值到分区编号的映射。只能根据 hash 分区，因此非 KV 型 RDD 需要先进行转换。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">rdd1 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">7</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">8</span><span style="color:#24292E;">]).map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:(x,x))</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(rdd1.partitionBy(</span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: x </span><span style="color:#D73A49;">%</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">).glom().collect())</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><code>repartition(N)</code>：将 RDD 重新分为 N 个分区，一般除了要全局排序外，不会进行充分区。</p><p><code>coalesce()</code>：对分区数量进行加减，比<code>repartition()</code>常用，<code>rdd.coalesce(4,shuffle=True)</code> 如果 <code>shuffle</code> 为 <code>Flase</code>，那么将忽略分区增加操作，仅支持分区减少。 <strong>尽量不要增加分区，可能破坏内存迭代的计算管道。</strong></p><p><code>mapValues(func)</code>：仅针对 KV 型 RDD，功能等价于 <code>.map(lambda x:(x[0],func(x[1])))</code></p><h4 id="action-算子" tabindex="-1"><a class="header-anchor" href="#action-算子" aria-hidden="true">#</a> <strong>Action 算子</strong></h4><p><code>countByKey()</code>：返回一个 <code>collections.defaultdict</code></p><p><code>collect()</code>：统一各分区的数据，形成一个<code>List</code>。 <strong>谨慎使用：结果数据集太大的话，Driver 内存会爆炸。</strong></p><p><code>reduce(func)</code>：迭代地减小数据维度：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">]).reduce(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> a, b: a </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> b)</span></span>
<span class="line"><span style="color:#6A737D;"># 返回 15</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><code>fold()</code>：分别对各分区进行 reduce，然后聚合。reduce 是有初始值的：如：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">rdd1 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(rdd1.glom().collect())</span></span>
<span class="line"><span style="color:#6A737D;"># [[1, 2], [3, 4], [5, 6]]</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(rdd1.fold(</span><span style="color:#005CC5;">10</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> a, b: a </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> b))  </span></span>
<span class="line"><span style="color:#6A737D;"># (1+2+10) + (3+4+10)+(5+6+10) = 61</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>first()</code>：返回第一个元素。</p><p><code>take(N)</code>：返回前 N 个元素，<code>List</code> 形式储存。</p><p><code>top(N)</code>：返回降序排序后的前 N 个值，<code>List</code> 储存。</p><p><code>count()</code>：返回 RDD 数据数量，<code>int</code>。</p><p><code>takeSample(withReplacement=True, num=10, seed=7))</code>：随机抽取 n 个样本。</p><p><code>takeOrdered(num, key=func)</code>：返回 <code>List</code> 为根据 <code>func()</code> 排序后的前 <code>num</code> 个元素</p><p><code>foreach(func)</code>：迭代对所有元素进行处理。该执行不经过 Driver。</p><p><code>saveAsTextFile()</code>：将 RDD 写入文本文件中。由 Executor 直接执行，执行结果不会发送到 Driver。</p><p><code>foreachPartition()</code>：与 <code>foreach()</code> 类似，但调用一次函数处理一个分区的数据。</p><h3 id="rdd-持久化" tabindex="-1"><a class="header-anchor" href="#rdd-持久化" aria-hidden="true">#</a> RDD 持久化</h3>`,41),A=s("code",null,"persist()",-1),S=s("code",null,"cache()",-1),x=s("code",null,"persist(StorageLevel.MEMORY_AND_DISK)",-1),F={href:"https://spark.apache.org/docs/3.1.2/rdd-programming-guide.html#rdd-persistence",target:"_blank",rel:"noopener noreferrer"},q=s("code",null,"rdd.unpersist()",-1),R=s("strong",null,"缓存并不是安全的，内存不足/断电/硬盘损坏等都可能造成数据出错。",-1),_=l(`<p>RDD 的 CheckPoint 被设计认为是安全的（不排除物理因素破坏数据），仅支持硬盘储存。checkpoint 可以选择数据备份地址，因此可以存储在 HDFS 中；性能比缓存更好；不管分区多少，风险是一样的（不同于缓存）。</p><p>配置 checkpoint 目标地址：<code>sc.setCheckpointDir(&quot;hdfs://node1:8020/output/bj52kp&quot;)</code>；保存时直接调用：<code>rdd.checkpoint()</code></p><p><strong>缓存与 checkpoint 都不是 action 操作，所以后面要加上 Action。</strong></p><h3 id="共享变量" tabindex="-1"><a class="header-anchor" href="#共享变量" aria-hidden="true">#</a> 共享变量</h3><p>每个 excutor 进程可以处理多个分区。当 excutor 处理 RDD 时需要某些 python 变量（RDD 外的 python 变量都由 Driver 处理与储存。），Driver 需要为每个分区发送一份数据。因此每个 excutor 中就有多个相同数据，造成额外网络 IO 开销与内存浪费。</p><p><strong>广播变量：</strong> 将 python 变量数据 <code>python_val</code> 标记为广播变量：<code>broadcase = sc.broadcast(python_val)</code>。使用时：<code>broadcast.value</code> 提取数据。</p><p><strong>累加器：</strong> 需求：当不同分区运行时，他们需要对同一个全局变量进行操作。python 的 <code>global</code> 无法满足需求，由于机器不同，无法通过指针等实现。需要使用 spark 提供的累加器。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">acmlt </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.accumulator(</span><span style="color:#005CC5;">66</span><span style="color:#24292E;">)  </span><span style="color:#6A737D;"># 初始化值为 acmlt=66</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">map_func</span><span style="color:#24292E;">(data):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">global</span><span style="color:#24292E;"> acmlt</span></span>
<span class="line"><span style="color:#24292E;">    acmlt </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>累加器需注意：</strong> 可能需要加缓存来解决以下问题。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">rdd </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">2</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">3</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">4</span><span style="color:#24292E;">],</span><span style="color:#005CC5;">2</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">acmlt </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.accumulator(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">rdd2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> rdd.map(map_func)</span></span>
<span class="line"><span style="color:#24292E;">rdd2.collect()  </span><span style="color:#6A737D;"># rdd2 action 后不保存数据。此时 acmlt 为 4</span></span>
<span class="line"><span style="color:#24292E;">rdd3 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> rdd2.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:x)  </span><span style="color:#6A737D;"># rdd3 构造需要再次构造 rdd2，此时 acmlt 为 8.</span></span>
<span class="line"><span style="color:#24292E;">rdd3.collect() </span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(accmlt)  </span><span style="color:#6A737D;"># 8</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="内核调度" tabindex="-1"><a class="header-anchor" href="#内核调度" aria-hidden="true">#</a> 内核调度</h3><p>根据 RDD 的迭代特性，程序的整个计算路程可以通过 <strong>DAG</strong> 有向无环图表示。每个 Action 操作的执行都会触发该一个 JOB 来计算 Action 之前的子图。<code>1 个 action=1 个有向无环图=1 个 JOB</code>。如果有 3 个 action，那么代码就产生 3 个 JOB，这些 JOB，Spark 称为 Application。</p><p>DAG 中 RDD 节点之间的关系分为： <strong>窄依赖</strong> ：父 RDD 的一个分区，全部将数据发给子 RDD 的一个分区。 <strong>宽依赖/shuffle</strong> ：父 RDD 的一个分区，将数据发给子 RDD 的多个分区。</p><p><strong>stage 根据宽依赖进行划分</strong> ，因此每个 stage 内部都是窄依赖。</p><p>每个 task 是一个线程（DAG 上呈现为，一个 stage 中的一条来连通的计算流程线），线程内是纯内存计算，所有线程并行计算，并行程度受全局并行度 <code>spark.default.parallelism</code>、分区数量影响。 <strong>spark 一般不要再算子上设置并行度，除了部分排序算子，分区数量让系统自动设置即可</strong> 。</p><p>集群中，并行度可以设置为 CPU 总核心的 2 到 10 倍。<code>spark.default.parallelism=1000</code>。</p><p>基于以上分析，spark 程序的调度流程为：</p><ul><li>构建 Driver</li><li>构建 SparkContext</li><li>产生 DAG，基于 <strong>DAG Scheduler</strong> 构建逻辑 task 分配</li><li>基于 <strong>Task Scheduler</strong> ，将 Task 分配到 executor 上工作并监督他们，executor 工作时汇报进度。</li></ul><p>其中 DAG Scheduler 与 Task Scheduler 为 Driver 内部的两个组件。</p><h3 id="总结-spark-层级" tabindex="-1"><a class="header-anchor" href="#总结-spark-层级" aria-hidden="true">#</a> 总结 Spark 层级</h3><p>一个 Spark Application （如 bin/pyspark）中，包含多个 Job，每个 Job 由多个 Stage 组成，每个 Job 执行按照 DAG 图进行。每个 Stage 中包含多个 Task 任务，每个 Task 以线程 Thread 方式执行，需要 1Core CPU。</p><ul><li><strong>Job：</strong> 由多个 Task 的并行计算部分，一般 Spark 中的 action 操作（如 save、collect），会生成一个 Job。</li><li><strong>Stage：</strong> Job 的组成单位，一个 Job 会切分成多个 Stage，Stage 彼此之间相互依赖顺序执行，而每个 Stage 是多个 Task 的集合，类似 map 和 reduce stage。</li><li><strong>Task：</strong> 被分配到各个 Executor 的单位工作内容，它是 Spark 中的最小执行单位，一般来说有多少个 Paritition（物理层面的概念，即分支可以理解为将数据划分成不同部分并行处理），就会有多少个 Task，每个 Task 只会处理单一分支上的数据。</li></ul><p>在 18080 中选择 刚刚提交的 pi 计算任务。选择 Executors 查看。从 页面结果可以看出，Spark Application 运行到集群上时，由两部分组成：Driver Program 和 Executors</p><h3 id="spark-shuffle" tabindex="-1"><a class="header-anchor" href="#spark-shuffle" aria-hidden="true">#</a> Spark Shuffle</h3><p>DAG 将一个 Job 划分为多个 Stage，若用 map 或 reduce 来标注每个 Stage，Spark Shuffle 的作用时将 map 的输出对应到 reduce 上。shuffle 分为 shuffle write（map 的最后一步） 与 shuffle read（reduce 的第一步）。因此数据流大致为 <code> stage1 - partition - stage2</code>。</p><h2 id="sparksql" tabindex="-1"><a class="header-anchor" href="#sparksql" aria-hidden="true">#</a> SparkSQL</h2><p>用于存储海量结构化数据。支持 SQL，HIVE 等。SparkSQL 与 HIVE 都为分布式 SQL 计算引擎，SparkSQL 具有更好的性能。SparkSQL 中共有 DataSet、DataFrame 对象。Python 仅支持 DataFrame 对象，即一助攻二维表结构数据。</p><h3 id="快速体验" tabindex="-1"><a class="header-anchor" href="#快速体验" aria-hidden="true">#</a> 快速体验</h3><p>Spark 2.0 后，推出了 SparkSession 统一编码入口对象，支持 RDD 编程与 SparkSQL 编程。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> pyspark.sql </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> SparkSession</span></span>
<span class="line"><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__name__</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&#39;__main__&#39;</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">    spark </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> SparkSession.builder. \\</span></span>
<span class="line"><span style="color:#24292E;">        appName(</span><span style="color:#032F62;">&quot;local[*]&quot;</span><span style="color:#24292E;">). \\</span></span>
<span class="line"><span style="color:#24292E;">        config(</span><span style="color:#032F62;">&quot;spark.sql.shuffle.partitions&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;4&quot;</span><span style="color:#24292E;">). \\</span></span>
<span class="line"><span style="color:#24292E;">        getOrCreate()</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># appName 设置程序名称, config 设置一些常用属性</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 最后通过 getOrCreate()方法创建 SparkSession 对象</span></span>
<span class="line"><span style="color:#24292E;">    df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.read.csv(</span><span style="color:#032F62;">&#39;file:///home/data/sql/stu_score.txt&#39;</span><span style="color:#24292E;">, </span><span style="color:#E36209;">sep</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&#39;,&#39;</span><span style="color:#24292E;">, </span><span style="color:#E36209;">header</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">    df2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> df.toDF(</span><span style="color:#032F62;">&#39;id&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;name&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;score&#39;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">    df2.printSchema()</span></span>
<span class="line"><span style="color:#24292E;">    df2.show()</span></span>
<span class="line"><span style="color:#24292E;">    df2.createTempView(</span><span style="color:#032F62;">&quot;score&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    spark.sql(</span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">    SELECT * FROM score WHERE name=&#39;语文&#39; LIMIT 5</span></span>
<span class="line"><span style="color:#032F62;">    &quot;&quot;&quot;</span><span style="color:#24292E;">).show()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="dataframe" tabindex="-1"><a class="header-anchor" href="#dataframe" aria-hidden="true">#</a> DataFrame</h3><p>DataFrame 为二维表结构，其中储存四个对象：</p><p><strong>StructType</strong> ：整个表结构的信息 <strong>StructField</strong> ：描述列的信息 <strong>Row</strong> ：行数据 <strong>Column</strong> ：列数据以及列的信息</p><h4 id="创建-dataframe" tabindex="-1"><a class="header-anchor" href="#创建-dataframe" aria-hidden="true">#</a> 创建 DataFrame</h4><p>从 RDD 创建，数据类型根据 RDD 推断。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sc </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.sparkContext</span></span>
<span class="line"><span style="color:#24292E;">rdd </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.textFile(</span><span style="color:#032F62;">&quot;../data/sql/people.txt&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#005CC5;">map</span><span style="color:#24292E;">(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: x.split(</span><span style="color:#032F62;">&#39;,&#39;</span><span style="color:#24292E;">)).\\</span></span>
<span class="line"><span style="color:#005CC5;">map</span><span style="color:#24292E;">(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: [x[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">int</span><span style="color:#24292E;">(x[</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">])])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.createDataFrame(rdd, </span><span style="color:#E36209;">schema</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> [</span><span style="color:#032F62;">&#39;name&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;age&#39;</span><span style="color:#24292E;">])</span></span>
<span class="line"><span style="color:#6A737D;"># 或 df = rdd.toDF([&#39;name&#39;, &#39;age&#39;])</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中的 <code>schema</code> 参数可以通过 <code>StructType</code> 定义：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> pyspark.sql.types </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> StructType, StringType, IntegerType</span></span>
<span class="line"><span style="color:#24292E;">schema </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> StructType().\\</span></span>
<span class="line"><span style="color:#24292E;">add(</span><span style="color:#032F62;">&quot;id&quot;</span><span style="color:#24292E;">, IntegerType(), </span><span style="color:#E36209;">nullable</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">add(</span><span style="color:#032F62;">&quot;name&quot;</span><span style="color:#24292E;">, StringType(), </span><span style="color:#E36209;">nullable</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">add(</span><span style="color:#032F62;">&quot;score&quot;</span><span style="color:#24292E;">, IntegerType(), </span><span style="color:#E36209;">nullable</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>从 <code>pd.DataFrame</code> 创建：直接使用 <code>spark_df = spark.createDataFrame(p_df)</code></p><p>从外部文件读取：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">schema </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> StructType().add(</span><span style="color:#032F62;">&quot;data&quot;</span><span style="color:#24292E;">, StringType(), </span><span style="color:#E36209;">nullable</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.read.format(</span><span style="color:#032F62;">&quot;text&quot;</span><span style="color:#24292E;">)\\</span></span>
<span class="line"><span style="color:#24292E;">.schema(schema)\\</span></span>
<span class="line"><span style="color:#24292E;">.load(</span><span style="color:#032F62;">&quot;../data/sql/people.txt&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>一般读取的时 <code>json</code>、<code>parquet</code> 类型的话，不需要提供<code>schema</code>。</p><p>对于 CSV 等，可能需要提供 <code>option</code> 参数。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.read.format(</span><span style="color:#032F62;">&quot;csv&quot;</span><span style="color:#24292E;">)\\</span></span>
<span class="line"><span style="color:#24292E;">.option(</span><span style="color:#032F62;">&quot;sep&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;;&quot;</span><span style="color:#24292E;">)\\</span></span>
<span class="line"><span style="color:#24292E;">.option(</span><span style="color:#032F62;">&quot;header&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)\\</span></span>
<span class="line"><span style="color:#24292E;">.option(</span><span style="color:#032F62;">&quot;encoding&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;utf-8&quot;</span><span style="color:#24292E;">)\\</span></span>
<span class="line"><span style="color:#24292E;">.schema(</span><span style="color:#032F62;">&quot;name STRING, age INT, job STRING&quot;</span><span style="color:#24292E;">)\\</span></span>
<span class="line"><span style="color:#24292E;">.load(</span><span style="color:#032F62;">&quot;../data/sql/people.csv&quot;</span><span style="color:#24292E;">)</span><span style="color:#6A737D;">#</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="dataframe-操作" tabindex="-1"><a class="header-anchor" href="#dataframe-操作" aria-hidden="true">#</a> DataFrame 操作</h4>`,45),O=s("code",null,"sparksession.sql()",-1),T=s("code",null,'df.createTempView("temp_name")',-1),N=s("code",null,"sparksession.sql()",-1),M={href:"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame",target:"_blank",rel:"noopener noreferrer"},w=l(`<p><code>.show(n)</code>，<code>.printSchema()</code>：查看数据信息。</p><p><code>.select()</code>：选择指定的列，传入列名或 Column 对象。</p><p><code>.filter(condition)</code>，<code>.where()</code>：筛选行，条件如：布尔值的列数据<code>df[&#39;score&#39;]&lt;9</code> 或 SQL 风格<code>&#39;score&#39;&lt;9</code></p><p><code>.groupBy()</code>：按列分组，传入列名或列对象。返回 <code>GroupedData</code> 对象</p><p><code>from pyspark.sql import functions as F</code> 提供了基础表数据计算功能，如 <code>F.round()</code>, <code>F.avg()</code>, <code>F.min()</code>, <code>F.count</code> 等来直接对二维数据进行计算。</p><p><code>.dropDuplicates()</code>：默认对整体去重，传入需要去重的列。</p><p><code>.dropna(thresh, subset)</code>：针对 <code>subset</code> 给出的列，有效信息至少为 <code>thresh</code> 个才保留该行数据。</p><p><code>.fillna()</code>： 传入一个填充规则，表示每个列的填充值<code>{&#39;name&#39;:&quot;unk&quot;,&quot;job&quot;:0}</code></p><h4 id="读写数据" tabindex="-1"><a class="header-anchor" href="#读写数据" aria-hidden="true">#</a> 读写数据</h4><p>写出为文件：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">df.write.mode(</span><span style="color:#032F62;">&quot;overwrite&quot;</span><span style="color:#24292E;">).format(</span><span style="color:#032F62;">&quot;csv&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;sep&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;,&quot;</span><span style="color:#24292E;">).option(</span><span style="color:#032F62;">&quot;header&quot;</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">save(</span><span style="color:#032F62;">&quot;../mydata.csv&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>对于 <code>json</code>格式直接写出，不需要 <code>option</code>，默认的文件写出格式为 <code>parquet</code>。</p><p><strong>使用 JDBC 读写数据库</strong> ，需要驱动<code>mysql-connector-java-5.1.41-bin.jar</code> 不同 mysql 对应版本不同。jar 包存放地址：<code>py 解析器环境/lib/python3.8/site-packages/pyspark/jars</code></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">df.write.mode(</span><span style="color:#032F62;">&quot;overwrite&quot;</span><span style="color:#24292E;">).format(</span><span style="color:#032F62;">&quot;jdbc&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;url&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;jdbc:mysql://node1:3306/databese?useSSL=false&amp;useUnicode=true&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;dbtable&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;u_data&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;user&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;root&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;password&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;123456&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">save()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>jdbc</code> 连接字符串中，建议使用 <code>useSSL=false</code> 保证正常连接，<code>useUnicode=true</code> 保证传输无乱码。<code>dbtable</code> 为写出的表名。</p><p><strong>使用 JDBC 读取数据库：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> read.format(</span><span style="color:#032F62;">&quot;jdbc&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;url&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;jdbc:mysql://node1:3306/databese?useSSL=false&amp;useUnicode=true&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;dbtable&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;u_data&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;user&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;root&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;password&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;123456&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">load()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="sparksql-定义-udf" tabindex="-1"><a class="header-anchor" href="#sparksql-定义-udf" aria-hidden="true">#</a> SparkSQL 定义 UDF</h3><p>User-Define-Function，对 python 函数进行注册，返回可用于 DSL 的函数操作，注册的函数名称可用于 SQL 风格。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">udf </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.udf.register(</span><span style="color:#032F62;">&quot;udf1&quot;</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: x </span><span style="color:#D73A49;">&lt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">10</span><span style="color:#24292E;">, BooleanType())</span></span>
<span class="line"><span style="color:#24292E;">df.filter(udf(df[</span><span style="color:#032F62;">&#39;age&#39;</span><span style="color:#24292E;">])).show()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>对于传递的返回类型参数，需要从 <code>pyspark.sql.types</code> 中选取，数组类型可用 <code>ArrayType(StringType())</code>（即 python 中的 <code>List(Str)</code>）；字典类型使用 <code>StructType()</code>，此处需要提前声明 <code>StructTrype()</code> 中的结构体信息。</p><h3 id="窗口函数" tabindex="-1"><a class="header-anchor" href="#窗口函数" aria-hidden="true">#</a> 窗口函数</h3><p>可以将聚合前与聚合后的数据显示在同一个表中。</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;"># 在 </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> 数据后追加一列窗口，代表学生的平均成绩</span></span>
<span class="line"><span style="color:#005CC5;">spark</span><span style="color:#24292E;">.</span><span style="color:#005CC5;">sql</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">          SELECT *, AVG(SCORE) OVER() AS avg_score FROM stu</span></span>
<span class="line"><span style="color:#032F62;">          &quot;&quot;&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">          </span></span>
<span class="line"><span style="color:#24292E;"># 聚合类型 SUM</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">MIN</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">MAX</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">AVG</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">COUNT</span></span>
<span class="line"><span style="color:#005CC5;">SUM</span><span style="color:#24292E;">() </span><span style="color:#D73A49;">OVER</span><span style="color:#24292E;">([PARTITION BY XX][ORDER BY XX])</span></span>
<span class="line"><span style="color:#24292E;"># 排序类型 RANK</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">ROW_NUMBER</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">DENSE_RANK</span></span>
<span class="line"><span style="color:#005CC5;">RANK</span><span style="color:#24292E;">() </span><span style="color:#D73A49;">OVER</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;"># 分区类型 NTILE</span></span>
<span class="line"><span style="color:#005CC5;">NTILE</span><span style="color:#24292E;">(</span><span style="color:#D73A49;">number</span><span style="color:#24292E;">) </span><span style="color:#D73A49;">OVER</span><span style="color:#24292E;">() </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="sql-流程" tabindex="-1"><a class="header-anchor" href="#sql-流程" aria-hidden="true">#</a> SQL 流程</h3><p>SparkSQL 可以自动优化（依赖于 Catalyst 优化器），提升代码运行效率。SparkSQL 接收到 SQL 语句后，通过 Catalyst 解析，并生成 RDD 执行计划，而后交给集群执行。</p><p><strong>Catalyst 优化：</strong></p><p>首先生成 AST 抽象语法树，在 AST 中加入元数据信息（以供优化）。优化方式包括：</p><ul><li><p><strong>Predicate Pushdown</strong> 断言下推：将 Filter 这种可以减小数据集的操作下推，放在 Scan 的位置，减少无用操作量。（如提前执行 where）</p></li><li><p><strong>Column Pruning</strong> 列值裁剪：断言下推后，对无用的列进行裁剪。（如提前规划 select 数量）</p></li><li><p><strong>等等许多优化方案</strong> ，具体可查看 <code>org.apache.spark.sql.catalyst.optimizer.Optimizer</code> 源码</p></li></ul><h3 id="spark-on-hive" tabindex="-1"><a class="header-anchor" href="#spark-on-hive" aria-hidden="true">#</a> Spark on Hive</h3><p>相当于将 HIVE SQL 解释器引擎换成了 SparkSQL 解释器引擎。</p><p>Spark 自身没有元数据管理功能，当 Spark 执行 SQL 风格语句如 <code>SELECT * FROM person</code> 时候，如果没有 person 储存位置，person 包含的字段、字段类型的话，则 SQL 语句将无法被解析并执行。</p><p>SparkSQL 中将这些元数据信息注册在了 DataFrame 中，而数据库的元数据信息，由 Hive 的 MetaStore 来提供管理，Spark 只是提供执行引擎。因此 Spark 能够链接上 Hive 的 MetaStore 就可以了，MetaStore 需要存在并开机，通过配置 <code>hive-site.xml</code> Spark 能知道 MetaStore 的端口号：</p><div class="language-xml line-numbers-mode" data-ext="xml"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">&lt;</span><span style="color:#22863A;">property</span><span style="color:#24292E;">&gt;</span></span>
<span class="line"><span style="color:#24292E;">    &lt;</span><span style="color:#22863A;">name</span><span style="color:#24292E;">&gt;hive.metastore.uris&lt;/</span><span style="color:#22863A;">name</span><span style="color:#24292E;">&gt;</span></span>
<span class="line"><span style="color:#24292E;">    &lt;</span><span style="color:#22863A;">value</span><span style="color:#24292E;">&gt;thrift://node1:9083&lt;/</span><span style="color:#22863A;">value</span><span style="color:#24292E;">&gt;</span></span>
<span class="line"><span style="color:#24292E;">&lt;/</span><span style="color:#22863A;">property</span><span style="color:#24292E;">&gt;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>代码中只需要增加 3 行代码以继承 Hive</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">spark </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> SparkSession.builder. \\</span></span>
<span class="line"><span style="color:#24292E;">        appName(</span><span style="color:#032F62;">&quot;local[*]&quot;</span><span style="color:#24292E;">). \\</span></span>
<span class="line"><span style="color:#24292E;">        config(</span><span style="color:#032F62;">&quot;spark.sql.shuffle.partitions&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;4&quot;</span><span style="color:#24292E;">). \\</span></span>
<span class="line"><span style="color:#24292E;">        config(</span><span style="color:#032F62;">&quot;spark.sql.warehouse.dir&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;hdfs://node1:8020/user/hive/warehouse&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">        config(</span><span style="color:#032F62;">&quot;hive.metastore.uris&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;thrift://node1:9083&quot;</span><span style="color:#24292E;">).\\</span></span>
<span class="line"><span style="color:#24292E;">        enableHiveSupport().\\ </span></span>
<span class="line"><span style="color:#24292E;">        getOrCreate()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="分布式-sql-执行引擎" tabindex="-1"><a class="header-anchor" href="#分布式-sql-执行引擎" aria-hidden="true">#</a> 分布式 SQL 执行引擎</h3><p>Spark 的 ThriftServer 服务可以在 10000 端口监听。通过该服务，用户会写 SQL 就可操作 spark。</p><p><strong>启动 ThriftServer 服务</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">$SPARK_HOME/sbin/start-thriftserver.sh \\</span></span>
<span class="line"><span style="color:#6F42C1;">--hiveconf</span><span style="color:#24292E;"> </span><span style="color:#032F62;">hive.server2.thrift.port=</span><span style="color:#005CC5;">10000</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">--hiveconf </span><span style="color:#032F62;">hive.server.thrift.bind.host=node1</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">--master </span><span style="color:#032F62;">local[</span><span style="color:#005CC5;">2</span><span style="color:#032F62;">]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>常用的用来链接的客户端工具有 DBeaver, datagrip, heidisql。python 代码链接 Thrift 可使用 pyhive 库。</p><p><code>pip install pyhive pymysql sasl thrift thrift_sasl</code></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> pyhive </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> hive</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">conn </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hive.Connection(</span><span style="color:#E36209;">host</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;node1&quot;</span><span style="color:#24292E;">, </span><span style="color:#E36209;">port</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">10000</span><span style="color:#24292E;">, </span><span style="color:#E36209;">username</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;hadoop&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">cursor </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> conn.cursor()</span></span>
<span class="line"><span style="color:#24292E;">cursor.execute(</span><span style="color:#032F62;">&quot;SELECT * FROM test&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">result </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> cursor.fetchall()</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(result)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h1 id="其他" tabindex="-1"><a class="header-anchor" href="#其他" aria-hidden="true">#</a> 其他</h1><p>Koalas——基于 Apache Spark 的 pandas API 实现</p>`,45),L={href:"https://spark.apache.org/docs/latest/ml-guide.html",target:"_blank",rel:"noopener noreferrer"},P=l(`<h1 id="其他运行模式搭建" tabindex="-1"><a class="header-anchor" href="#其他运行模式搭建" aria-hidden="true">#</a> 其他运行模式搭建</h1><h2 id="standalone-架构" tabindex="-1"><a class="header-anchor" href="#standalone-架构" aria-hidden="true">#</a> Standalone 架构</h2><p>Standalone 模式是 Spark 自带的一种集群模式，该模式中 master 与 worker 以独立进程的形式存在。 StandAlone 是完整的 Spark 运行环境，其中: Master 角色以 Master 进程存在, Worker 角色以 Worker 进程存在 Driver 和 Executor 运行于 Worker 进程内, 由 Worker 提供资源供给它们运行</p><p>StandAlone 集群在进程上主要有 3 类进程:</p><ul><li>主节点 Master 进程：Master 角色, 管理整个集群资源，并托管运行各个任务的 Driver</li><li>从节点 Workers：Worker 角色, 管理每个机器的资源，分配对应的资源来- 运行 Executor(Task)； 每个从节点分配资源信息给 Worker 管理，资源信息包含内存 Memory 和 CPU Cores 核数</li><li>历史服务器 HistoryServer(可选)：Spark Application 运行完成以后，保存事件日志数据至 HDFS，启动 HistoryServer 可以查看应用运行相关信息。</li></ul><h3 id="实例集群规划" tabindex="-1"><a class="header-anchor" href="#实例集群规划" aria-hidden="true">#</a> 实例集群规划</h3><p>尝试使用三台 Linux 虚拟机来组成集群环境进行体验，非别是:</p><p>node1 运行: Spark 的 Master 进程 和 1 个 Worker 进程 node2 运行: spark 的 1 个 worker 进程 node3 运行: spark 的 1 个 worker 进程</p><p>整个集群提供: 1 个 master 进程 和 3 个 worker 进程</p><h3 id="安装" tabindex="-1"><a class="header-anchor" href="#安装" aria-hidden="true">#</a> 安装</h3><p>在所有节点上安装 python anaconda ，同时不要忘记 都创建<code>pyspark</code>虚拟环境 以及安装虚拟环境所需要的包<code>pyspark jieba pyhive</code></p><p>为了让 spark 拥有 hdfs 最大权限，spark 安装也使用 hadoop 用户：<code>chown -R hadoop:hadoop spark*</code></p><h3 id="配置配置文件" tabindex="-1"><a class="header-anchor" href="#配置配置文件" aria-hidden="true">#</a> 配置配置文件</h3><p>进入到 spark 的配置文件目录中, <code>cd $SPARK_HOME/conf</code></p><p>配置 workers 文件</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 改名, 去掉后面的.template 后缀</span></span>
<span class="line"><span style="color:#6F42C1;">mv</span><span style="color:#24292E;"> </span><span style="color:#032F62;">workers.template</span><span style="color:#24292E;"> </span><span style="color:#032F62;">workers</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 编辑 worker 文件</span></span>
<span class="line"><span style="color:#6F42C1;">vim</span><span style="color:#24292E;"> </span><span style="color:#032F62;">workers</span></span>
<span class="line"><span style="color:#6A737D;"># 将里面的 localhost 删除, 追加</span></span>
<span class="line"><span style="color:#6F42C1;">node1</span></span>
<span class="line"><span style="color:#6F42C1;">node2</span></span>
<span class="line"><span style="color:#6F42C1;">node3</span></span>
<span class="line"><span style="color:#6F42C1;">到</span><span style="color:#24292E;"> </span><span style="color:#032F62;">workers</span><span style="color:#24292E;"> </span><span style="color:#032F62;">文件内</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 功能: 这个文件就是指示了  当前 SparkStandAlone 环境下, 有哪些 worker</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>配置 spark-env.sh 文件</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 1. 改名</span></span>
<span class="line"><span style="color:#6F42C1;">mv</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-env.sh.template</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-env.sh</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 2. 编辑 spark-env.sh, 在底部追加如下内容</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## 设置 JAVA 安装目录</span></span>
<span class="line"><span style="color:#24292E;">JAVA_HOME</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/jdk</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## HADOOP 软件配置文件目录，读取 HDFS 上文件和运行 YARN 集群</span></span>
<span class="line"><span style="color:#24292E;">HADOOP_CONF_DIR</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/hadoop/etc/hadoop</span></span>
<span class="line"><span style="color:#24292E;">YARN_CONF_DIR</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/hadoop/etc/hadoop</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## 指定 spark 老大 Master 的 IP 和提交任务的通信端口</span></span>
<span class="line"><span style="color:#6A737D;"># 告知 Spark 的 master 运行在哪个机器上</span></span>
<span class="line"><span style="color:#D73A49;">export</span><span style="color:#24292E;"> SPARK_MASTER_HOST</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">node1</span></span>
<span class="line"><span style="color:#6A737D;"># 告知 sparkmaster 的通讯端口</span></span>
<span class="line"><span style="color:#D73A49;">export</span><span style="color:#24292E;"> SPARK_MASTER_PORT</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">7077</span></span>
<span class="line"><span style="color:#6A737D;"># 告知 spark master 的 webui 端口</span></span>
<span class="line"><span style="color:#24292E;">SPARK_MASTER_WEBUI_PORT</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">8080</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># worker cpu 可用核数</span></span>
<span class="line"><span style="color:#24292E;">SPARK_WORKER_CORES</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span></span>
<span class="line"><span style="color:#6A737D;"># worker 可用内存</span></span>
<span class="line"><span style="color:#24292E;">SPARK_WORKER_MEMORY</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#032F62;">g</span></span>
<span class="line"><span style="color:#6A737D;"># worker 的工作通讯地址</span></span>
<span class="line"><span style="color:#24292E;">SPARK_WORKER_PORT</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">7078</span></span>
<span class="line"><span style="color:#6A737D;"># worker 的 webui 地址</span></span>
<span class="line"><span style="color:#24292E;">SPARK_WORKER_WEBUI_PORT</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">8081</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## 设置历史服务器</span></span>
<span class="line"><span style="color:#6A737D;"># 配置的意思是  将 spark 程序运行的历史日志 存到 hdfs 的/sparklog 文件夹中</span></span>
<span class="line"><span style="color:#24292E;">SPARK_HISTORY_OPTS</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>注意, 上面的配置的路径 要根据你自己机器实际的路径来写</p><p>在 HDFS 上创建程序运行历史记录存放的文件夹:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">hadoop</span><span style="color:#24292E;"> </span><span style="color:#032F62;">fs</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-mkdir</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/sparklog</span></span>
<span class="line"><span style="color:#6F42C1;">hadoop</span><span style="color:#24292E;"> </span><span style="color:#032F62;">fs</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-chmod</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">777</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/sparklog</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>配置 spark-defaults.conf 文件</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 1. 改名</span></span>
<span class="line"><span style="color:#6F42C1;">mv</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-defaults.conf.template</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-defaults.conf</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 2. 修改内容, 追加如下内容</span></span>
<span class="line"><span style="color:#6A737D;"># 开启 spark 的日期记录功能</span></span>
<span class="line"><span style="color:#6F42C1;">spark.eventLog.enabled</span><span style="color:#24292E;"> 	</span><span style="color:#005CC5;">true</span></span>
<span class="line"><span style="color:#6A737D;"># 设置 spark 日志记录的路径</span></span>
<span class="line"><span style="color:#6F42C1;">spark.eventLog.dir</span><span style="color:#24292E;">	 </span><span style="color:#032F62;">hdfs://node1:8020/sparklog/</span><span style="color:#24292E;"> </span></span>
<span class="line"><span style="color:#6A737D;"># 设置 spark 日志是否启动压缩</span></span>
<span class="line"><span style="color:#6F42C1;">spark.eventLog.compress</span><span style="color:#24292E;"> 	</span><span style="color:#005CC5;">true</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>配置 log4j.properties 文件 [可选配置]</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 1. 改名</span></span>
<span class="line"><span style="color:#6F42C1;">mv</span><span style="color:#24292E;"> </span><span style="color:#032F62;">log4j.properties.template</span><span style="color:#24292E;"> </span><span style="color:#032F62;">log4j.properties</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 2. 修改日志警报级别 因为 Spark 是个话痨</span></span>
<span class="line"><span style="color:#6F42C1;">log4j.rootCategory</span><span style="color:#24292E;">=WARN, </span><span style="color:#032F62;">console</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="将-spark-安装文件夹-分发到其它的节点上" tabindex="-1"><a class="header-anchor" href="#将-spark-安装文件夹-分发到其它的节点上" aria-hidden="true">#</a> 将 Spark 安装文件夹 分发到其它的节点上</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">scp</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-r</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-3.1.2-bin-hadoop3.2</span><span style="color:#24292E;"> </span><span style="color:#032F62;">node2:/export/server/</span></span>
<span class="line"><span style="color:#6F42C1;">scp</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-r</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-3.1.2-bin-hadoop3.2</span><span style="color:#24292E;"> </span><span style="color:#032F62;">node3:/export/server/</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>不要忘记, 在 node2 和 node3 上 给 spark 安装目录增加软链接</p><p><code>ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark</code></p><h3 id="验证环境" tabindex="-1"><a class="header-anchor" href="#验证环境" aria-hidden="true">#</a> 验证环境</h3><p>先开启 zoopkeeper <code>zookeeper/bin/zkServer.sh start</code>, hadoop <code>s</code>等。 启动历史服务器 <code>sbin/start-history-server.sh</code> 启动全部 workers 和 master：<code>sbin/start-all.sh</code></p><p>分别测试 <code>pyspark</code>, <code>spark-shell</code>, <code>spark-submit</code> 使用情况：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">bin/pyspark</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark://node1:7077</span></span>
<span class="line"><span style="color:#6A737D;"># 提供 --master 连接到 StandAlone，不写默认是 local 模式</span></span>
<span class="line"><span style="color:#6F42C1;">bin/spark-shell</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark://node1:7077</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#6F42C1;">bin/spark-submit</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark://node1:7077</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/export/server/spark/examples/src/main/python/pi.py</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">100</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行后通过 web <code>node1:18080</code> 查看历史服务器信息；<code>node1:8080</code> 查看 master。</p><h2 id="spark-standalone-ha" tabindex="-1"><a class="header-anchor" href="#spark-standalone-ha" aria-hidden="true">#</a> Spark StandAlone HA</h2>`,35),H={href:"https://spark.apache.org/docs/3.1.2/spark-standalone.html#standby-masters-with-zookeeper",target:"_blank",rel:"noopener noreferrer"},K=l(`<h3 id="步骤" tabindex="-1"><a class="header-anchor" href="#步骤" aria-hidden="true">#</a> 步骤</h3><p>前提: 确保 Zookeeper 和 HDFS 均已经启动 在<code>spark-env.sh</code>中, 删除: <code>SPARK_MASTER_HOST=node1</code> 在<code>spark-env.sh</code>中, 增加:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">SPARK_DAEMON_JAVA_OPTS</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span></span>
<span class="line"><span style="color:#6A737D;"># spark.deploy.recoveryMode 指定 HA 模式 基于 Zookeeper 实现</span></span>
<span class="line"><span style="color:#6A737D;"># 指定 Zookeeper 的连接地址</span></span>
<span class="line"><span style="color:#6A737D;"># 指定在 Zookeeper 中注册临时节点的路径</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,3),I={href:"http://xn--spark-env-kj5q.sh/",target:"_blank",rel:"noopener noreferrer"},B=l(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">scp</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-env.sh</span><span style="color:#24292E;"> </span><span style="color:#032F62;">node2:/export/server/spark/conf/</span></span>
<span class="line"><span style="color:#6F42C1;">scp</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-env.sh</span><span style="color:#24292E;"> </span><span style="color:#032F62;">node3:/export/server/spark/conf/</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>停止当前 StandAlone 集群</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">sbin/stop-all.sh</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="master-主备切换" tabindex="-1"><a class="header-anchor" href="#master-主备切换" aria-hidden="true">#</a> master 主备切换</h3><p>提交一个 spark 任务到当前<code>alive</code>master 上:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">bin/spark-submit</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark://node1:7077</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/export/server/spark/examples/src/main/python/pi.py</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>在提交成功后, 将 alivemaster 直接 kill 掉，不会影响程序运行:<br><img src="https://pybd.yuque.com/api/filetransfer/images?url=https%3A%2F%2Fimage-set.oss-cn-zhangjiakou.aliyuncs.com%2Fimg-out%2F2021%2F09%2F08%2F20210908162555.png&amp;sign=6c267116ad788645fdc2af413af7ac1c6e22ae0d655afe3dd4fda1117f6d5253#from=url&amp;id=AAdNb&amp;margin=[object Object]&amp;originHeight=314&amp;originWidth=1889&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none" alt="" loading="lazy"><br> 当新的 master 接收集群后, 程序继续运行, 正常得到结果。同时新 master 的 8080 界面显示状态从 STANDBY 变为 RECOVERING/ ACTIVE。</p><p>HA 模式下, 主备切换 不会影响到正在运行的程序.</p><h2 id="spark-on-yarn-环境搭建" tabindex="-1"><a class="header-anchor" href="#spark-on-yarn-环境搭建" aria-hidden="true">#</a> Spark On YARN 环境搭建</h2><p>许多企业不管做什么业务,都基本上会有 Hadoop 集群. 也就是会有 YARN 集群。因此在有 YARN 集群的前提下单独准备 Spark StandAlone 集群,对资源的利用就不高。 Spark on YARN 是最常见的应用框架。 对于 Spark On YARN, 无需部署 Spark 集群, 只要找一台服务器, 充当 Spark 的客户端, 即可提交任务到 YARN 集群中运行。</p><p><strong>本质</strong></p><p>Master 角色由 YARN 的 ResourceManager 担任 Worker 角色由 YARN 的 NodeManager 担任 Driver 角色运行在 YARN 容器内或提交任务的客户端进程中 真正干活的 Executor 运行在 YARN 提供的容器内</p><p><strong>部署</strong></p><p>配置 spark-env.sh 中的 HADOOP_CONF_DIR、 YARN_CONF_DIR 环境变量，指向 hadoop 与 yarn 的配置文件目录</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">HADOOP_CONF_DIR</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/hadoop/etc/hadoop/</span></span>
<span class="line"><span style="color:#24292E;">YARN_CONF_DIR</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/hadoop/etc/hadoop/</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,15),j={href:"https://www.cnblogs.com/rmxd/p/12273395.html",target:"_blank",rel:"noopener noreferrer"};function V(z,Q){const n=p("ExternalLinkIcon");return r(),c("div",null,[s("blockquote",null,[y,s("p",null,[a("python 上 Spark 不断个更新，实时关注 "),s("a",u,[a("官方文档"),e(n)])])]),t("more"),h,s("p",null,[a("Spark 用于大规模数据处理的统一分析引擎。其特点就是对任意类型的数据进行自定义计算。 RDD 是一种分布式内存抽象，使程序员能够在大规模集群中做内存运算，并且有一定的容错方式。这也是 Spark 的核心数据结构。"),s("a",v,[a("知乎解答 - spark 概述"),e(n)])]),m,s("p",null,[a("根据 "),s("a",b,[a("官方指南"),e(n)]),a("， spark 提供了多种运行模式：包括本地，集群（StandAlone, YARN, K8S），云模式。")]),E,s("p",null,[s("a",g,[a("官方文档"),e(n)]),a("对于大规模数据集处理，安装直接使用 "),k,a(" 即可。")]),D,s("p",null,[a("详细算子 "),s("a",f,[a("官网"),e(n)]),a(" 可查看，以下总结一些常用的。")]),C,s("p",null,[a("spark 提供了 RDD 缓存 API 以减少重复计算。可以使用 "),A,a(" 或 "),S,a(" 来缓存，一般用的较多的是："),x,a(" 。对于 python，都是用 Pickle 进行序列化缓存，更多缓存等级参考："),s("a",F,[a("官方"),e(n)]),a("。主动清理缓存的 API："),q,a("。 "),R]),_,s("p",null,[a("DataFrame 可以通过 "),O,a(" 直接操作，使用 sql 命令需要先注册成临时表："),T,a("，以下展示部分处 "),N,a(" 外的常用操作，详细查看 "),s("a",M,[a("官方 API"),e(n)])]),w,s("p",null,[s("a",L,[a("Spark mllib 文档"),e(n)]),a(" MLlib 提供基础的机械学习算法，代码风格类似 sklearn。")]),P,s("p",null,[a("Spark Standalone 集群是 Master-Slaves 架构的集群模式，和大部分的 Master-Slaves 结构集群一样，存在着 Master 单点故障（SPOF）的问题。 如何解决这个单点故障的问题，Spark 提供了两种方案： 1.基于文件系统的单点恢复(Single-Node Recovery with Local File System)--只能用于开发或测试环境。 2.基于 zookeeper 的 Standby Masters(Standby Masters with ZooKeeper)--可以用于生产环境。 ZooKeeper 提供了一个 Leader Election 机制，利用这个机制可以保证虽然集群存在多个 Master，但是只有一个是 Active 的，其他的都是 Standby。当 Active 的 Master 出现故障时，另外的一个 Standby Master 会被选举出来。由于集群的信息，包括 Worker，Driver 和 Application 的信息都已经持久化到文件系统，因此在切换的过程中只会影响新 Job 的提交，对于正在进行的 Job 没有任何的影响。加入 ZooKeeper 的集群整体架构如下图所示。 toadd "),s("a",H,[a("基于 Zookeeper 实现 HA"),e(n)])]),K,s("p",null,[s("a",I,[a("将 spark-env.sh"),e(n)]),a(" 分发到每一台服务器上")]),B,s("p",null,[s("a",j,[a("参考链接 - 连接到 YARN 中"),e(n)])])])}const W=o(d,[["render",V],["__file","笔记spark.html.vue"]]);export{W as default};
