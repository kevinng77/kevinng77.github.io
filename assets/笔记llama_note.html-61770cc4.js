import{_ as t,E as p,S as e,W as o,$ as n,a3 as s,Z as l,aS as c}from"./framework-d5c0d2cb.js";const i={},u=c(`<h2 id="llama-note" tabindex="-1"><a class="header-anchor" href="#llama-note" aria-hidden="true">#</a> llama note</h2><h3 id="llama-系列模型" tabindex="-1"><a class="header-anchor" href="#llama-系列模型" aria-hidden="true">#</a> LLaMa 系列模型</h3><p>baichuan，qwen 等与 llama 架构类似。不同点在于中文的这几个模型对此表进行了扩充，预训练方式不同，instruction tuning prompt template 不同，baichuan，qwen 分别采用 <code>w_proj</code> 和 <code>c_proj</code> 来代替 hf llama 官方的 <code>k,q,v_proj</code>。因此除了 lora 训练时需要映射一下位置，GPTQ 也需要做一下调整。</p><p>lora 有些人直接给设置成对 <code>q_proj, k_proj, v_proj, W_proj</code> 等等一系列不同模型采用的权重名称，似乎也不是不行。</p><h3 id="swiglu" tabindex="-1"><a class="header-anchor" href="#swiglu" aria-hidden="true">#</a> SwiGLU</h3><p>官方实现方式为：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>intermediate_size <span class="token operator">=</span> config<span class="token punctuation">.</span>intermediate_size
        self<span class="token punctuation">.</span>gate_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act_fn <span class="token operator">=</span> ACT2FN<span class="token punctuation">[</span>config<span class="token punctuation">.</span>hidden_act<span class="token punctuation">]</span> 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token builtin">slice</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>intermediate_size <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp
            gate_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            up_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            down_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

            gate_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> gate_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>
            <span class="token punctuation">)</span>
            up_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> up_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

            intermediate_states <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>gate_proj<span class="token punctuation">)</span> <span class="token operator">*</span> up_proj<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            down_proj <span class="token operator">=</span> <span class="token punctuation">[</span>
                F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>intermediate_states<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> down_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span>
            <span class="token punctuation">]</span>
            down_proj <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>down_proj<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            down_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> down_proj
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>因此，尽管 llama 的 config 文件中，写了 hidden_act 为 silu，但是实际上用的的确时 SwiGLU。</p>`,8),r=n("p",{class:"katex-block"},[n("span",{class:"katex-display"},[n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[n("semantics",null,[n("mrow",null,[n("mi",null,"S"),n("mi",null,"i"),n("mi",null,"l"),n("mi",null,"u"),n("mo",null,"="),n("mi",null,"σ"),n("mo",{stretchy:"false"},"("),n("mi",null,"x"),n("mo",{stretchy:"false"},")"),n("mo",null,"∗"),n("mi",null,"x")]),n("annotation",{encoding:"application/x-tex"}," Silu = \\sigma (x) * x ")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),n("span",{class:"mord mathnormal"},"i"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),n("span",{class:"mord mathnormal"},"u"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"σ"),n("span",{class:"mopen"},"("),n("span",{class:"mord mathnormal"},"x"),n("span",{class:"mclose"},")"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),n("span",{class:"mbin"},"∗"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.4306em"}}),n("span",{class:"mord mathnormal"},"x")])])])])],-1),m=n("p",null,"SwiGLU 融合了 GLU 的思想，结合 LLaMa 中 FFN 没有 bias，因此可以写成：",-1),k=n("p",{class:"katex-block"},[n("span",{class:"katex-display"},[n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[n("semantics",null,[n("mrow",null,[n("mi",null,"S"),n("mi",null,"w"),n("mi",null,"i"),n("mi",null,"G"),n("mi",null,"L"),n("mi",null,"U"),n("mo",null,"="),n("mi",null,"S"),n("mi",null,"i"),n("mi",null,"l"),n("mi",null,"u"),n("mo",{stretchy:"false"},"("),n("mi",null,"x"),n("mi",null,"W"),n("mo",{stretchy:"false"},")"),n("mo",null,"×"),n("mo",{stretchy:"false"},"("),n("mi",null,"x"),n("mi",null,"V"),n("mo",{stretchy:"false"},")")]),n("annotation",{encoding:"application/x-tex"}," SwiGLU = Silu(xW)\\times (xV) ")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6833em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"Sw"),n("span",{class:"mord mathnormal"},"i"),n("span",{class:"mord mathnormal"},"G"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"LU"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),n("span",{class:"mord mathnormal"},"i"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),n("span",{class:"mord mathnormal"},"u"),n("span",{class:"mopen"},"("),n("span",{class:"mord mathnormal"},"x"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"mclose"},")"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),n("span",{class:"mbin"},"×"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),n("span",{class:"mopen"},"("),n("span",{class:"mord mathnormal"},"x"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V"),n("span",{class:"mclose"},")")])])])])],-1),d=n("p",null,[s("其中 "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"W"),n("mo",{separator:"true"},","),n("mi",null,"V")]),n("annotation",{encoding:"application/x-tex"},"W,V")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.8778em","vertical-align":"-0.1944em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"mpunct"},","),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V")])])]),s(" 分别表示 "),n("code",null,"gate_proj"),s(" 和 "),n("code",null,"up_proj")],-1),h=n("h3",{id:"训练",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#训练","aria-hidden":"true"},"#"),s(" 训练")],-1),_=n("p",null,"LLaMa 出了名的，很多仓库做好了适配，没必要去自己根据 huggingface 写一个（当然，除非有很好的改进想法，如优化 loss 计算方式等）。现阶段完全采用 CasualLM 的训练优化方案了。",-1),g=n("div",{class:"hint-container warning"},[n("p",{class:"hint-container-title"},"注意"),n("p",null,"很重要的一点，HF 的模型在计算 loss 时候，会自动对 label 进行 shift，以此计算 next token prediction 的 loss。因此我们输入中的 label 不需要进行提前的 shift。"),n("p",null,"此外 可以通过 ignore token 来进行多轮对话训练（参考 fschat 的代码）")],-1),f=n("p",null,"单卡 7B-lora 训练可以用：",-1),b={href:"https://github.com/hiyouga/LLaMA-Efficient-Tuning",target:"_blank",rel:"noopener noreferrer"},v=n("p",null,"多卡多轮对话训练：",-1),w=n("ul",null,[n("li",null,"FastChat")],-1);function y(x,j){const a=p("ExternalLinkIcon");return e(),o("div",null,[u,r,m,k,d,h,_,g,f,n("ul",null,[n("li",null,[n("a",b,[s("llama-efficient-tuning"),l(a)])])]),v,w])}const M=t(i,[["render",y],["__file","笔记llama_note.html.vue"]]);export{M as default};
