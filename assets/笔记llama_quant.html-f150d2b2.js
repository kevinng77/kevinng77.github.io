import{_ as l}from"./plugin-vue_export-helper-c27b6911.js";import{r as t,o as r,c as p,a,b as s,d as e,f as o}from"./app-c5365da1.js";const c={},i=a("p",null,"本文导论部署 LLaMa 系列模型常用的几种方案，并作速度测试。包括 Huggingface 自带的 LLM.int8()，AutoGPTQ, GPTQ-for-LLaMa, exllama。",-1),d=a("p",null,"总结来看，对 7B 级别的 LLaMa 系列模型，经过 GPTQ 量化后，在 4090 上可以达到 140+ tokens/s 的推理速度。在 3070 上可以达到 40 tokens/s 的推理速度。",-1),u=a("h2",{id:"lm-int8",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#lm-int8","aria-hidden":"true"},"#"),s(),a("strong",null,"LM.int8()")],-1),h={href:"https://arxiv.org/pdf/2208.07339.pdf",target:"_blank",rel:"noopener noreferrer"},g={href:"https://huggingface.co/docs/transformers/main_classes/quantization",target:"_blank",rel:"noopener noreferrer"},b=a("figure",null,[a("img",{src:"https://pic1.zhimg.com/80/v2-316fd349517183edf3508ddd907f01c4_1440w.png?source=d16d100b",alt:"img",tabindex:"0",loading:"lazy"}),a("figcaption",null,"img")],-1),y={href:"https://huggingface.co/blog/hf-bitsandbytes-integration",target:"_blank",rel:"noopener noreferrer"},m=a("p",null,"结合论文中的实验结果，模型越大，int8() 加速越明显，个人猜测是由于非 outlier 数量变多了，更多的参数进行了 int8 计算，抵消了额外的量化转化时间开销？",-1),f=a("figure",null,[a("img",{src:"https://picx.zhimg.com/80/v2-ffdf642b3e4922782c100ccd5cd9356c_1440w.png?source=d16d100b",alt:"img",tabindex:"0",loading:"lazy"}),a("figcaption",null,"img")],-1),_=a("h2",{id:"gptq",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#gptq","aria-hidden":"true"},"#"),s(),a("strong",null,"GPTQ")],-1),v=a("p",null,"GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS",-1),L={href:"https://zhuanlan.zhihu.com/p/629517722",target:"_blank",rel:"noopener noreferrer"},C={href:"https://github.com/oobabooga/text-generation-webui",target:"_blank",rel:"noopener noreferrer"},E={id:"gptq-for-llama",tabindex:"-1"},T=a("a",{class:"header-anchor",href:"#gptq-for-llama","aria-hidden":"true"},"#",-1),G={href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/fastest-inference-4bit",target:"_blank",rel:"noopener noreferrer"},P=a("strong",null,"GPTQ-for-LLaMa",-1),k={href:"https://huggingface.co/TheBloke",target:"_blank",rel:"noopener noreferrer"},Q={href:"https://huggingface.co/datasets/allenai/c4",target:"_blank",rel:"noopener noreferrer"},M=o(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">CUDA_VISIBLE_DEVICES</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">python</span><span style="color:#24292E;"> </span><span style="color:#032F62;">llama.py</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/models/vicuna-7b</span><span style="color:#24292E;"> </span><span style="color:#032F62;">c4</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--wbits</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">4</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--true-sequential</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--groupsize</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">128</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--save_safetensors</span><span style="color:#24292E;"> </span><span style="color:#032F62;">vicuna7b-gptq-4bit-128g.safetensors</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>由于 GPTQ 是 Layer-Wise Quantization，因此进行量化时对内存和显存要求会少一点。在 4090 测试，最高峰显存占用 7000MiB，整个 GPTQ 量化过程需要 10 分钟。量化后进行 PPL 测试，7b 在没有 arc_order 量化下，c4 的 ppl 大概会在 5-6 左右：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">CUDA_VISIBLE_DEVICES</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">python</span><span style="color:#24292E;"> </span><span style="color:#032F62;">llama.py</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/models/vicuna-7b</span><span style="color:#24292E;"> </span><span style="color:#032F62;">c4</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--wbits</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">4</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--groupsize</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">128</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--load</span><span style="color:#24292E;"> </span><span style="color:#032F62;">vicuna7b-gptq-4bit-128g.safetensors</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--benchmark</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2048</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--check</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,3),x={href:"https://github.com/FranxYao/chain-of-thought-hub/tree/main",target:"_blank",rel:"noopener noreferrer"},q={href:"https://huggingface.co/TheBloke",target:"_blank",rel:"noopener noreferrer"},F=a("p",null,"GPTQ-for-LLaMa 的一些坑：",-1),A={href:"https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/discussions/5",target:"_blank",rel:"noopener noreferrer"},B=a("li",null,[a("p",null,"left-padding 问题：目前 GPTQ-for-LLaMa 的所有分支（triton, old-cuda 或 fastest-inference-int4）都存在该问题。如果模型对存在 left-padding 的输入进行预测时候，输出结果是混乱的。这导致了 GPTQ-for-LLaMa 目前无法支持正确的 batch inference。")],-1),z={href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa/issues/89",target:"_blank",rel:"noopener noreferrer"},I={href:"https://github.com/oobabooga/GPTQ-for-LLaMa",target:"_blank",rel:"noopener noreferrer"},w={href:"https://github.com/oobabooga/GPTQ-for-LLaMa",target:"_blank",rel:"noopener noreferrer"},D={href:"https://github.com/oobabooga/text-generation-webui",target:"_blank",rel:"noopener noreferrer"},U={id:"autogptq",tabindex:"-1"},O=a("a",{class:"header-anchor",href:"#autogptq","aria-hidden":"true"},"#",-1),S={href:"https://github.com/PanQiWei/AutoGPTQ",target:"_blank",rel:"noopener noreferrer"},V=a("strong",null,"AutoGPTQ",-1),H={href:"https://github.com/PanQiWei/AutoGPTQ/blob/main/docs/tutorial/01-Quick-Start.md",target:"_blank",rel:"noopener noreferrer"},N={href:"https://github.com/PanQiWei/AutoGPTQ/blob/main/docs/tutorial/02-Advanced-Model-Loading-and-Best-Practice.md",target:"_blank",rel:"noopener noreferrer"},R=o(`<p>AutoGPTQ 可以直接加载 GPTQ-for-LLaMa 的量化模型：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> auto_gptq </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> AutoGPTQForCausalLM</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">model </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> AutoGPTQForCausalLM.from_quantized(</span></span>
<span class="line"><span style="color:#24292E;">    model_dir,     </span><span style="color:#6A737D;"># 存放模型的文件路径，里面包含 config.json, tokenizer.json 等模型配置文件</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">model_basename</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;vicuna7b-gptq-4bit-128g.safetensors&quot;</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">use_safetensors</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;cuda:0&quot;</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">use_triton</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">,    </span><span style="color:#6A737D;"># Batch inference 时候开启 triton 更快</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">max_memory</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> {</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">: </span><span style="color:#032F62;">&quot;20GIB&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;cpu&quot;</span><span style="color:#24292E;">: </span><span style="color:#032F62;">&quot;20GIB&quot;</span><span style="color:#24292E;">}    </span><span style="color:#6A737D;"># </span></span>
<span class="line"><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>AutoGPTQ 提供了更多的量化加载选项，如是否采用 fused_attention，配置 CPU offload 等。用 AutoGPTQ 加载权重会省去很多不必要的麻烦，如 AutoGPTQ 并没有 GPTQ-for-LLaMa 类似的 left-padding bug，对 Huggingface 其他 LLM 模型的兼容性更好。因此如果做 GPTQ-INT4 batch inference 的话，AutoGPTQ 会是首选。</p><p>但对于 LLaMa 系列模型，AutoGPTQ 的速度会明显慢于 GPTQ-for-LLaMa。在 4090 上测试，GPTQ-for-LLaMa 的推理速度会块差不多 30%。</p>`,4),W={id:"exllama",tabindex:"-1"},j=a("a",{class:"header-anchor",href:"#exllama","aria-hidden":"true"},"#",-1),K={href:"https://github.com/turboderp/exllama",target:"_blank",rel:"noopener noreferrer"},Y=a("strong",null,"exllama",-1),Z={href:"https://github.com/turboderp/exllama",target:"_blank",rel:"noopener noreferrer"},J={href:"https://github.com/oobabooga/text-generation-webui/blob/main/modules/exllama_hf.py",target:"_blank",rel:"noopener noreferrer"},X={id:"gptq-1",tabindex:"-1"},$=a("a",{class:"header-anchor",href:"#gptq-1","aria-hidden":"true"},"#",-1),aa={href:"https://github.com/IST-DASLab/gptq",target:"_blank",rel:"noopener noreferrer"},sa=a("p",null,"GPTQ 的官方仓库。以上大部分仓库都是基于官方仓库开发的，感谢 GPTQ 的开源，让单卡 24G 显存也能跑上 33B 的大模型。",-1),na=a("h2",{id:"ggml",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#ggml","aria-hidden":"true"},"#"),s(" GGML")],-1),ea={href:"https://github.com/ggerganov/ggml",target:"_blank",rel:"noopener noreferrer"},oa={href:"https://github.com/ggerganov/llama.cpp",target:"_blank",rel:"noopener noreferrer"},la={href:"https://github.com/abetlen/llama-cpp-python",target:"_blank",rel:"noopener noreferrer"},ta=a("p",null,"llama.cpp 在一个月前支持了全面 GPU 加速（在推理的时候，可以把整个模型放在 GPU 上推理）。参考后文的测试，LLaMa.cpp 比 AutoGPTQ 有更快的推理速度，但是还是比 exllama 慢很多。",-1),ra={href:"https://github.com/ggerganov/llama.cpp#quantization",target:"_blank",rel:"noopener noreferrer"},pa={href:"https://github.com/ggerganov/llama.cpp#docker-with-cuda",target:"_blank",rel:"noopener noreferrer"},ca=a("code",null,".devops/full-cuda.Dockerfile",-1),ia={href:"https://github.com/ggerganov/llama.cpp/blob/master/.devops/full-cuda.Dockerfile#L33",target:"_blank",rel:"noopener noreferrer"},da=o(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">docker</span><span style="color:#24292E;"> </span><span style="color:#032F62;">build</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-t</span><span style="color:#24292E;"> </span><span style="color:#032F62;">local/llama.cpp:full-cuda</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-f</span><span style="color:#24292E;"> </span><span style="color:#032F62;">.devops/full-cuda.Dockerfile</span><span style="color:#24292E;"> </span><span style="color:#032F62;">.</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>构建成功后开启容器（models 映射到模型文件路径）：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">docker</span><span style="color:#24292E;"> </span><span style="color:#032F62;">run</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-it</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--name</span><span style="color:#24292E;"> </span><span style="color:#032F62;">ggml</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--gpus</span><span style="color:#24292E;"> </span><span style="color:#032F62;">all</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-p</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">8080</span><span style="color:#032F62;">:8080</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-v</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/home/kevin/models:/models</span><span style="color:#24292E;"> </span><span style="color:#032F62;">local/llama.cpp:full-cuda</span><span style="color:#24292E;"> </span><span style="color:#032F62;">bash</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>`,3),ua={href:"https://github.com/ggerganov/llama.cpp#prepare-data--run",target:"_blank",rel:"noopener noreferrer"},ha=o(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 转换 ggml 权重</span></span>
<span class="line"><span style="color:#6F42C1;">python3</span><span style="color:#24292E;"> </span><span style="color:#032F62;">convert.py</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/models/Llama-2-13b-chat-hf/</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 量化</span></span>
<span class="line"><span style="color:#6F42C1;">./quantize</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/models/Llama-2-13b-chat-hf/ggml-model-f16.bin</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/models/Llama-2-13b-chat-GGML_q4_0/ggml-model-q4_0.bin</span><span style="color:#24292E;"> </span><span style="color:#032F62;">q4_0</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>完成后开启 server 测试</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">./server</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-m</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/models/Llama-2-13b-chat-GGML_q4_0/ggml-model-q4_0.bin</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--host</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0.0</span><span style="color:#032F62;">.0.0</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--ctx-size</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2048</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--n-gpu-layers</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">128</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>发送请求测试：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">curl</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--request</span><span style="color:#24292E;"> </span><span style="color:#032F62;">POST</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--url</span><span style="color:#24292E;"> </span><span style="color:#032F62;">http://localhost:8080/completion</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--header</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;Content-Type: application/json&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--data</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&#39;{&quot;prompt&quot;: &quot;Once a upon time,&quot;,&quot;n_predict&quot;: 200}&#39;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,5),ga={href:"https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md",target:"_blank",rel:"noopener noreferrer"},ba=o('<ul><li><code>--ctx-size</code>: 上下文长度。</li><li><code>--n-gpu-layers</code>：在 GPU 上放多少模型 layer，我们选择将整个模型放在 GPU 上。</li><li><code>--batch-size</code>：处理 prompt 时候的 batch size。</li></ul><p>使用 llama.cpp 部署的请求，速度与 llama-cpp-python 差不多。对于上述例子中，发送 <code>Once a upon time,</code> 并返回 200 个字符，两者完成时间都在 2400 ms 左右（约 80 tokens/秒）。</p><h2 id="推理部署" tabindex="-1"><a class="header-anchor" href="#推理部署" aria-hidden="true">#</a> <strong>推理部署</strong></h2><p>记得在 bert 时代，部署 Pytorch 模型时可能会考虑一些方面，比如动态图转静态图，将模型导出到 onnx，torch jit 等，混合精度推理，量化，剪枝，蒸馏等。对于这些推理加速方案，我们可能需要自己手动应用到训练好的模型上。但在 LLaMa 时代，感受到最大的变化就是，一些开源的框架似乎为你做好了一切，只需要把你训练好的模型权重放上去就能实现比 HF 模型快 n 倍的推理速度。</p><p>以下对比这些推理加速方案：HF 官方 float16（基线）, vllm，llm.int8()，GPTQ-for-LLaMa，AUTOGPTQ，exllama, llama.cpp。</p><table><thead><tr><th>Model_name</th><th>tool</th><th>tokens/s</th></tr></thead><tbody><tr><td>vicuna 7b</td><td>float16</td><td>43.27</td></tr><tr><td>vicuna 7b</td><td>load-in-8bit (HF)</td><td>19.21</td></tr><tr><td>vicuna 7b</td><td>load-in-4bit (HF)</td><td>28.25</td></tr><tr><td>vicuna7b-gptq-4bit-128g</td><td>AUTOGPTQ</td><td>79.8</td></tr><tr><td>vicuna7b-gptq-4bit-128g</td><td>GPTQ-for-LLaMa</td><td>80.0</td></tr><tr><td>vicuna7b-gptq-4bit-128g</td><td>exllama</td><td>143.0</td></tr><tr><td>Llama-2-7B-Chat-GGML (q4_0)</td><td>llama.cpp</td><td>111.25</td></tr><tr><td>Llama-2-13B-Chat-GGML (q4_0)</td><td>llama.cpp</td><td>72.69</td></tr><tr><td>Wizard-Vicuna-13B-GPTQ</td><td>exllama</td><td>90</td></tr><tr><td>Wizard-Vicuna-30B-uncensored-GPTQ</td><td>exllama</td><td>43.1</td></tr><tr><td>Wizard-Vicuna-30B-uncensored_4_0</td><td>llama.cpp</td><td>34.03</td></tr><tr><td>Wizard-Vicuna-30B-uncensored-GPTQ</td><td>AUTOGPTQ</td><td>31</td></tr></tbody></table>',6),ya={href:"https://github.com/oobabooga/text-generation-webui",target:"_blank",rel:"noopener noreferrer"},ma={href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa#result",target:"_blank",rel:"noopener noreferrer"},fa={href:"https://github.com/turboderp/exllama/tree/master#new-implementation",target:"_blank",rel:"noopener noreferrer"},_a=a("h3",{id:"一些备注",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#一些备注","aria-hidden":"true"},"#"),s(),a("strong",null,"一些备注")],-1),va={href:"https://discuss.huggingface.co/t/baffling-performance-issue-on-most-nvidia-gpus-with-simple-transformers-pytorch-code/39292/3",target:"_blank",rel:"noopener noreferrer"},La=a("li",null,"对于 stable diffusion，torch cuda118 能比 torch cuda 117 速度快上 1 倍。但对于 LLaMa 来说，cuda 117 和 118 差别不大。",-1),Ca=a("li",null,"量化 batch inference 首选 AUTOGPTQ (TRITON)，尽管 AutoGPTQ 速度慢点，但目前版本的 GPTQ-for-LLaMa 存在 left-padding 问题，无法使用 batch inference；batch size = 1 时，首选 exllama 或者 GPTQ-for-LLaMa。",-1),Ea=a("li",null,"vllm 部署 fp16 的模型速度也不错（80+ tokens/s），同时也做了内存优化；如果设备资源够的话，可以考虑下 vllm，毕竟采用 GPTQ 还是有一点精度偏差的。",-1),Ta=a("li",null,"TheBloke 早期发布的一些模型可能无法加载到 exllama 当中，可以使用最新版本的 GPTQ-for-LLaMa 训练一个新模型。",-1),Ga=a("li",null,"当显卡容量无法加载整个模型时（比如在单卡 4090 上加载 llama-2-70B-chat），llama.cpp 比 GPTQ 速度更快。",-1);function Pa(ka,Qa){const n=t("ExternalLinkIcon");return r(),p("div",null,[i,d,u,a("p",null,[s("来自论文："),a("a",h,[s("LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"),e(n)])]),a("p",null,[s("LM.int8() 时 Hugingface 集成的"),a("a",g,[s("量化策略"),e(n)]),s("。能够通过在 .from_pretrain() 时候传递 load_in_8bit 来实现，针对几乎所有的 HF Transformers 模型都有效。大致方法是，在矩阵点积计算过程中， 将其中的 outliers 参数找出来（以行或列为单位），然后用类似 absolute maximum (absmax) quantization 的方法，根据行/列对 Regular 参数做量化处理，outlier 参数仍然做 fp16 计算，最后相加。")]),b,a("p",null,[s("根据 "),a("a",y,[s("huggingface 的博客"),e(n)]),s("， LLM.INT8() 能够再模型性能不影响很多的前提下，让我们能用更少的资源进行 LLM 推理。但 LLM.int8() 普遍的推理速度会比 fp16 慢。博客中指出，对于越小的模型， int8() 会导致更慢的速度。")]),m,f,_,v,a("p",null,[s("使用 GPTQ 量化的模型具有很大的速度优势，与 LLM.int8() 不同，GPTQ 要求对模型进行 post-training quantization，来得到量化权重。GPTQ 主要参考了 Optimal Brain Quanization (OBQ)，对 OBQ 方法进行了提速改进。有网友在 "),a("a",L,[s("文章"),e(n)]),s(" 中对 GPTQ, OBQ, OBS 等量化策略进行了整理，这里就不多赘述了。")]),a("p",null,[s("以下对几个 GPTQ 仓库进行介绍。以下所有测试均在 4090 上进行，模型推理速度采用 "),a("a",C,[s("oobabooga/text-generation-webui"),e(n)]),s(" 提供的 UI。")]),a("h3",E,[T,s(),a("a",G,[P,s(),e(n)])]),a("p",null,[s("专门针对 LLaMa 提供 GPTQ 量化方案的仓库，如果考虑 GPU 部署 LLaMa 模型的话，GPTQ-for-LLaMa 是十分指的参考的一个工具。像 huggingface.co 上的 "),a("a",k,[s("Thebloke"),e(n)]),s(" 很大部分模型都是采用 GPTQ-for-LLaMa 进行量化的。")]),a("p",null,[s("Post Training Quantization：GPTQ-for-LLaMa 默认采用 "),a("a",Q,[s("C4"),e(n)]),s(" 数据集进行量化训练（只采用了 C4 中英文数据的一部分进行量化，而非全部 9TB+的数据）：")]),M,a("p",null,[s("对量化模型在 MMLU 任务上"),a("a",x,[s("测试"),e(n)]),s("，量化后 MMLU 为，于 fp16（46.1）稍微有点差距。")]),a("p",null,[s("Huggingface 上的 "),a("a",q,[s("TheBloke"),e(n)]),s(" 发布的大部分 LLaMa GPTQ 模型，都是通过以上方式（C4 数据集 + wbit 4 + group 128 + no arc_order + true-sequential）量化的。若由于 GPTQ-for-LLaMa 及 transformers 仓库不断更新，Huggingface.co 上发布的模型可能存在无法加载或精度误差等问题，可以考虑重新量化，并通过优化量化数据集、添加 arc_order 等操作来提高量化精度。")]),F,a("ul",null,[a("li",null,[a("p",null,[s("模型加载问题：使用 gptq-for-llama 时，因 transformer 版本不同，可能出现模型加载不上问题。如加载 "),a("a",A,[s("TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ"),e(n)]),s(" 时，用最新版的 GPTQ-for-LLaMa 就会出现权重于模型 registry 名称不匹配的情况。")])]),B,a("li",null,[a("p",null,[s("经过测试，问题存在于 llama.py 中的 quant.make_quant_attn(model)。使用 quant_attn 能够极大提升模型推理速度。参考这个历史 ISSUE，估计是 position_id 的推理 cache 在 Attention layer 中的配置存在了问题。"),a("a",z,[s("left-padding issue"),e(n)])])]),a("li",null,[a("p",null,[s("GPTQ-for-LLaMa 版本变动大，如果其他仓库有使用 GPTQ-for-LLaMa 依赖的话，需要认真检查以下版本。如 "),a("a",I,[s("obbabooga"),e(n)]),s(" fork 了一个单独的 "),a("a",w,[s("GPTQ-for-LLaMa"),e(n)]),s(" 为 "),a("a",D,[s("oobabooga/text-generation-webui"),e(n)]),s(" 做支持。最新版的 GPTQ-for-LLaMa 在 text-generation-webui 中使用会有 BUG。")])])]),a("h3",U,[O,s(),a("a",S,[V,s(),e(n)])]),a("p",null,[s("AutoGPTQ 使用起来相对容易，它提供了对大多数 Huggingface LLM 模型的量化方案，如 LLaMa 架构系列模型，bloom，moss，falcon，gpt_bigcode 等。（没在支持表中看到 ChatGLM 系列模型）。具体可以参考 官方的 "),a("a",H,[s("快速上手"),e(n)]),s(" 和 "),a("a",N,[s("进阶使用"),e(n)]),s(" 来进行量化模型训练和部署。")]),R,a("h3",W,[j,s(),a("a",K,[Y,s(),e(n)])]),a("p",null,[a("a",Z,[s("exllama"),e(n)]),s(" 为了让 LLaMa 的 GPTQ 系列模型在 4090/3090 Ti 显卡上跑更快，推理平均能达到 140+ tokens/s。当然为了实现那么高的性能加速，exllama 中的模型移除了 HF transformers 模型的大部分依赖，这也导致如果在项目中使用 exllama 模型需要额外的适配工作。text-generation-webui 中对 exllama 进行了 HF 适配，使得我们能够像使用 HF 模型一样使用 exllama，代价是牺牲了一些性能，参考 "),a("a",J,[s("exllama_hf"),e(n)]),s("。")]),a("h3",X,[$,s(),a("a",aa,[s("gptq"),e(n)])]),sa,na,a("p",null,[a("a",ea,[s("GGML"),e(n)]),s(" 是一个机械学习架构，使用 C 编写，支持 Integer quantization（4-bit, 5-bit, 8-bit） 以及 16-bit float。同时也对部分硬件架构进行了加速优化。本章中讨论到的 LLaMa 量化加速方案来源于 "),a("a",oa,[s("LLaMa.cpp"),e(n)]),s(" 。LLaMa.cpp 有很多周边产品，如 "),a("a",la,[s("llama-cpp-python"),e(n)]),s(" 等，在下文中，我们以 GGML 称呼这类模型量化方案。")]),ta,a("p",null,[s("GGML 有不同的量化策略（"),a("a",ra,[s("具体量化类型参考"),e(n)]),s("），以下使用 Q4_0 对 LLaMa-2-13B-chat-hf 进行量化和测试。")]),a("p",null,[s("此处采用 "),a("a",pa,[s("docker with cuda"),e(n)]),s(" 部署，为方便自定义，先注释掉 "),ca,s(" 中的 "),a("a",ia,[s("EntryPoint"),e(n)]),s("。而后构建镜像：")]),da,a("p",null,[s("参考"),a("a",ua,[s("官方文档"),e(n)]),s("，进行权重转换即量化：")]),ha,a("p",null,[s("使用 llama.cpp server 时，具体参数解释参考"),a("a",ga,[s("官方文档"),e(n)]),s("。主要参数有：")]),ba,a("p",null,[s("以上所有测试均在 4090 + Inter i9-13900K 上进行，模型推理速度采用 "),a("a",ya,[s("oobabooga/text-generation-webui"),e(n)]),s(" 提供的 UI（text-generation-webui 的推理速度会比实际 API 部署慢一点）。这边只做速度测试，关于精度测试，可以查看 "),a("a",ma,[s("GPT-for-LLaMa result"),e(n)]),s(" 和 "),a("a",fa,[s("exllama results"),e(n)]),s("。")]),_a,a("ol",null,[a("li",null,[s("模型推理的速度受 GPU 即 CPU 的影响最大。有网友指出 "),a("a",va,[s("link"),e(n)]),s("，同样对于 4090，在 CPU 不同的情况下，7B LLaMa fp16 快的时候有 50 tokens/s，慢的时候能达到 23 tokens/s。")]),La,Ca,Ea,Ta,Ga])])}const qa=l(c,[["render",Pa],["__file","笔记llama_quant.html.vue"]]);export{qa as default};
