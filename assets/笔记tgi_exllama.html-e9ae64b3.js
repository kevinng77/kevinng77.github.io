import{_ as l}from"./plugin-vue_export-helper-c27b6911.js";import{r as t,o as e,c as p,a as s,b as n,d as o,f as r}from"./app-8f289594.js";const d={},c=s("p",null,"本文对 Text generation inference + exllama 的 LLaMa 量化服务方案进行单卡 4090 部署测试。",-1),i={href:"https://zhuanlan.zhihu.com/p/645732302",target:"_blank",rel:"noopener noreferrer"},y=r(`<p>在上期中我们提到了 TGI 和 vllm 的对比测试，在使用 vllm 和 TGI 对 float16 模型进行部署后，我们能够在单卡 4090 上达到 3.5+ request/秒的吞吐量。</p><p>就在几天前 TGI 优化了 exllama，基于之前量化 LLaMa 的测试，exllama 能在控制精度损失的情况下，将模型的推理速度提升。</p><p>以下参考 TGI 的官方手册对采用 AUTOGPTQ 量化后的 LLaMa v2 gptq4 权重进行部署:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">docker</span><span style="color:#24292E;"> </span><span style="color:#032F62;">run</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--rm</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--name</span><span style="color:#24292E;"> </span><span style="color:#032F62;">tgi</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--runtime=nvidia</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--gpus</span><span style="color:#24292E;"> </span><span style="color:#032F62;">all</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">-p</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">5001</span><span style="color:#032F62;">:5001</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">-v</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/home/kevin/models:/models</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">ghcr.io/huggingface/text-generation-inference:1.0.0</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--model-id</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/models/llama2-7b-chat-gptq-int4</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--hostname</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0.0</span><span style="color:#032F62;">.0.0</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--port</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">5001</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--max-concurrent-requests</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">256</span><span style="color:#24292E;">  </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--quantize</span><span style="color:#24292E;"> </span><span style="color:#032F62;">gptq</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--trust-remote-code</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--max-batch-total-tokens</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">30000</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--sharded</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">false</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--max-input-length</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1024</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--validation-workers</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">4</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，<code>llama2-7b-chat-gptq-int4</code> 量化采用 AUTOGPTQ 提供的示例量化代码进行量化，量化数据集选择 wikitext：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># git clone AUTOGPTQ 仓库后进入 \`examples/quantization\` 文件夹</span></span>
<span class="line"><span style="color:#6A737D;"># 修改以下 pretrained_model_dir 和 quantized_model_dir 选择用 Llama-2-7b-chat-hf 量化</span></span>
<span class="line"><span style="color:#6F42C1;">python</span><span style="color:#24292E;"> </span><span style="color:#032F62;">basic_usage_wikitext2.py</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>当然 TGI 和 GPTQ-for-LLaMa 也提供了 llama 量化脚本，但是对 llama v2 进行 GPTQ 量化时，AUTOGPTQ 的损失总是比较小， <strong>量化后的模型输出也更稳定一些</strong> ，原因未知。</p><p>目前 TGI 版本（1.0.0）对本地加载 exllama 模型仍有不少问题，如果遇到了 weight gptq_bits not found 的话，在模型文件夹下加入一个 safetensor，其中储存好 gptq_bits 和 group 就行。具体 exllama 权重加载逻辑可以看 <code>utils.weight.Wight</code>。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> torch</span></span>
<span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> safetensors.torch </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> save_file</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">tensors </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> {</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">&quot;gptq_bits&quot;</span><span style="color:#24292E;">: torch.tensor(</span><span style="color:#005CC5;">4</span><span style="color:#24292E;">),</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">&quot;gptq_groupsize&quot;</span><span style="color:#24292E;">: torch.tensor(</span><span style="color:#005CC5;">128</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">}</span></span>
<span class="line"><span style="color:#24292E;">save_file(tensors, </span><span style="color:#032F62;">&quot;/home/kevin/models/llama2-7b-chat-gptq-int4/gptq_config.safetensors&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>目前 TGI 中对量化权重的处理方法不是很兼容 AUTOGPTQ 等 GPTQ 采用的数据储存方式，但有几个 PR 中已经在对此优化。</p><h2 id="tgi-exllama-测试" tabindex="-1"><a class="header-anchor" href="#tgi-exllama-测试" aria-hidden="true">#</a> TGI + EXLLAMA 测试</h2><p>部署后，发送单一的请求进行速度测试：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;">###</span></span>
<span class="line"><span style="color:#6F42C1;">POST</span><span style="color:#24292E;"> </span><span style="color:#032F62;">http://127.0.0.1:5001/generate</span></span>
<span class="line"><span style="color:#6F42C1;">Content-Type:</span><span style="color:#24292E;"> </span><span style="color:#032F62;">application/json</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">{</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6F42C1;">&quot;inputs&quot;</span><span style="color:#005CC5;">:</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;Once a upon time,&quot;,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6F42C1;">&quot;parameters&quot;</span><span style="color:#005CC5;">:</span><span style="color:#24292E;">{&quot;</span><span style="color:#6F42C1;">max_new_tokens</span><span style="color:#6F42C1;">&quot;:100,&quot;</span><span style="color:#6F42C1;">tempareture</span><span style="color:#6F42C1;">&quot;:0.6}</span></span>
<span class="line"><span style="color:#6F42C1;">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>发送请求后 853ms 得到预测结果，平均 117.23 tokens/s。比 TGI + AUTOGPTQ （约 80 tokens/s）快。但还是不如 exllama 官方的推理速度（140+ tokens/s）。</p><p>通过 vllm 提供的 server benchmark 文件 <code>benchmark/benchmark_serving.py</code> 进行测试，</p><ul><li>测试数据集：ShareGPT_V3_unfiltered_cleaned_split.json</li><li>num prompt: 100 （随机从 ShareGPT 提供的用户和 GPT 对话数据当中，筛选 100 个问题进行测试）</li><li>默认设备为：单卡 4090 + inter i9-13900K。（采用 3070 测试的数据有标注）</li><li>request 间隔: 每个 request 发送的间隔。</li></ul><table><thead><tr><th></th><th>request 间隔（秒）</th><th>Throughtput (request/s)</th><th>average speed (tokens/s)</th><th>lowest speed (tokens/s)</th></tr></thead><tbody><tr><td>vllm</td><td>1</td><td>0.95</td><td>51</td><td>39.4</td></tr><tr><td>vllm</td><td>0.5</td><td>1.66</td><td>44.96</td><td>29.41</td></tr><tr><td>vllm</td><td>0.25</td><td>2.48</td><td>37.6</td><td>24.05</td></tr><tr><td>vllm</td><td>0.05</td><td>3.24</td><td>26.31</td><td>4.13</td></tr><tr><td>TGI float16</td><td>1</td><td>0.96</td><td>80.15</td><td>40.91</td></tr><tr><td>TGI float16</td><td>0.5</td><td>1.81</td><td>74.62</td><td>32.97</td></tr><tr><td>TGI float16</td><td>0.25</td><td>2.67</td><td>59.36</td><td>22.59</td></tr><tr><td>TGI float16</td><td>0.05</td><td>3.6</td><td>37.39</td><td>4.12</td></tr><tr><td>TGI EXLLAMA</td><td>1</td><td>1.01</td><td>131.87</td><td>70.47</td></tr><tr><td>TGI EXLLAMA</td><td>0.5</td><td>1.86</td><td>97.28</td><td>44.22</td></tr><tr><td>TGI EXLLAMA</td><td>0.25</td><td>2.91</td><td>59.70</td><td>16.66</td></tr><tr><td>TGI EXLLAMA</td><td>0.02</td><td>4.89</td><td>41.41</td><td>14.88</td></tr><tr><td>TGI Exllama (3070)</td><td>1</td><td>0.42</td><td>8.58</td><td>0.72</td></tr><tr><td>TGI Exllama (3070)</td><td>0.25</td><td>0.35</td><td>2.39</td><td>0.16</td></tr><tr><td>TGI Exllama (3070)</td><td>0.02</td><td>0.43</td><td>2.61</td><td>0.19</td></tr></tbody></table><p>TGI + exllama 使得我们能够在一些小显存上部署模型，如 3070 (8GB)。期待 TGI，vllm 等部署服务对量化部署 llm 的持续优化，让私有部署模型有更好的体验。</p>`,18);function m(u,v){const a=t("ExternalLinkIcon");return e(),p("div",null,[c,s("p",null,[n("上期内容："),s("a",i,[n("vllm vs TGI 部署 llama v2 7B 踩坑笔记"),o(a)])]),y])}const E=l(d,[["render",m],["__file","笔记tgi_exllama.html.vue"]]);export{E as default};
