import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as p,o as t,c,a as s,b as n,d as l,f as e}from"./app-ede5ce6f.js";const r={},i=s("h2",{id:"sdxl",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#sdxl","aria-hidden":"true"},"#"),n(" SDXL")],-1),d={href:"https://arxiv.org/abs/2307.01952",target:"_blank",rel:"noopener noreferrer"},y={href:"https://www.zhihu.com/people/bei-jing-de-wen-zhou-ren",target:"_blank",rel:"noopener noreferrer"},u=s("p",null,"https://zhuanlan.zhihu.com/p/643420260",-1),_=s("p",null,"总结来说，SDXL 相对于 SD 模型有以下改进：",-1),m={href:"https://github.com/mlfoundations/open_clip",target:"_blank",rel:"noopener noreferrer"},E={href:"https://github.com/openai/CLIP/tree/main",target:"_blank",rel:"noopener noreferrer"},v=s("li",null,"引入了尺寸和裁剪调节，以保留训练数据，防止其被丢弃，并更好地控制生成图像的裁剪方式。",-1),b=s("figure",null,[s("img",{src:"https://pic2.zhimg.com/80/v2-133be5225344e7881807328f6b7b4d05_1440w.webp",alt:"SDXL 论文截图",tabindex:"0",loading:"lazy"}),s("figcaption",null,"SDXL 论文截图")],-1),f=s("ol",{start:"3"},[s("li",null,[n("SD 对 LDM 的生成图流程做了改进，由 base, refiner, VAE 组成："),s("code",null,"base"),n(" 模型（也可以作为独立模型运行）生成图像作为输入，输入到 "),s("code",null,"refiner"),n(" 模型中，后者添加额外的高质量细节。")])],-1),C=s("figure",null,[s("img",{src:"https://pic3.zhimg.com/80/v2-28c9d29925f7e2e8b93ecd6b7b04b1c6_1440w.webp",alt:"SDXL 论文截图： SDXL 生成图片架构",tabindex:"0",loading:"lazy"}),s("figcaption",null,"SDXL 论文截图： SDXL 生成图片架构")],-1),h={href:"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0",target:"_blank",rel:"noopener noreferrer"},g=e(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> diffusers </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> DiffusionPipeline</span></span>
<span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> torch</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># load both base &amp; refiner</span></span>
<span class="line"><span style="color:#24292E;">base </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> DiffusionPipeline.from_pretrained(</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span style="color:#24292E;">, </span><span style="color:#E36209;">torch_dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">torch.float16, </span><span style="color:#E36209;">variant</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;fp16&quot;</span><span style="color:#24292E;">, </span><span style="color:#E36209;">use_safetensors</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span></span>
<span class="line"><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">base.to(</span><span style="color:#032F62;">&quot;cuda&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">refiner </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> DiffusionPipeline.from_pretrained(</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">&quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">text_encoder_2</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">base.text_encoder_2,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">vae</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">base.vae,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">torch_dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">torch.float16,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">use_safetensors</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">variant</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;fp16&quot;</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">refiner.to(</span><span style="color:#032F62;">&quot;cuda&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Define how many steps and what % of steps to be run on each experts (80/20) here</span></span>
<span class="line"><span style="color:#24292E;">n_steps </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">40</span></span>
<span class="line"><span style="color:#24292E;">high_noise_frac </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0.8</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">prompt </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;A majestic lion jumping from a big stone at night&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># run both experts</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> base(</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">prompt</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">prompt,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">num_inference_steps</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">n_steps,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">denoising_end</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">high_noise_frac,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">output_type</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;latent&quot;</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">).images</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> refiner(</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">prompt</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">prompt,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">num_inference_steps</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">n_steps,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">denoising_start</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">high_noise_frac,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">image</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">image,</span></span>
<span class="line"><span style="color:#24292E;">).images[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，base 与 refiner 各自的推理步数由参数 <code>high_noise_frac</code> 决定。</p><p>假设总推理步数为 40，然后 <code>high_noise_frac=0.8</code>，那么 base 模型只进行 32 步推理：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># StableDiffusionXLPipeline 对 timesteps 的裁剪梳理</span></span>
<span class="line"><span style="color:#D73A49;">if</span><span style="color:#24292E;"> denoising_end </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">and</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(denoising_end, </span><span style="color:#005CC5;">float</span><span style="color:#24292E;">) </span><span style="color:#D73A49;">and</span><span style="color:#24292E;"> denoising_end </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">and</span><span style="color:#24292E;"> denoising_end </span><span style="color:#D73A49;">&lt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">    discrete_timestep_cutoff </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">int</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">round</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.config.num_train_timesteps</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">-</span><span style="color:#24292E;"> (denoising_end </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.config.num_train_timesteps)</span></span>
<span class="line"><span style="color:#24292E;">        )</span></span>
<span class="line"><span style="color:#24292E;">    )</span></span>
<span class="line"><span style="color:#24292E;">    num_inference_steps </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">len</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">list</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">filter</span><span style="color:#24292E;">(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> ts: ts </span><span style="color:#D73A49;">&gt;=</span><span style="color:#24292E;"> discrete_timestep_cutoff, timesteps)))</span></span>
<span class="line"><span style="color:#24292E;">    timesteps </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> timesteps[:num_inference_steps]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>refiner 大概只进行约 8 步:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># StableDiffusionXLImg2ImgPipeline 对 timesteps 的裁剪梳理</span></span>
<span class="line"><span style="color:#D73A49;">if</span><span style="color:#24292E;"> denoising_start </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">    discrete_timestep_cutoff </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">int</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">round</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.config.num_train_timesteps</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">-</span><span style="color:#24292E;"> (denoising_start </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.config.num_train_timesteps)</span></span>
<span class="line"><span style="color:#24292E;">        )</span></span>
<span class="line"><span style="color:#24292E;">    )</span></span>
<span class="line"><span style="color:#24292E;">    timesteps </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">list</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">filter</span><span style="color:#24292E;">(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> ts: ts </span><span style="color:#D73A49;">&lt;</span><span style="color:#24292E;"> discrete_timestep_cutoff, timesteps))</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> torch.tensor(timesteps), </span><span style="color:#005CC5;">len</span><span style="color:#24292E;">(timesteps)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="sdxl-lora" tabindex="-1"><a class="header-anchor" href="#sdxl-lora" aria-hidden="true">#</a> SDXL Lora</h3><p>参考 https://github.com/bmaltais/kohya_ss#tips-for-sdxl-training 的训练配置，参考以下 kohya 的训练代码：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">export</span><span style="color:#24292E;"> TF_ENABLE_ONEDNN_OPTS</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0</span></span>
<span class="line"><span style="color:#6F42C1;">accelerate</span><span style="color:#24292E;"> </span><span style="color:#032F62;">launch</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--num_cpu_threads_per_process=2</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;/workspace/kohya_ss/sdxl_train_network.py&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--enable_bucket</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--pretrained_model_name_or_path=</span><span style="color:#032F62;">&quot;/workspace/models/stable-diffusion-xl-base-1.0/sd_xl_base_1.0.safetensors&quot;</span><span style="color:#24292E;">  </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--train_data_dir=</span><span style="color:#032F62;">&quot;/workspace/coffee_mini&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--resolution=</span><span style="color:#032F62;">&quot;1024,1024&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--output_dir=</span><span style="color:#032F62;">&quot;/workspace/stable-diffusion-webui/models/Lora&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--logging_dir=</span><span style="color:#032F62;">&quot;/workspace/coffee_sdxl/logs&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--network_alpha=</span><span style="color:#032F62;">&quot;8&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--network_dim=8</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--save_model_as=safetensors</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--network_module=networks.lora</span><span style="color:#24292E;">   </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--unet_lr=0.0004</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--output_name=</span><span style="color:#032F62;">&quot;sdxl_xiaokafei_mini_v1.0&quot;</span><span style="color:#24292E;">             </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--lr_scheduler_num_cycles=</span><span style="color:#032F62;">&quot;1&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--no_half_vae</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--learning_rate=</span><span style="color:#032F62;">&quot;0.0004&quot;</span><span style="color:#24292E;">        </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--lr_scheduler=</span><span style="color:#032F62;">&quot;constant&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--train_batch_size=</span><span style="color:#032F62;">&quot;2&quot;</span><span style="color:#24292E;">        </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--max_train_steps=</span><span style="color:#032F62;">&quot;400&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--save_every_n_epochs=</span><span style="color:#032F62;">&quot;1&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--mixed_precision=</span><span style="color:#032F62;">&quot;bf16&quot;</span><span style="color:#24292E;">    </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--save_precision=</span><span style="color:#032F62;">&quot;bf16&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--seed=</span><span style="color:#032F62;">&quot;1234&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--caption_extension=</span><span style="color:#032F62;">&quot;.txt&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--cache_latents</span><span style="color:#24292E;">     </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--optimizer_type=</span><span style="color:#032F62;">&quot;Adafactor&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--max_data_loader_n_workers=</span><span style="color:#032F62;">&quot;2&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--clip_skip=2</span><span style="color:#24292E;">     </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--bucket_reso_steps=64</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--xformers</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--bucket_no_upscale</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--noise_offset=0.0</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--tokenizer_cache_dir=</span><span style="color:#032F62;">&quot;/workspace/models/&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--optimizer_args</span><span style="color:#24292E;"> </span><span style="color:#032F62;">scale_parameter=False</span><span style="color:#24292E;"> </span><span style="color:#032F62;">relative_step=False</span><span style="color:#24292E;"> </span><span style="color:#032F62;">warmup_init=False</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--gradient_checkpointing</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--cache_latents_to_disk</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--cache_text_encoder_outputs</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--network_train_unet_only</span><span style="color:#24292E;"> </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在相同的训练素材下，即便近训练 <code>Unet</code>，也必须要开启 <code>gradient_checkpointing</code>。以上代码训练占用显存 10 GB，训练时长约 4 分钟。</p>`,10),D={href:"https://huggingface.co/docs/diffusers/training/lora",target:"_blank",rel:"noopener noreferrer"},A=s("h2",{id:"controlnet",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#controlnet","aria-hidden":"true"},"#"),n(" Controlnet")],-1),k={href:"https://arxiv.org/pdf/2302.05543.pdf",target:"_blank",rel:"noopener noreferrer"},x=e(`<figure><img src="https://pic4.zhimg.com/80/v2-59072b5b1fc68d75ac9c1121bb4c4a93_1440w.webp" alt="Controlnet 论文截图：Controlnet 模型架构，及推理示意图。" tabindex="0" loading="lazy"><figcaption>Controlnet 论文截图：Controlnet 模型架构，及推理示意图。</figcaption></figure><p>参考 huggingface diffusers 中 <code>StableDiffusionControlNetPipeline</code> 的实现，在每次进行 diffusion backward processing 时，controlnet text to image 大致可以表示为以下伪代码：</p><ol><li>首先计算出 SD Unet 所需的残差值 <code>down_block_res_samples</code> 及 <code>mid_block_res_sample</code>。（分别对应上图中 SD Middle Block 和 SD Decoder Block 对应的蓝色连线）</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 详细请参考 diffusers StableDiffusionControlNetPipeline __call__ 方法</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">down_block_res_samples, mid_block_res_sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.controlnet(</span></span>
<span class="line"><span style="color:#24292E;">                    control_model_input,  </span><span style="color:#6A737D;"># latent (input z)</span></span>
<span class="line"><span style="color:#24292E;">                    t,  </span><span style="color:#6A737D;"># Time T encoder hidden state</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">encoder_hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">controlnet_prompt_embeds,  </span><span style="color:#6A737D;"># prompt encoder hidden state</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">controlnet_cond</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">image,  </span><span style="color:#6A737D;"># 如用 canny 图对应的 hidden state</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">conditioning_scale</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">cond_scale,  </span><span style="color:#6A737D;"># multi-controlnet 时用来控制权重的参数</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">guess_mode</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">guess_mode,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">return_dict</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                )</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>此外参考 <code>huggingface diffusers </code> <code>MultiControlNetModel</code> 的推理过程，在有多个 controlnet 情况下，<code>down_block_res_samples</code> 以及 <code> mid_block_res_sample</code> 则为所有风格的 controlnet 输出加和，如下伪代码，我们如果选择了对一张图片进行 reference only，而后对另一张进行 canny 控制，那么最后的<code>mid_block_res_sample</code> 即为两个不同 controlnet 输出 <code>mid_sample</code> 的总和:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;"> </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> i, (image, scale, controlnet) </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">enumerate</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">zip</span><span style="color:#24292E;">(controlnet_cond, conditioning_scale, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.nets)):</span></span>
<span class="line"><span style="color:#24292E;">            down_samples, mid_sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> controlnet(</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">sample</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">sample,</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#6A737D;"># other params... )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># merge samples</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> i </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                down_block_res_samples, </span><span style="color:#E36209;">mid_block_res_sample</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> down_samples, mid_sample</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">down_block_res_samples</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> [</span></span>
<span class="line"><span style="color:#24292E;">                    samples_prev </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> samples_curr</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> samples_prev, samples_curr </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">zip</span><span style="color:#24292E;">(down_block_res_samples, down_samples)</span></span>
<span class="line"><span style="color:#24292E;">                ]</span></span>
<span class="line"><span style="color:#24292E;">                mid_block_res_sample </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> mid_sample</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>使用 Unet2DConditional 对噪声进行预测。</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 详细请参考 diffusers StableDiffusionControlNetPipeline __call__ 方法</span></span>
<span class="line"><span style="color:#24292E;">noise_pred </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.unet(</span></span>
<span class="line"><span style="color:#24292E;">                    latent_model_input,</span></span>
<span class="line"><span style="color:#24292E;">                    t,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">encoder_hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">prompt_embeds,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">cross_attention_kwargs</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">cross_attention_kwargs,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">down_block_additional_residuals</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">down_block_res_samples,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">mid_block_additional_residual</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">mid_block_res_sample,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">return_dict</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                )[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>参考 huggingface diffusers <code>unet_2d_blocks.py</code> 中 <code>CrossAttnDownBlock2D</code> 的实现，controlnet 对残差使用求和处理，而非张量拼接。</p><p>controlnet 原作者给出了一些 controlnet 模型权重： https://huggingface.co/lllyasviel/sd_control_collection/tree/main</p>`,10),q={href:"https://huggingface.co/docs/diffusers/using-diffusers/controlnet",target:"_blank",rel:"noopener noreferrer"},w=s("p",null,"在 stablediffusoin webui 中，可以使用 https://github.com/Mikubill/sd-webui-controlnet 插件。",-1),F=s("h2",{id:"参考",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#参考","aria-hidden":"true"},"#"),n(" 参考")],-1),L=s("p",null,"https://huggingface.co/docs/diffusers/using-diffusers/sdxl",-1),S={href:"https://huggingface.co/papers/2307.01952",target:"_blank",rel:"noopener noreferrer"},z={href:"https://arxiv.org/abs/2307.01952",target:"_blank",rel:"noopener noreferrer"};function X(N,I){const a=p("ExternalLinkIcon");return t(),c("div",null,[i,s("p",null,[n("来自论文："),s("a",d,[n("SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"),l(a)])]),s("p",null,[n("SDXL 不论在模型架构还是 diffusion pipeline 上都与 SD 不同。十分推荐 "),s("a",y,[n("Rocky Ding"),l(a)]),n(" 的 SDXL 分享：")]),u,_,s("ol",null,[s("li",null,[n("模型更大：UNet 是原先的 3 倍大（从参数量来看）。Text Encoder 部分，采用了 "),s("a",m,[n("OpenCLIP-ViT/G"),l(a)]),n(" and "),s("a",E,[n("CLIP-ViT/L"),l(a)]),n(" 作为 text encoding。")]),v]),b,f,C,s("p",null,[n("参考 "),s("a",h,[n("HF diffusers"),l(a)]),n(" 的代码实现：")]),g,s("p",null,[n("除了 kohya 外，SDXL LORA 训练也可以参考："),s("a",D,[n("huggingface train lora guide"),l(a)])]),A,s("p",null,[s("a",k,[n("Adding Conditional Control to Text-to-Image Diffusion Models"),l(a)])]),x,s("p",null,[n("在 huggingface diffuser 中，调用 controlnet 可以参考"),s("a",q,[n("该连接"),l(a)]),n("。")]),w,F,L,s("p",null,[s("a",S,[n("SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"),l(a)])]),s("p",null,[s("a",z,[n("SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"),l(a)])])])}const M=o(r,[["render",X],["__file","笔记sdxl.html.vue"]]);export{M as default};
