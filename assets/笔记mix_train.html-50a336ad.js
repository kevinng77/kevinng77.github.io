import{_ as e}from"./plugin-vue_export-helper-c27b6911.js";import{r as i,o as p,c,a as s,b as a,d as n,f as t}from"./app-e29b0bb8.js";const r="/assets/img/mix_train/image-20220625100347436.png",m="/assets/img/mix_train/image-20220625103353074.png",o="/assets/img/mix_train/image-20220625103730307.png",h={},g={href:"https://arxiv.org/pdf/1710.03740.pdf",target:"_blank",rel:"noopener noreferrer"},d=t(`<h2 id="快速开始" tabindex="-1"><a class="header-anchor" href="#快速开始" aria-hidden="true">#</a> 快速开始</h2><p>在 AMP （自动混合精度训练）提出的一开始，大家使用的都是 NVIDIA 的 apex 库实现。后来各大深度学习平台纷纷添加了自带的 AMP API，如 tf，torch，paddle 等。</p><p>在 apex 中：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> apex </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> amp</span></span>
<span class="line"><span style="color:#24292E;">model, optimizer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> amp.initialize(model, optimizer, </span><span style="color:#E36209;">opt_level</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;O1&quot;</span><span style="color:#24292E;">) </span><span style="color:#6A737D;"># 这里是“欧一”，不是“零一”</span></span>
<span class="line"><span style="color:#D73A49;">with</span><span style="color:#24292E;"> amp.scale_loss(loss, optimizer) </span><span style="color:#D73A49;">as</span><span style="color:#24292E;"> scaled_loss:</span></span>
<span class="line"><span style="color:#24292E;">    scaled_loss.backward()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在 torch 中:：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">with</span><span style="color:#24292E;"> torch.cuda.amp.autocast():</span></span>
<span class="line"><span style="color:#24292E;">    output </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> net(</span><span style="color:#005CC5;">input</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">    loss </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> loss_fn(output, target)</span></span>
<span class="line"><span style="color:#24292E;">scaler.scale(loss).backward()</span></span>
<span class="line"><span style="color:#24292E;">scaler.step(optimizer)</span></span>
<span class="line"><span style="color:#24292E;">scaler.update()</span></span>
<span class="line"><span style="color:#24292E;">optimizer.zero_grad()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在 paddle 中：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> paddle.amp </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> GradScaler, auto_cast</span></span>
<span class="line"><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.fp16:</span></span>
<span class="line"><span style="color:#24292E;">            scaler </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> GradScaler(</span><span style="color:#E36209;">init_loss_scaling</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scale_loss)</span></span>
<span class="line"><span style="color:#D73A49;">with</span><span style="color:#24292E;"> auto_cast(</span><span style="color:#E36209;">enable</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.fp16,</span></span>
<span class="line"><span style="color:#24292E;">               </span><span style="color:#E36209;">custom_white_list</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">[</span><span style="color:#032F62;">&quot;softmax&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;gelu&quot;</span><span style="color:#24292E;">]):  </span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 此处可以选择混合精度模式 O1,O2 等，默认仅对部分线性层进行 FP16 转换，可以添加白名单来支持其他层的 FP16</span></span>
<span class="line"><span style="color:#24292E;">    loss </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> model(</span><span style="color:#D73A49;">**</span><span style="color:#24292E;">inputs)</span></span>
<span class="line"><span style="color:#24292E;">    loss </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> loss </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.gradient_accumulation_steps</span></span>
<span class="line"><span style="color:#24292E;">    losses </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> loss.detach()  </span><span style="color:#6A737D;"># losses for logging only</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.fp16:</span></span>
<span class="line"><span style="color:#24292E;">        scaled </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> scaler.scale(loss)</span></span>
<span class="line"><span style="color:#24292E;">        scaled.backward()</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">        loss.backward()    </span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> step </span><span style="color:#D73A49;">%</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.gradient_accumulation_steps </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">        global_step </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.fp16:</span></span>
<span class="line"><span style="color:#24292E;">            scaler.step(optimizer)</span></span>
<span class="line"><span style="color:#24292E;">            scaler.update()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            optimizer.step()</span></span>
<span class="line"><span style="color:#24292E;">            optimizer.clear_grad()</span></span>
<span class="line"><span style="color:#24292E;">            lr_scheduler.step()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="三个要点" tabindex="-1"><a class="header-anchor" href="#三个要点" aria-hidden="true">#</a> 三个要点</h2><p>混合精度意在提高计算密集型任务的效率，因此模型训练的 IO 瓶颈是无法通过该方案解决的。混合精度训练能够极大的提高模型训练速度，同时保留几乎 99%的训练精度。</p><h4 id="精度在哪丢失" tabindex="-1"><a class="header-anchor" href="#精度在哪丢失" aria-hidden="true">#</a> 精度在哪丢失</h4><p><strong>计算方式</strong></p>`,12),u={href:"https://en.wikipedia.org/wiki/Half-precision_floating-point_format",target:"_blank",rel:"noopener noreferrer"},y=s("figure",null,[s("img",{src:r,alt:"相关图片",height:"300",tabindex:"0",loading:"lazy"}),s("figcaption",null,"相关图片")],-1),b=s("p",null,"FP16 的计算方式如下：",-1),v=s("thead",null,[s("tr",null,[s("th",{style:{"text-align":"center"}},"Exponent"),s("th",{style:{"text-align":"center"}},"Significand = zero"),s("th",{style:{"text-align":"center"}},"Significand ≠ zero"),s("th",{style:{"text-align":"center"}},"Equation")])],-1),x=s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"0000"),s("msub",null,[s("mn",null,"0"),s("mn",null,"2")])]),s("annotation",{encoding:"application/x-tex"},"00000_2")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7944em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},"0000"),s("span",{class:"mord"},[s("span",{class:"mord"},"0"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])])],-1),f={style:{"text-align":"center"}},_={href:"https://en.wikipedia.org/wiki/0_(number)",target:"_blank",rel:"noopener noreferrer"},w={href:"https://en.wikipedia.org/wiki/%E2%88%920",target:"_blank",rel:"noopener noreferrer"},k={style:{"text-align":"center"}},z={href:"https://en.wikipedia.org/wiki/Subnormal_numbers",target:"_blank",rel:"noopener noreferrer"},E=s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",{stretchy:"false"},"("),s("mo",null,"−"),s("mn",null,"1"),s("msup",null,[s("mo",{stretchy:"false"},")"),s("mrow",null,[s("mi",null,"s"),s("mi",null,"i"),s("mi",null,"g"),s("mi",null,"n"),s("mi",null,"b"),s("mi",null,"i"),s("mi",null,"t")])]),s("mo",null,"×"),s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"14")])]),s("mo",null,"×"),s("mn",null,"0."),s("mi",null,"s"),s("mi",null,"i"),s("mi",null,"g"),s("mi",null,"n"),s("mi",null,"i"),s("mi",null,"f"),s("mi",null,"i"),s("mi",null,"c"),s("mi",null,"a"),s("mi",null,"n"),s("mi",null,"t"),s("mi",null,"b"),s("mi",null,"i"),s("mi",null,"t"),s("msub",null,[s("mi",null,"s"),s("mn",null,"2")])]),s("annotation",{encoding:"application/x-tex"},"(-1)^{signbit} × 2^{-14} × 0.significantbits_2")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0991em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"−"),s("span",{class:"mord"},"1"),s("span",{class:"mclose"},[s("span",{class:"mclose"},")"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8491em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"s"),s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"g"),s("span",{class:"mord mathnormal mtight"},"nbi"),s("span",{class:"mord mathnormal mtight"},"t")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"14")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},"0."),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),s("span",{class:"mord mathnormal"},"ni"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mord mathnormal"},"c"),s("span",{class:"mord mathnormal"},"an"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"bi"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])])],-1),P=s("tr",null,[s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"0000"),s("msub",null,[s("mn",null,"1"),s("mn",null,"2")]),s("mo",{separator:"true"},","),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},"."),s("mo",{separator:"true"},","),s("mn",null,"1111"),s("msub",null,[s("mn",null,"0"),s("mn",null,"2")])]),s("annotation",{encoding:"application/x-tex"},"00001_2, ..., 11110_2")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8389em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},"0000"),s("span",{class:"mord"},[s("span",{class:"mord"},"1"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},"..."),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},"1111"),s("span",{class:"mord"},[s("span",{class:"mord"},"0"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])])]),s("td",{style:{"text-align":"center"}},"normalized value"),s("td",{style:{"text-align":"center"}},"(−1)signbit × 2exponent−15 × 1.significantbits2"),s("td",{style:{"text-align":"center"}})],-1),F=s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"1111"),s("msub",null,[s("mn",null,"1"),s("mn",null,"2")])]),s("annotation",{encoding:"application/x-tex"},"11111_2")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7944em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},"1111"),s("span",{class:"mord"},[s("span",{class:"mord"},"1"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])])],-1),M={style:{"text-align":"center"}},A={href:"https://en.wikipedia.org/wiki/Infinity",target:"_blank",rel:"noopener noreferrer"},D={style:{"text-align":"center"}},O={href:"https://en.wikipedia.org/wiki/NaN",target:"_blank",rel:"noopener noreferrer"},C=s("td",{style:{"text-align":"center"}},null,-1),L=s("p",null,"通过几个例子说明：",-1),q=s("table",null,[s("thead",null,[s("tr",null,[s("th",{style:{"text-align":"center"}},"Binary"),s("th",{style:{"text-align":"center"}},"Hex"),s("th",{style:{"text-align":"center"}},"Value"),s("th",{style:{"text-align":"center"}},"Notes")])]),s("tbody",null,[s("tr",null,[s("td",{style:{"text-align":"center"}},"0 00000 0000000000"),s("td",{style:{"text-align":"center"}},"0000"),s("td",{style:{"text-align":"center"}},"0"),s("td",{style:{"text-align":"center"}})]),s("tr",null,[s("td",{style:{"text-align":"center"}},"0 00000 0000000001"),s("td",{style:{"text-align":"center"}},"0001"),s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"14")])]),s("mo",null,"×"),s("mo",{stretchy:"false"},"("),s("mn",null,"0"),s("mo",null,"+"),s("mn",null,"1"),s("mi",{mathvariant:"normal"},"/"),s("mn",null,"1024"),s("mo",{stretchy:"false"},")"),s("mo",null,"≈"),s("mn",null,"0.000000059604645")]),s("annotation",{encoding:"application/x-tex"},"2^{-14} × (0 + 1/1024 ) \\approx 0.000000059604645")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"14")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"0"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"1/1024"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.000000059604645")])])])]),s("td",{style:{"text-align":"center"}},"smallest positive subnormal number")]),s("tr",null,[s("td",{style:{"text-align":"center"}},"0 00000 1111111111"),s("td",{style:{"text-align":"center"}},"03ff"),s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"14")])]),s("mo",null,"×"),s("mo",{stretchy:"false"},"("),s("mn",null,"0"),s("mo",null,"+"),s("mn",null,"1023"),s("mi",{mathvariant:"normal"},"/"),s("mn",null,"1024"),s("mo",{stretchy:"false"},")"),s("mo",null,"≈"),s("mn",null,"0.000060975552")]),s("annotation",{encoding:"application/x-tex"},"2^{-14} × (0 + 1023/1024 ) \\approx 0.000060975552")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"14")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"0"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"1023/1024"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.000060975552")])])])]),s("td",{style:{"text-align":"center"}},"largest subnormal number")]),s("tr",null,[s("td",{style:{"text-align":"center"}},"0 00001 0000000000"),s("td",{style:{"text-align":"center"}},"0400"),s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"14")])]),s("mo",null,"×"),s("mo",{stretchy:"false"},"("),s("mn",null,"1"),s("mo",null,"+"),s("mn",null,"0"),s("mi",{mathvariant:"normal"},"/"),s("mn",null,"1024"),s("mo",{stretchy:"false"},")"),s("mo",null,"≈"),s("mn",null,"0.00006103515625")]),s("annotation",{encoding:"application/x-tex"},"2^{-14} × (1 + 0/1024 ) \\approx 0.00006103515625")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"14")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"0/1024"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.00006103515625")])])])]),s("td",{style:{"text-align":"center"}},"smallest positive normal number")]),s("tr",null,[s("td",{style:{"text-align":"center"}},"0 01101 0101010101"),s("td",{style:{"text-align":"center"}},"3555"),s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"2")])]),s("mo",null,"×"),s("mo",{stretchy:"false"},"("),s("mn",null,"1"),s("mo",null,"+"),s("mn",null,"341"),s("mi",{mathvariant:"normal"},"/"),s("mn",null,"1024"),s("mo",{stretchy:"false"},")"),s("mo",null,"≈"),s("mn",null,"0.33325195")]),s("annotation",{encoding:"application/x-tex"},"2^{-2} × (1 + 341/1024 ) \\approx 0.33325195")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"2")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"341/1024"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.33325195")])])])]),s("td",{style:{"text-align":"center"}},"nearest value to 1/3")]),s("tr",null,[s("td",{style:{"text-align":"center"}},"0 01110 1111111111"),s("td",{style:{"text-align":"center"}},"3bff"),s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"1")])]),s("mo",null,"×"),s("mo",{stretchy:"false"},"("),s("mn",null,"1"),s("mo",null,"+"),s("mn",null,"1023"),s("mi",{mathvariant:"normal"},"/"),s("mn",null,"1024"),s("mo",{stretchy:"false"},")"),s("mo",null,"≈"),s("mn",null,"0.99951172")]),s("annotation",{encoding:"application/x-tex"},"2^{-1} × (1 + 1023/1024 ) \\approx 0.99951172")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"1")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"1023/1024"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.99951172")])])])]),s("td",{style:{"text-align":"center"}},"largest number less than one")]),s("tr",null,[s("td",{style:{"text-align":"center"}},"0 01111 0000000000"),s("td",{style:{"text-align":"center"}},"3c00"),s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mn",null,"0")]),s("mo",null,"×"),s("mo",{stretchy:"false"},"("),s("mn",null,"1"),s("mo",null,"+"),s("mn",null,"0"),s("mi",{mathvariant:"normal"},"/"),s("mn",null,"1024"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mn",null,"1")]),s("annotation",{encoding:"application/x-tex"},"2^0 × (1 + 0/1024 ) = 1")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"0")])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"0/1024"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"1")])])])]),s("td",{style:{"text-align":"center"}},"one")]),s("tr",null,[s("td",{style:{"text-align":"center"}},"0 01111 0000000001"),s("td",{style:{"text-align":"center"}},"3c01"),s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mn",null,"0")]),s("mo",null,"×"),s("mo",{stretchy:"false"},"("),s("mn",null,"1"),s("mo",null,"+"),s("mn",null,"1"),s("mi",{mathvariant:"normal"},"/"),s("mn",null,"1024"),s("mo",{stretchy:"false"},")"),s("mo",null,"≈"),s("mn",null,"1.00097656")]),s("annotation",{encoding:"application/x-tex"},"2^0 × (1 + 1/1024 ) \\approx 1.00097656")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"0")])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"1/1024"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"1.00097656")])])])]),s("td",{style:{"text-align":"center"}},"smallest number larger than one")]),s("tr",null,[s("td",{style:{"text-align":"center"}},"0 11110 1111111111"),s("td",{style:{"text-align":"center"}},"7bff"),s("td",{style:{"text-align":"center"}},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mn",null,"1")]),s("mn",null,"5"),s("mo",null,"×"),s("mo",{stretchy:"false"},"("),s("mn",null,"1"),s("mo",null,"+"),s("mn",null,"1023"),s("mi",{mathvariant:"normal"},"/"),s("mn",null,"1024"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mn",null,"65504")]),s("annotation",{encoding:"application/x-tex"},"2^15 × (1 + 1023/1024 ) = 65504")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])])])])])]),s("span",{class:"mord"},"5"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"1023/1024"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"65504")])])])]),s("td",{style:{"text-align":"center"}},"largest normal number")]),s("tr",null,[s("td",{style:{"text-align":"center"}},"0 11111 0000000000"),s("td",{style:{"text-align":"center"}},"7c00"),s("td",{style:{"text-align":"center"}},"∞"),s("td",{style:{"text-align":"center"}},"infinity")]),s("tr",null,[s("td",{style:{"text-align":"center"}},"1 00000 0000000000"),s("td",{style:{"text-align":"center"}},"8000"),s("td",{style:{"text-align":"center"}},"−0"),s("td",{style:{"text-align":"center"}})]),s("tr",null,[s("td",{style:{"text-align":"center"}},"1 10000 0000000000"),s("td",{style:{"text-align":"center"}},"c000"),s("td",{style:{"text-align":"center"}},"-2"),s("td",{style:{"text-align":"center"}})]),s("tr",null,[s("td",{style:{"text-align":"center"}},"1 11111 0000000000"),s("td",{style:{"text-align":"center"}},"fc00"),s("td",{style:{"text-align":"center"}},"−∞"),s("td",{style:{"text-align":"center"}},"negative infinity")])])],-1),I=s("p",null,"从上表不难发现， FP16 的计算存在几个默认规则：",-1),N=s("ul",null,[s("li",null,"exponent 或 fraction 为 0 时 存在特定计算方式。"),s("li",null,"当 exponent 不为 0 时，默认 fraction 部分有一个 1。这是后 fraction 部分的总精度将是 11 个 bit 而非图上给的 10 bit。")],-1),G=s("p",null,[s("strong",null,"舍入误差")],-1),S=s("p",null,"由于浮点数的特性，FP16 在两个相邻的，能够被 FP16 表达的数值之间存在一定的间隔，当计算数值存在于间隔之中时，运算将会出现舍入误差。",-1),V=s("p",null,[a("如 FP16 中 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"3")])]),s("mo",null,"+"),s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"14")])]),s("mo",null,"="),s("mn",null,"0.125")]),s("annotation",{encoding:"application/x-tex"},"2^{-3} + 2^{-14}=0.125")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"3")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"14")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.125")])])]),a("，而 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"3")])]),s("mo",null,"+"),s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"13")])]),s("mo",null,"="),s("mn",null,"0.1251")]),s("annotation",{encoding:"application/x-tex"},"2^{-3}+2^{-13}=0.1251")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8974em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"3")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"13")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.1251")])])]),a("，由于 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"3")])])]),s("annotation",{encoding:"application/x-tex"},"2^{-3}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"3")])])])])])])])])])])]),a(" 的最小间隔为 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"13")])])]),s("annotation",{encoding:"application/x-tex"},"2^{-13}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"13")])])])])])])])])])])]),a("，因此 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"14")])])]),s("annotation",{encoding:"application/x-tex"},"2^{-14}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"14")])])])])])])])])])])]),a(" 将在这次相加中丢失。")],-1),B=t('<p><strong>精度溢出</strong></p><p>FP16 取值范围是 5.96× 10−8 ~ 65504，而 FP32 则是 1.4×10-45 ~ 3.4×1038。由 FP32 转到 FP16 存在下溢出与上溢出。对于下溢出，数值将被置为 0；对于上溢出，可能出现 NAN 或者无穷数。</p><figure><img src="'+m+'" alt="相关图片" height="300" tabindex="0" loading="lazy"><figcaption>相关图片</figcaption></figure><p>上图为 SSD 训练过程中激活函数数值分布图。可以发现大部分的计算数值在 FP16 中会被置 0。</p><p>如果直接将 FP32 的模型全部采用 FP16 保存，那么模型的训练效果将大打折扣。因此，混合精度训练的要点在于，如何最小地避免精度丢失：</p><h3 id="保存模型的-fp32-主权重副本" tabindex="-1"><a class="header-anchor" href="#保存模型的-fp32-主权重副本" aria-hidden="true">#</a> <strong>保存模型的 FP32 主权重副本</strong></h3><figure><img src="'+o+'" alt="相关图片" height="300" tabindex="0" loading="lazy"><figcaption>相关图片</figcaption></figure><p>如上图所示，在传导过程中使用 FP16，然后使用 FP32 接受更新的梯度以及保存模型，因此我们需要同时保存模型 fp32 以及 fp16 的版本。</p><p>尽管保存着两种权重样本，这理论上会增加 50%的显存占用。但实际上，我们训练起来后会发现显存占用大大减少，因为训练中大部分的显存占用存在于激活函数的计算中。</p><p>由于混合精度训练 O1 模式下通常可以指定白名单，因此笔者在训练某 NLP Transformer Encoder 架构模型时，对激活函数进行了混合精度训练测试，下面为测试结果。</p><p>当我们将 softmax 以及 gelu 等激活函数采用 FP16 计算时，显存才开始大大降低。当仅对线性层等采用 fp16 计算后，虽然显存没有明显的降低，但计算速度也得到了很大的提高。</p><table><thead><tr><th>是否混合精度训练</th><th>白名单（额外应用 fp16 的层）</th><th>time/batch</th><th>显存占用/batch</th></tr></thead><tbody><tr><td>否</td><td>-</td><td>1.875s</td><td>1.68GB</td></tr><tr><td>是</td><td>-</td><td>0.9375s</td><td>1.625GB</td></tr><tr><td>是</td><td>softmax</td><td>0.565s</td><td>1.3GB</td></tr><tr><td>是</td><td>softmax + gelu</td><td>0.55s</td><td>1.1GB</td></tr></tbody></table><h3 id="loss-scaling" tabindex="-1"><a class="header-anchor" href="#loss-scaling" aria-hidden="true">#</a> <strong>loss-scaling</strong></h3><p>再看看精度溢出中的激活函数数值分布图。如果我们在反向传播之前，将 FP32 的 <code>loss</code> 乘以 <code>scaler_factor</code>，那么我们能够保证反向传播时，大部分的数值保持在 FP16 的范围内，也就是红线右边。通常这个 <code>scaler_factor</code> 在 8-32K 之间，或者更大。较大的 <code>scaler_factor</code> 是没问题的，只要不出现 FP16 的 overflowing 问题即可。</p><h3 id="改进算数方法" tabindex="-1"><a class="header-anchor" href="#改进算数方法" aria-hidden="true">#</a> <strong>改进算数方法</strong></h3>',15),T=s("p",null,[a("计算可以分为 reduction, point-wise operations, vector dot-production 三种，前两种操作主要受内存带宽限制，因此采用 FP16 或者 FP32 影响不是很大。对于后者，部分的模型需要采用 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"F"),s("mi",null,"P"),s("mn",null,"16"),s("mo",null,"∗"),s("mi",null,"F"),s("mi",null,"P"),s("mn",null,"16"),s("mo",null,"+"),s("mi",null,"F"),s("mi",null,"P"),s("mn",null,"32")]),s("annotation",{encoding:"application/x-tex"},"FP16 * FP16 + FP32")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"FP"),s("span",{class:"mord"},"16"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"∗"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7667em","vertical-align":"-0.0833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"FP"),s("span",{class:"mord"},"16"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"FP"),s("span",{class:"mord"},"32")])])]),a(" 来保留精度。原文描述是这样的：")],-1),R=t('<blockquote><p>To maintain model accuracy, we found that some networks require that FP16 vector dot-product accumulates the partial products into an FP32 value, which is converted to FP16 before writing to memory. Without this accumulation in FP32, some FP16 models did not match the accuracy of the baseline models.</p></blockquote><h2 id="训练要点" tabindex="-1"><a class="header-anchor" href="#训练要点" aria-hidden="true">#</a> 训练要点</h2><h4 id="训练模式" tabindex="-1"><a class="header-anchor" href="#训练模式" aria-hidden="true">#</a> 训练模式</h4><p>混合精度训练通常有 O0，O1, O2, O3 模式。O0 为全 FP32 训练，O3 为全 FP16 训练，常用的是 O1, O2。</p><p>NVIDIA AMP 的默认策略就是 O2：除了 batch norm 和输入采用 FP32，其余均为 FP16。因此需要额外一个 FP32 权重来实现梯度更新。</p><p>O1 模式笔者认为更方便，其提供了黑白名单，能让设计者根据自己模型的特点来选择需要进行 FP16 的部分，如 softmax，layernorm 等。通常 O1 模式会有默认的白名单，如最常用的线性层就在白名单中。白名单中添加的模型层将强制采用 FP16 计算，黑名单中强制使用 FP32，对于剩下的层，将根据对应的输入进行判断，若输入有一个 FP32 则使用 FP32。</p><h3 id="o1-伪代码" tabindex="-1"><a class="header-anchor" href="#o1-伪代码" aria-hidden="true">#</a> O1 伪代码</h3>',7),H={href:"https://github.com/NVIDIA/apex/tree/1403c21acf87b0f2245278309071aef17d80c13b/apex/amp",target:"_blank",rel:"noopener noreferrer"},X={href:"https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247576950&idx=2&sn=bf5e3727c688ded3008774910232714c&chksm=96eb72f6a19cfbe0867df484d8267803867b6c34b22025e4199c66ef2e36218075425043f32c&mpshare=1&scene=24&srcid=0622dEcRq0ki5FPffp4Grf4o&sharer_sharetime=1655876508786&sharer_shareid=aa88399bd33117178ca30f3bb172ce11#rd",target:"_blank",rel:"noopener noreferrer"},K=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> apex </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> amp</span></span>
<span class="line"><span style="color:#24292E;">model, optimizer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> amp.initialize(model, optimizer, </span><span style="color:#E36209;">opt_level</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;O1&quot;</span><span style="color:#24292E;">) </span></span>
<span class="line"><span style="color:#D73A49;">with</span><span style="color:#24292E;"> amp.scale_loss(loss, optimizer) </span><span style="color:#D73A49;">as</span><span style="color:#24292E;"> scaled_loss:</span></span>
<span class="line"><span style="color:#24292E;">    scaled_loss.backward()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>以上代码的运行逻辑大致为：</p><p><code>amp.initialize()</code>：根据黑白名单对 PyTorch 内置的函数进行包装 [4]。白名单函数强制 FP16，黑名单函数强制 FP32。其余函数则根据参数类型自动判断，如果参数都是 FP16，则以 FP16 运行，如果有一个参数为 FP32，则以 FP32 运行。</p><p>对于每次迭代（调用 <code>scaled_loss.backward()</code>）：</p><ol><li>前向传播：模型权重是 FP32，按照黑白名单自动选择算子精度。</li><li>将 loss 乘以 <code>loss_scale</code></li><li>反向传播，因为模型权重是 FP32，所以即使函数以 FP16 运行，也会得到 FP32 的梯度。</li><li>将梯度 unscale，即除以 loss_scale</li><li>如果检测到 inf 或 nan <code>loss_scale /= 2</code>，而后跳过此次更新。</li><li><code>optimizer.step()</code>，执行此次更新</li><li>如果连续 2000 次迭代都没有出现 inf 或 nan，则 <code>loss_scale *= 2</code></li></ol><p>O2 与 O1 主要差别在于初始化方式，以及 O2 维护了额外的模型副本进行梯度更新。</p><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考" aria-hidden="true">#</a> 参考</h2>`,7),W={href:"https://zhuanlan.zhihu.com/p/79887894",target:"_blank",rel:"noopener noreferrer"},j={href:"https://zhuanlan.zhihu.com/p/103685761",target:"_blank",rel:"noopener noreferrer"},J={href:"https://zhuanlan.zhihu.com/p/84219777",target:"_blank",rel:"noopener noreferrer"},Q={href:"https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247576950&idx=2&sn=bf5e3727c688ded3008774910232714c&chksm=96eb72f6a19cfbe0867df484d8267803867b6c34b22025e4199c66ef2e36218075425043f32c&mpshare=1&scene=24&srcid=0622dEcRq0ki5FPffp4Grf4o&sharer_sharetime=1655876508786&sharer_shareid=aa88399bd33117178ca30f3bb172ce11#rd",target:"_blank",rel:"noopener noreferrer"};function U(Y,Z){const l=i("ExternalLinkIcon");return p(),c("div",null,[s("blockquote",null,[s("p",null,[a("混合精度训练，短短的几行代码，在节省显存占用 40%+，训练速度翻倍的前提下，能够做到模型准确率几乎不减少！强烈推荐阅读这篇只有 9 页的文章："),s("a",g,[a("MIXED PRECISION TRAINING"),n(l)]),a(" 。")])]),d,s("p",null,[a("关于 FP16 的储存格式等，可以参考 "),s("a",u,[a("wiki 百科"),n(l)]),a("。")]),y,b,s("table",null,[v,s("tbody",null,[s("tr",null,[x,s("td",f,[s("a",_,[a("zero"),n(l)]),a(", "),s("a",w,[a("−0"),n(l)])]),s("td",k,[s("a",z,[a("subnormal numbers"),n(l)])]),E]),P,s("tr",null,[F,s("td",M,[a("±"),s("a",A,[a("infinity"),n(l)])]),s("td",D,[s("a",O,[a("NaN"),n(l)]),a(" (quiet, signalling)")]),C])])]),L,q,I,N,G,S,V,B,T,R,s("p",null,[a("对于混合精度训练的细节，可以参考 NVIDAI APEX "),s("a",H,[a("源码"),n(l)]),a("。以下对整体流程最总结，内容参考与 "),s("a",X,[a("由浅入深的混合精度训练教程"),n(l)])]),K,s("p",null,[s("a",W,[a("【PyTorch】唯快不破：基于 Apex 的混合精度加速"),n(l)])]),s("p",null,[s("a",j,[a("fp16 详细入门 知乎"),n(l)])]),s("p",null,[s("a",J,[a("一文搞懂神经网络混合精度训练"),n(l)])]),s("p",null,[s("a",Q,[a("由浅入深的混合精度训练教程"),n(l)])])])}const as=e(h,[["render",U],["__file","笔记mix_train.html.vue"]]);export{as as default};
