const t=JSON.parse('{"key":"v-4cc95552","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0llm3.html","title":"Instruction Tuning 后时代的模型笔记（二）","lang":"zh-CN","frontmatter":{"title":"Instruction Tuning 后时代的模型笔记（二）","date":"2023-03-25T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC"],"description":"Alpaca，ChatGLM 6B 等模型的效果可以接受，下文总结部分笔记，为训练自定义小型化（7B）模型提供点知识储备。包括模型论文 LLAMA, PaLM, BLOOM, BLOOMZ-mT LLAMA LLaMA: Open and Efficient Foundation Language Models 论文的重点在于预训练，虽然也尝试了使用 instruction tuning 进行模型测试，但介绍部分并不多。 数据： 约 1.4 T token 预训练，多语言但就是没有中文；","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0llm3.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"Instruction Tuning 后时代的模型笔记（二）"}],["meta",{"property":"og:description","content":"Alpaca，ChatGLM 6B 等模型的效果可以接受，下文总结部分笔记，为训练自定义小型化（7B）模型提供点知识储备。包括模型论文 LLAMA, PaLM, BLOOM, BLOOMZ-mT LLAMA LLaMA: Open and Efficient Foundation Language Models 论文的重点在于预训练，虽然也尝试了使用 instruction tuning 进行模型测试，但介绍部分并不多。 数据： 约 1.4 T token 预训练，多语言但就是没有中文；"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-07-27T15:28:00.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:published_time","content":"2023-03-25T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-07-27T15:28:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Instruction Tuning 后时代的模型笔记（二）\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-03-25T00:00:00.000Z\\",\\"dateModified\\":\\"2023-07-27T15:28:00.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":3,"title":"LLAMA","slug":"llama","link":"#llama","children":[]},{"level":3,"title":"PaLM","slug":"palm","link":"#palm","children":[]},{"level":3,"title":"Bloom","slug":"bloom","link":"#bloom","children":[]},{"level":3,"title":"Bloomz/mT0","slug":"bloomz-mt0","link":"#bloomz-mt0","children":[]},{"level":3,"title":"SELF-INSTRUCT","slug":"self-instruct","link":"#self-instruct","children":[]}],"git":{"createdTime":1690471680000,"updatedTime":1690471680000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":4.44,"words":1333},"filePathRelative":"posts/notes/articles/笔记llm3.md","localizedDate":"2023年3月25日","excerpt":"<p>Alpaca，ChatGLM 6B 等模型的效果可以接受，下文总结部分笔记，为训练自定义小型化（7B）模型提供点知识储备。包括模型论文 LLAMA, PaLM, BLOOM, BLOOMZ-mT</p>\\n<!--more-->\\n<h3> <strong>LLAMA</strong></h3>\\n<p>LLaMA: Open and Efficient Foundation Language Models</p>\\n<p>论文的重点在于预训练，虽然也尝试了使用 instruction tuning 进行模型测试，但介绍部分并不多。</p>\\n<p><strong>数据：</strong> 约 1.4 T token 预训练，多语言但就是没有中文；</p>","autoDesc":true}');export{t as data};
