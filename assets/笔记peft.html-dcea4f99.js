import{_ as t,a as p,b as i,c as r,d as o,e as m}from"./image-20210926171006482-f8307cd9.js";import{_ as c}from"./image-20210928224419246-ba231669.js";import{_ as h}from"./plugin-vue_export-helper-c27b6911.js";import{r as d,o as g,c as u,a as s,b as a,d as e,f as n}from"./app-3910e477.js";const y={},v=n('<h1 id="llm-高效训练方案整理" tabindex="-1"><a class="header-anchor" href="#llm-高效训练方案整理" aria-hidden="true">#</a> LLM 高效训练方案整理</h1><p>本文基于 Huggingface PEFT，回顾整理常见的 LLM 高效训练方式，包括 prefix-tuning, p-tuning, lora, prompt tuning。对于 PEFT 中的模型，如 <code>PeftModelForSequenceClassification</code>。可以分为以下四种方式进行讨论：</p><h2 id="prefix-tuning-p-tuning-v2" tabindex="-1"><a class="header-anchor" href="#prefix-tuning-p-tuning-v2" aria-hidden="true">#</a> Prefix-Tuning (P-Tuning v2)</h2><p>论文：Prefix-tuning- Optimizing continuous prompts for generation</p><p>论文：P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</p><figure><img src="'+t+'" alt="相关图片" height="300" tabindex="0" loading="lazy"><figcaption>相关图片</figcaption></figure><h3 id="prefix-tuning-的直觉" tabindex="-1"><a class="header-anchor" href="#prefix-tuning-的直觉" aria-hidden="true">#</a> Prefix-tuning 的直觉</h3><p>对于原先的 GPT2 模型，我们在不同任务上 finetune 的时候经常需要对所有的参数进行微调，然后保存不同模型的权重。因此，如上图，论文作者提出在模型前加入可学习的 prefix 参数来引导整个模型的注意力机制，在区分不同下游任务的同时提高模型的学习能力，尽可能多得保留原预训练模型的知识。</p><figure><img src="'+p+'" alt="图：prefix tuning 示例" tabindex="0" loading="lazy"><figcaption>图：prefix tuning 示例</figcaption></figure>',9),b=s("p",null,[a("参考以上例图，原输入序列 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",{stretchy:"false"},"["),s("msub",null,[s("mi",null,"X"),s("mrow",null,[s("mi",null,"i"),s("mi",null,"d"),s("mi",null,"x")])]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"Y"),s("mrow",null,[s("mi",null,"i"),s("mi",null,"d"),s("mi",null,"x")])]),s("mo",{stretchy:"false"},"]")]),s("annotation",{encoding:"application/x-tex"},"[X_{idx},Y_{idx}]")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mord mathnormal mtight"},"d"),s("span",{class:"mord mathnormal mtight"},"x")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"Y"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.2222em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mord mathnormal mtight"},"d"),s("span",{class:"mord mathnormal mtight"},"x")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},"]")])])]),a(" 添加 prefix id 后变成了 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",{stretchy:"false"},"["),s("msub",null,[s("mi",null,"P"),s("mrow",null,[s("mi",null,"i"),s("mi",null,"d"),s("mi",null,"x")])]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"X"),s("mrow",null,[s("mi",null,"i"),s("mi",null,"d"),s("mi",null,"x")])]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"Y"),s("mrow",null,[s("mi",null,"i"),s("mi",null,"d"),s("mi",null,"x")])]),s("mo",{stretchy:"false"},"]")]),s("annotation",{encoding:"application/x-tex"},"[P_{idx},X_{idx},Y_{idx}]")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mord mathnormal mtight"},"d"),s("span",{class:"mord mathnormal mtight"},"x")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mord mathnormal mtight"},"d"),s("span",{class:"mord mathnormal mtight"},"x")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"Y"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.2222em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mord mathnormal mtight"},"d"),s("span",{class:"mord mathnormal mtight"},"x")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},"]")])])]),a(" 。不同于 LM 模型，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"P"),s("mrow",null,[s("mi",null,"i"),s("mi",null,"d"),s("mi",null,"x")])])]),s("annotation",{encoding:"application/x-tex"},"P_{idx}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mord mathnormal mtight"},"d"),s("span",{class:"mord mathnormal mtight"},"x")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(" 部分的隐状态使用单独的 prompt encoder 进行计算，因此对于添加 prefix 之后的模型，所有 hidden state 计算方式为：")],-1),f=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"h"),s("mi",null,"i")]),s("mo",null,"="),s("mrow",null,[s("mo",{fence:"true"},"{"),s("mtable",{rowspacing:"0.16em",columnalign:"left left",columnspacing:"1em"},[s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mrow",null,[s("msub",null,[s("mi",null,"P"),s("mi",null,"θ")]),s("mo",{stretchy:"false"},"["),s("mi",null,"i"),s("mo",{separator:"true"},","),s("mo",null,":"),s("mo",{stretchy:"false"},"]"),s("mo",{separator:"true"},",")])])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mrow",null,[s("mtext",null," if "),s("mi",null,"i"),s("mo",null,"∈"),s("msub",null,[s("mi",{mathvariant:"normal"},"P"),s("mrow",null,[s("mi",{mathvariant:"normal"},"i"),s("mi",{mathvariant:"normal"},"d"),s("mi",{mathvariant:"normal"},"x")])])])])])]),s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mrow",null,[s("msub",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"L"),s("mi",{mathvariant:"normal"},"M")]),s("mi",null,"ϕ")]),s("mrow",null,[s("mo",{fence:"true"},"("),s("msub",null,[s("mi",null,"z"),s("mi",null,"i")]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"h"),s("mrow",null,[s("mo",null,"<"),s("mi",null,"i")])]),s("mo",{fence:"true"},")")]),s("mo",{separator:"true"},",")])])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mtext",null," otherwise ")])])])])])]),s("annotation",{encoding:"application/x-tex"}," h_{i}=\\left\\{\\begin{array}{ll} P_{\\theta}[i,:], & \\text { if } i \\in \\mathrm{P}_{\\mathrm{idx}} \\\\ \\mathrm{LM}_{\\phi}\\left(z_{i}, h_{<i}\\right), & \\text { otherwise } \\end{array}\\right. ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8444em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"h"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.4em","vertical-align":"-0.95em"}}),s("span",{class:"minner"},[s("span",{class:"mopen delimcenter",style:{top:"0em"}},[s("span",{class:"delimsizing size3"},"{")]),s("span",{class:"mord"},[s("span",{class:"mtable"},[s("span",{class:"arraycolsep",style:{width:"0.5em"}}),s("span",{class:"col-align-l"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.45em"}},[s("span",{style:{top:"-3.61em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mopen"},"["),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},":"),s("span",{class:"mclose"},"]"),s("span",{class:"mpunct"},",")])]),s("span",{style:{top:"-2.41em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mord mathrm"},"LM")]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"ϕ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"minner"},[s("span",{class:"mopen delimcenter",style:{top:"0em"}},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.044em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"h"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mrel mtight"},"<"),s("span",{class:"mord mathnormal mtight"},"i")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1774em"}},[s("span")])])])])]),s("span",{class:"mclose delimcenter",style:{top:"0em"}},")")]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mpunct"},",")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.95em"}},[s("span")])])])]),s("span",{class:"arraycolsep",style:{width:"0.5em"}}),s("span",{class:"arraycolsep",style:{width:"0.5em"}}),s("span",{class:"col-align-l"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.45em"}},[s("span",{style:{top:"-3.61em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord text"},[s("span",{class:"mord"}," if ")]),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"∈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathrm"},"P"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathrm mtight"},"idx")])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])]),s("span",{style:{top:"-2.41em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord text"},[s("span",{class:"mord"}," otherwise ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.95em"}},[s("span")])])])]),s("span",{class:"arraycolsep",style:{width:"0.5em"}})])]),s("span",{class:"mclose nulldelimiter"})])])])])])],-1),_=s("p",null,[a("其中 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"i")]),s("annotation",{encoding:"application/x-tex"},"i")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6595em"}}),s("span",{class:"mord mathnormal"},"i")])])]),a(" 为对应 prefix 的 index。")],-1),E=s("p",null,"作者发现若直接对 prefix 参数进行更新会出现学习不稳定，模型表现变差等问题。于是添加了一个临时的 MLP 层与更小的临时参数矩阵来计算 prefix 参数，即：",-1),x=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"P"),s("mi",null,"θ")]),s("mo",{stretchy:"false"},"["),s("mi",null,"i"),s("mo",{separator:"true"},","),s("mo",null,":"),s("mo",{stretchy:"false"},"]"),s("mo",null,"="),s("msub",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"MLP"),s("mo",null,"⁡")]),s("mi",null,"θ")]),s("mrow",null,[s("mo",{fence:"true"},"("),s("msubsup",null,[s("mi",null,"P"),s("mi",null,"θ"),s("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")]),s("mo",{stretchy:"false"},"["),s("mi",null,"i"),s("mo",{separator:"true"},","),s("mo",null,":"),s("mo",{stretchy:"false"},"]"),s("mo",{fence:"true"},")")])]),s("annotation",{encoding:"application/x-tex"}," P_{\\theta}[i,:]=\\operatorname{MLP}_{\\theta}\\left(P_{\\theta}^{\\prime}[i,:]\\right) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mopen"},"["),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},":")]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mclose"},"]"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0519em","vertical-align":"-0.25em"}}),s("span",{class:"mop"},[s("span",{class:"mop"},[s("span",{class:"mord mathrm"},"MLP")]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"minner"},[s("span",{class:"mopen delimcenter",style:{top:"0em"}},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8019em"}},[s("span",{style:{top:"-2.453em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])]),s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"′")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.247em"}},[s("span")])])])])]),s("span",{class:"mopen"},"["),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},":"),s("span",{class:"mclose"},"]"),s("span",{class:"mclose delimcenter",style:{top:"0em"}},")")])])])])])],-1),k=s("p",null,[a("当训练完成后，只保留 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"P"),s("mi",null,"θ")])]),s("annotation",{encoding:"application/x-tex"},"P_\\theta")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(" 。同时在训练过程中 "),s("strong",null,"其他模型参数将会被冻结"),a(" 。从作者的实验结果看出，prefix-tuning 在数据量小的时候，能够用更少的参数实现更好的效果。")],-1),w=n('<figure><img src="'+i+`" alt="image-20210926221531444" tabindex="0" loading="lazy"><figcaption>image-20210926221531444</figcaption></figure><h3 id="参考-peft-实现" tabindex="-1"><a class="header-anchor" href="#参考-peft-实现" aria-hidden="true">#</a> 参考 PEFT 实现</h3><p>PEFT 的 prefix tuning 在每层 transformer layer 处添加上 prompt embedding。在 PEFT 中，采用了 Transformer 中的 cache 机制巧妙地实现了这种方案。大致的流程可以通过以下伪代码实现：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">prompt_encoder </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> PrefixEncoder　　</span><span style="color:#6A737D;"># 在 peft.tuners.prefix_tuning.py 查看</span></span>
<span class="line"><span style="color:#24292E;">prompt_tokens </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.arange(config.num_virtual_tokens).long().unsqueeze(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">).expand(batch_size, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 生成连续的 prompt, shape: [num_virtual_tokens, num_layers * 2 * token_dim]</span></span>
<span class="line"><span style="color:#24292E;">past_key_values </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> prompt_encoder(prompt_tokens)  </span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 将 prompt 映射为 Transformer cache 时需要的格式</span></span>
<span class="line"><span style="color:#6A737D;"># 在 Huggingface 中，巧妙地采用了 past_key_values 来传导模型推理时的 cache 信息。</span></span>
<span class="line"><span style="color:#24292E;">past_key_values </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> past_key_values.view(</span></span>
<span class="line"><span style="color:#24292E;">                batch_size,</span></span>
<span class="line"><span style="color:#24292E;">                peft_config.num_virtual_tokens,  </span><span style="color:#6A737D;"># \`prompt_token\` 的长度</span></span>
<span class="line"><span style="color:#24292E;">                peft_config.num_layers </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                peft_config.num_attention_heads,</span></span>
<span class="line"><span style="color:#24292E;">                peft_config.token_dim </span><span style="color:#D73A49;">//</span><span style="color:#24292E;"> peft_config.num_attention_heads,</span></span>
<span class="line"><span style="color:#24292E;">            )</span></span>
<span class="line"><span style="color:#24292E;">output </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.base_model(</span><span style="color:#E36209;">input_ids</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">input_ids, </span><span style="color:#E36209;">past_key_values</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">past_key_values, </span><span style="color:#D73A49;">**</span><span style="color:#24292E;">kwargs)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>猜测 past_key_values，主要采用的方式是与新的 <code>input_embedding</code> 计算出来的 kv 进行拼接，而后进行 multi-head Attention 等计算。所以 prefix-tuning，实际上是提供了可以训练的 kv 参数？？</p><h3 id="prefixencoder" tabindex="-1"><a class="header-anchor" href="#prefixencoder" aria-hidden="true">#</a> PrefixEncoder</h3><p>参考 prefix 原文，prompt 对应的 hidden state 计算方式就是索引，因此用 Embedding 即可，参考 prefix-tuning 作者的实验结果，也可以再 Embedding 之后加上一层 MLP 来提升训练稳定性。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">PrefixEncoder</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">torch</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):    </span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, prefix: torch.Tensor):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.prefix_projection:</span></span>
<span class="line"><span style="color:#24292E;">            prefix_tokens </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.embedding(prefix)</span></span>
<span class="line"><span style="color:#24292E;">            past_key_values </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.transform(prefix_tokens)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            past_key_values </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.embedding(prefix)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># past_key_values.shape: [num_virtual_tokens, num_layers * 2 * token_dim]</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> past_key_values</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="prompt-tuning" tabindex="-1"><a class="header-anchor" href="#prompt-tuning" aria-hidden="true">#</a> Prompt tuning</h2><blockquote><p>论文：The Power of Scale for Parameter-Efficient Prompt Tuning</p></blockquote>`,10),z={href:"https://zhuanlan.zhihu.com/p/415168620",target:"_blank",rel:"noopener noreferrer"},A=s("p",null,[a("主要方式为在输入的 embedding 前面，添加上 prompt embedding。使用可学习的 prompt 参数"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"P"),s("mi",null,"e")]),s("mo",null,"∈"),s("msup",null,[s("mi",{mathvariant:"double-struck"},"R"),s("mrow",null,[s("mi",null,"p"),s("mo",null,"×"),s("mi",null,"e")])])]),s("annotation",{encoding:"application/x-tex"},"P_{e} \\in \\mathbb{R}^{p \\times e}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"∈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7713em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathbb"},"R"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7713em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"p"),s("span",{class:"mbin mtight"},"×"),s("span",{class:"mord mathnormal mtight"},"e")])])])])])])])])])])]),a("作为输入前缀，与长度为 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"n")]),s("annotation",{encoding:"application/x-tex"},"n")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"n")])])]),a(" 的原输入 embedding "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"X"),s("mi",null,"e")]),s("mo",null,"∈"),s("msup",null,[s("mi",{mathvariant:"double-struck"},"R"),s("mrow",null,[s("mi",null,"n"),s("mo",null,"×"),s("mi",null,"e")])])]),s("annotation",{encoding:"application/x-tex"},"X_{e} \\in \\mathbb{R}^{n \\times e}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"∈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7713em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathbb"},"R"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7713em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"n"),s("span",{class:"mbin mtight"},"×"),s("span",{class:"mord mathnormal mtight"},"e")])])])])])])])])])])]),a(" 拼接得到 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mrow",null,[s("mo",{fence:"true"},"["),s("msub",null,[s("mi",null,"P"),s("mi",null,"e")]),s("mo",{separator:"true"},";"),s("msub",null,[s("mi",null,"X"),s("mi",null,"e")]),s("mo",{fence:"true"},"]")]),s("mo",null,"∈"),s("msup",null,[s("mi",{mathvariant:"double-struck"},"R"),s("mrow",null,[s("mo",{stretchy:"false"},"("),s("mi",null,"p"),s("mo",null,"+"),s("mi",null,"n"),s("mo",{stretchy:"false"},")"),s("mo",null,"×"),s("mi",null,"e")])])]),s("annotation",{encoding:"application/x-tex"},"\\left[P_{e} ; X_{e}\\right] \\in \\mathbb{R}^{(p+n) \\times e}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"minner"},[s("span",{class:"mopen delimcenter",style:{top:"0em"}},"["),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},";"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose delimcenter",style:{top:"0em"}},"]")]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"∈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.888em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathbb"},"R"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.888em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mopen mtight"},"("),s("span",{class:"mord mathnormal mtight"},"p"),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mathnormal mtight"},"n"),s("span",{class:"mclose mtight"},")"),s("span",{class:"mbin mtight"},"×"),s("span",{class:"mord mathnormal mtight"},"e")])])])])])])])])])])]),a("，其中 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"p")]),s("annotation",{encoding:"application/x-tex"},"p")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal"},"p")])])]),a(" 为 prompt 的长度（超参），"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"e")]),s("annotation",{encoding:"application/x-tex"},"e")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"e")])])]),a(" 为 embedding 的维度大小。训练时针对 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"Pr"),s("mo",null,"⁡")]),s("mrow",null,[s("mi",null,"θ"),s("mo",{separator:"true"},";"),s("msub",null,[s("mi",null,"θ"),s("mi",null,"P")])])]),s("mo",{stretchy:"false"},"("),s("mi",null,"Y"),s("mo",null,"∣"),s("mo",{stretchy:"false"},"["),s("mi",null,"P"),s("mo",{separator:"true"},";"),s("mi",null,"X"),s("mo",{stretchy:"false"},"]"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"\\operatorname{Pr}_{\\theta ; \\theta_{P}}(Y \\mid[P ; X])")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0361em","vertical-align":"-0.2861em"}}),s("span",{class:"mop"},[s("span",{class:"mop"},[s("span",{class:"mord mathrm"},"Pr")]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ"),s("span",{class:"mpunct mtight"},";"),s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3567em","margin-left":"-0.0278em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"P")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1433em"}},[s("span")])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"Y"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"∣"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mpunct"},";"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"mclose"},"])")])])]),a(" 进行优化，冻结预训练模型权重，单独对 prompt 参数进行训练与更新。")],-1),P=n(`<p>参考 Huggingface PEFT，模型的整个前向推导流程可以看作：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">inputs_embeds </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.word_embeddings(input_ids)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Prompt Tunign 使用的 prompt_encoder 是单纯的 embedding</span></span>
<span class="line"><span style="color:#24292E;">prompt_encoder </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.nn.Embedding(</span></span>
<span class="line"><span style="color:#24292E;">    config.num_virtual_tokens </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> config.num_transformer_submodules, config.token_dim</span></span>
<span class="line"><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 此处的 num_transformer_submodules 对于 encoder-decoder 架构为 2，对于 decoder 为 1</span></span>
<span class="line"><span style="color:#24292E;">prompt_token </span><span style="color:#D73A49;">=</span><span style="color:#24292E;">  torch.arange(config.num_virtual_tokens </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> config.num_transformer_submodules).long().unsqueeze(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">).expand(batch_size, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">if</span><span style="color:#24292E;"> labels </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">    prefix_labels </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.full((batch_size, peft_config.num_virtual_tokens), </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">100</span><span style="color:#24292E;">).to(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.device)</span></span>
<span class="line"><span style="color:#24292E;">    kwargs[</span><span style="color:#032F62;">&quot;labels&quot;</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.cat((prefix_labels, labels), </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">prompts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> prompt_encoder(prompt_tokens).to(inputs_embeds.dtype)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 在 input embedding 前面添加 prompt embedding 即可。</span></span>
<span class="line"><span style="color:#24292E;">inputs_embeds </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.cat((prompts, inputs_embeds), </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">output </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.base_model(</span><span style="color:#E36209;">inputs_embeds</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">inputs_embeds, </span><span style="color:#D73A49;">**</span><span style="color:#24292E;">kwargs)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>由 prompt tuning 中的实验数据可看出，仅对 prompt 参数进行学习，在模型规模足够大时媲美对模型全参数进行 finetune。由于更新的参数熟练减小，训练难度降低，同时也不需要针对不同的任务各保存一份完整的 T5 模型，只需储存 prompt 参数部分即可。</p><figure><img src="`+c+'" alt="相关图片" height="300" tabindex="0" loading="lazy"><figcaption>相关图片</figcaption></figure><p>(图：T5 模型在不同训练方式下的结果)</p><h2 id="p-tuning" tabindex="-1"><a class="header-anchor" href="#p-tuning" aria-hidden="true">#</a> P-Tuning</h2><p>论文：P-tuning-GPT Understands, Too.</p><figure><img src="'+r+`" alt="image-20210926165212251" tabindex="0" loading="lazy"><figcaption>image-20210926165212251</figcaption></figure><p>P-Tuning 与 Prompt Tuning 较为相似，参考 PEFT 的实现，两者的主要差别在于：</p><ol><li>Prompt Tuning 中的 prompt_encoder 简单地使用了 Embedding，而 P-Tuning 在 Embedding 基础上，另外添加了额外地 layer 进行处理。</li><li>除此外， P-Tuning 主要是针对 NLU 任务进行实验，因此 prompt 添加的位置并非全部置于前面。</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, indices):</span></span>
<span class="line"><span style="color:#24292E;">    input_embeds </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.embedding(indices)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.encoder_type </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> PromptEncoderReparameterizationType.</span><span style="color:#005CC5;">LSTM</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">        output_embeds </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.mlp_head(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.lstm_head(input_embeds)[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">])</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">elif</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.encoder_type </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> PromptEncoderReparameterizationType.</span><span style="color:#005CC5;">MLP</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">        output_embeds </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.mlp_head(input_embeds)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">raise</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">ValueError</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> output_embeds</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>为了使得 prompt 的编码之间存在相关性，并解决 embedding 分布离散 （Discreteness）的问题（PLM 中的 embedding 高度离散导致使用 SGD 会很容易陷入局部最优），作者使用 BiLSTM 计算 prompt 的 hidden state。</p>`,12),C={href:"https://zhuanlan.zhihu.com/p/364141928",target:"_blank",rel:"noopener noreferrer"},D=s("strong",null,"对下游任务目标与其他任务（如 LM 或者 MLM）一起优化",-1),M=n('<p>此外作者还发现加入一下小标志符号有助于 NLU，如“[PRE][prompt tokens][HYP]?[prompt tokens][MASK]”中的问号。</p><p>作者对比了以下四种训练方式：（MP 代表 Manual Prompt）</p><figure><img src="'+o+'" alt="image-20210926171640874" tabindex="0" loading="lazy"><figcaption>image-20210926171640874</figcaption></figure><figure><img src="'+m+`" alt="相关图片" tabindex="0" loading="lazy"><figcaption>相关图片</figcaption></figure><p>结果是 GPT2 在大部分数据集上优胜。</p><h2 id="lora" tabindex="-1"><a class="header-anchor" href="#lora" aria-hidden="true">#</a> Lora</h2><p>论文: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</p><p>不同于前三者，PEFT 对于 lora 的实现主要是在模型架构上，因此整个前项传导过程中不会设计任何的 prompt 因素：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">output </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.base_model(</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">input_ids</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">input_ids,</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">attention_mask</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">attention_mask,</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">inputs_embeds</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">inputs_embeds,</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">labels</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">labels,</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">output_attentions</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">output_attentions,</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">output_hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">output_hidden_states,</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">return_dict</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">return_dict,</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">**</span><span style="color:#24292E;">kwargs,</span></span>
<span class="line"><span style="color:#24292E;">            )</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>模型架构的更改由 <code>peft.peft_model.PeftModel.add_adapter()</code> 完成，更改包括以下：</p>`,10),L=n(`<li><p>使用 <code>LORA_MODEL</code> 来包装 huggingface 模型，</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">LoraModel</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">torch</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, model, config, adapter_name):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.model </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> model    </span><span style="color:#6A737D;"># 该 model 为 huggingface.transformer 加载的模型</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.forward </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.model.forward</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.peft_config </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config   </span><span style="color:#6A737D;"># config 为 lora-config</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.add_adapter(adapter_name, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.peft_config[adapter_name])</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li>`,1),T=s("code",null,"peft.tuners.lora.Linear()",-1),F=s("code",null,"fan_in_fan_out",-1),q={href:"https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L148",target:"_blank",rel:"noopener noreferrer"},R=n('<figure><img src="https://picx.zhimg.com/80/v2-88b5c9d3d36f578f1ddb7edc3218b130_1440w.png" alt="lora 模块" height="400" tabindex="0" loading="lazy"><figcaption>lora 模块</figcaption></figure><ol start="3"><li><p>需要替换的模块可以通过 lora_config 中的 target_modules 参数进行传递，比如 <code>[&quot;q&quot;, &quot;v&quot;]</code>。因为 transformer 中的模型 module 通常会包含 <code>k, q, v</code> 等字样（比如第 1 层的 multi-headattention 可能命名为 <code>decoder.layer_0.attn.q_proj.weight</code>），因此在锁定更换模块时，只需要进行文字匹配即可。</p><p>参考下图 LORA 论文中作者的实验，冻结 q 和 v 可能是一个不错的选择：</p><figure><img src="https://pica.zhimg.com/80/v2-159d46226ccb4a7ae44f196e0a3c65d7_1440w.png" alt="image-20230426000623300" tabindex="0" loading="lazy"><figcaption>image-20230426000623300</figcaption></figure></li><li><p>除了 lora 对应的 layer，冻结其他所有参数。</p><p>通过 LORA 论文的实验结果，RANK=8 配合 <code>target_modules= [&quot;q&quot;,&quot;v&quot;]</code>可能是不错的选择。</p><figure><img src="https://picx.zhimg.com/80/v2-5b4564f113999e2059d98e7e41847382_1440w.png" alt="image-20230426000752577" tabindex="0" loading="lazy"><figcaption>image-20230426000752577</figcaption></figure></li></ol>',2);function X(N,O){const l=d("ExternalLinkIcon");return g(),u("div",null,[v,b,f,_,E,x,k,w,s("p",null,[a("Prompt Tuning 在笔者的这篇 "),s("a",z,[a("文章"),e(l)]),a(" 中有稍微介绍过。论文使用了 encoder-decoder 结构的 T5 模型对参数化的 prompt 训练进行了研究。")]),A,P,s("p",null,[a("根据"),s("a",C,[a("网友的咨询与解析"),e(l)]),a(" 论文作者认为此处一种更自然的做法 "),D,a(" ，类似 PET 的优化方案。")]),M,s("ol",null,[L,s("li",null,[s("p",null,[a("遍历模型中的所有 module，将需要替换的模块更换为 "),T,a("，其结构如下图，除更换模块之外，还需要对 "),F,a(", int8 计算等进行适配操作，具体可以看 "),s("a",q,[a("peft 中 lora 实现方式"),e(l)]),a(" 。")])])]),R])}const U=h(y,[["render",X],["__file","笔记peft.html.vue"]]);export{U as default};
