const s=JSON.parse('{"key":"v-01ff8acc","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0parallel_DL.html","title":"深度学习多卡训练","lang":"zh-CN","frontmatter":{"title":"深度学习多卡训练","date":"2021-06-07T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP"],"description":"单机单卡 梯度累加 单机单卡下可以通过梯度累加来增大训练时候的理论 batch size。如： for i, (inputs, labels) in enumerate(training_set): loss = model(inputs, labels) loss = loss / accumulation_steps loss.backward() if (i+1) % accumulation_steps == 0: global_step += 1 optimizer.step() optimizer.clean_grad()","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0parallel_DL.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"深度学习多卡训练"}],["meta",{"property":"og:description","content":"单机单卡 梯度累加 单机单卡下可以通过梯度累加来增大训练时候的理论 batch size。如： for i, (inputs, labels) in enumerate(training_set): loss = model(inputs, labels) loss = loss / accumulation_steps loss.backward() if (i+1) % accumulation_steps == 0: global_step += 1 optimizer.step() optimizer.clean_grad()"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:published_time","content":"2021-06-07T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"深度学习多卡训练\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2021-06-07T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"单机单卡","slug":"单机单卡","link":"#单机单卡","children":[{"level":3,"title":"梯度累加","slug":"梯度累加","link":"#梯度累加","children":[]},{"level":3,"title":"混合精度训练","slug":"混合精度训练","link":"#混合精度训练","children":[]}]},{"level":2,"title":"单机多卡","slug":"单机多卡","link":"#单机多卡","children":[{"level":3,"title":"多 GPU 操作总结","slug":"多-gpu-操作总结","link":"#多-gpu-操作总结","children":[]},{"level":3,"title":"进程间通信","slug":"进程间通信","link":"#进程间通信","children":[]},{"level":3,"title":"多卡训练提示","slug":"多卡训练提示","link":"#多卡训练提示","children":[]}]},{"level":2,"title":"其他参考","slug":"其他参考","link":"#其他参考","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":3.56,"words":1069},"filePathRelative":"posts/notes/articles/笔记parallel_DL.md","localizedDate":"2021年6月7日","excerpt":"<h2> 单机单卡</h2>\\n<h3> 梯度累加</h3>\\n<p>单机单卡下可以通过梯度累加来增大训练时候的理论 batch size。如：</p>\\n<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"shiki github-light\\" style=\\"background-color: #fff\\" tabindex=\\"0\\"><code><span class=\\"line\\"><span style=\\"color: #D73A49\\">for</span><span style=\\"color: #24292E\\"> i, (inputs, labels) </span><span style=\\"color: #D73A49\\">in</span><span style=\\"color: #24292E\\"> </span><span style=\\"color: #005CC5\\">enumerate</span><span style=\\"color: #24292E\\">(training_set):</span></span>\\n<span class=\\"line\\"><span style=\\"color: #24292E\\">  loss </span><span style=\\"color: #D73A49\\">=</span><span style=\\"color: #24292E\\"> model(inputs, labels)                   </span></span>\\n<span class=\\"line\\"><span style=\\"color: #24292E\\">  loss </span><span style=\\"color: #D73A49\\">=</span><span style=\\"color: #24292E\\"> loss </span><span style=\\"color: #D73A49\\">/</span><span style=\\"color: #24292E\\"> accumulation_steps                </span></span>\\n<span class=\\"line\\"><span style=\\"color: #24292E\\">  loss.backward()                                 </span></span>\\n<span class=\\"line\\"><span style=\\"color: #24292E\\">  </span><span style=\\"color: #D73A49\\">if</span><span style=\\"color: #24292E\\"> (i</span><span style=\\"color: #D73A49\\">+</span><span style=\\"color: #005CC5\\">1</span><span style=\\"color: #24292E\\">) </span><span style=\\"color: #D73A49\\">%</span><span style=\\"color: #24292E\\"> accumulation_steps </span><span style=\\"color: #D73A49\\">==</span><span style=\\"color: #24292E\\"> </span><span style=\\"color: #005CC5\\">0</span><span style=\\"color: #24292E\\">: </span></span>\\n<span class=\\"line\\"><span style=\\"color: #24292E\\">      global_step </span><span style=\\"color: #D73A49\\">+=</span><span style=\\"color: #24292E\\"> </span><span style=\\"color: #005CC5\\">1</span></span>\\n<span class=\\"line\\"><span style=\\"color: #24292E\\">      optimizer.step()                            </span></span>\\n<span class=\\"line\\"><span style=\\"color: #24292E\\">      optimizer.clean_grad() </span></span>\\n<span class=\\"line\\"></span></code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","autoDesc":true}');export{s as data};
