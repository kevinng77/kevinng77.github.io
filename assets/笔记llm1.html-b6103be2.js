const t=JSON.parse('{"key":"v-495fa414","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0llm1.html","title":"Instruction Tuning 时代的模型笔记","lang":"zh-CN","frontmatter":{"title":"Instruction Tuning 时代的模型笔记","date":"2023-03-25T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC"],"description":"Instruction Tuning 时代的模型笔记 Alpaca，ChatGLM 等模型的效果可以接受，下文总结部分笔记，为训练自定义小型化模型提供点知识储备。包括模型论文 LaMDA, Muppet, FLAN, T0, FLAN-PLAM, FLAN-T5 LaMDA 论文：Language Models for Dialog Applications LaMDA 没用到 Instrcution Tuning，但下文中部分模型基于 LaMDA 进行微调。 模型： 大小从 2B 到 137B 不等。","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0llm1.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"Instruction Tuning 时代的模型笔记"}],["meta",{"property":"og:description","content":"Instruction Tuning 时代的模型笔记 Alpaca，ChatGLM 等模型的效果可以接受，下文总结部分笔记，为训练自定义小型化模型提供点知识储备。包括模型论文 LaMDA, Muppet, FLAN, T0, FLAN-PLAM, FLAN-T5 LaMDA 论文：Language Models for Dialog Applications LaMDA 没用到 Instrcution Tuning，但下文中部分模型基于 LaMDA 进行微调。 模型： 大小从 2B 到 137B 不等。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-03-26T07:48:04.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:published_time","content":"2023-03-25T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-03-26T07:48:04.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Instruction Tuning 时代的模型笔记\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-03-25T00:00:00.000Z\\",\\"dateModified\\":\\"2023-03-26T07:48:04.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":3,"title":"LaMDA","slug":"lamda","link":"#lamda","children":[]},{"level":3,"title":"Muppet","slug":"muppet","link":"#muppet","children":[]},{"level":3,"title":"FLAN","slug":"flan","link":"#flan","children":[]},{"level":3,"title":"T0","slug":"t0","link":"#t0","children":[]},{"level":3,"title":"FLAN-PLAM","slug":"flan-plam","link":"#flan-plam","children":[]},{"level":3,"title":"","slug":"","link":"#","children":[]}],"git":{"createdTime":1679816884000,"updatedTime":1679816884000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":4.7,"words":1409},"filePathRelative":"posts/notes/articles/笔记llm1.md","localizedDate":"2023年3月25日","excerpt":"<h1> Instruction Tuning 时代的模型笔记</h1>\\n<p>Alpaca，ChatGLM 等模型的效果可以接受，下文总结部分笔记，为训练自定义小型化模型提供点知识储备。包括模型论文 LaMDA, Muppet, FLAN, T0, FLAN-PLAM, FLAN-T5</p>\\n<h3> <strong>LaMDA</strong></h3>\\n<p>论文：Language Models for Dialog Applications</p>\\n<p>LaMDA 没用到 Instrcution Tuning，但下文中部分模型基于 LaMDA 进行微调。</p>\\n<p><strong>模型：</strong> 大小从 2B 到 137B 不等。</p>","autoDesc":true}');export{t as data};
