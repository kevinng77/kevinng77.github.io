import{_ as t}from"./plugin-vue_export-helper-c27b6911.js";import{r,o as l,c as i,a as e,b as s,d as o,f as n}from"./app-c5f67444.js";const p={},c=e("p",null,"妙鸭的热度过了一阵子了，网上对妙鸭背后的实现逻辑有这种各样的猜测，不少网友认为妙鸭只是简单的采用 SD + Lora。本文主要对 SD + Lora 方案进行探索，分析妙鸭采用 SD + Lora 方案的可能性。",-1),d=e("h2",{id:"环境准备",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#环境准备","aria-hidden":"true"},"#"),s(),e("strong",null,"环境准备")],-1),h={href:"https://github.com/AUTOMATIC1111/stable-diffusion-webui",target:"_blank",rel:"noopener noreferrer"},u={href:"https://blog.csdn.net/weixin_43732022/article/details/129336297",target:"_blank",rel:"noopener noreferrer"},_=n('<h2 id="数据准备" tabindex="-1"><a class="header-anchor" href="#数据准备" aria-hidden="true">#</a> <strong>数据准备</strong></h2><h3 id="收集图片" tabindex="-1"><a class="header-anchor" href="#收集图片" aria-hidden="true">#</a> <strong>收集图片</strong></h3><p>大约 16 张图片。图中人物的风格尽量不同，图像的长宽大小没有要求，尽量像素高一点。</p><h3 id="图像人物裁剪" tabindex="-1"><a class="header-anchor" href="#图像人物裁剪" aria-hidden="true">#</a> <strong>图像人物裁剪</strong></h3><p>在训练 lora 的过程中，我们希望尽可能减少背景带来的影响。可以通过目标检测等技术对图像进行裁剪：</p><figure><img src="https://picx.zhimg.com/80/v2-0cc39055272b0c72cab14d84f5885cfa_1440w.png?source=d16d100b" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>当图中有多个人物时，使用目标检测自动裁剪会有点小复杂。因此我们尽量似使用单人照片进行处理。当然，如果只是想要训练个人 LORA 的话，手动对 16 张照片进行裁剪一下就行。</p><h3 id="图像质量提升" tabindex="-1"><a class="header-anchor" href="#图像质量提升" aria-hidden="true">#</a> <strong>图像质量提升</strong></h3>',8),m={href:"https://github.com/AUTOMATIC1111/stable-diffusion-webui",target:"_blank",rel:"noopener noreferrer"},g=n('<figure><img src="https://picx.zhimg.com/80/v2-297f1f46fb72b1d5ddbbe365557a1e6c_1440w.png?source=d16d100b" alt="AUTO1111 WEBUI 操作截图" tabindex="0" loading="lazy"><figcaption>AUTO1111 WEBUI 操作截图</figcaption></figure><p>scale 前后效果对比（左图为优化后的结果）：</p><figure><img src="https://picx.zhimg.com/80/v2-e62ce4ee1af4f139ea5ab5d807a755b3_1440w.png?source=d16d100b" alt="优化后图片 vs 优化前图片" tabindex="0" loading="lazy"><figcaption>优化后图片 vs 优化前图片</figcaption></figure><p>也可以采用 stable-diffusion 中 img2img 等功能，生成提升图片的质量。</p><h3 id="caption" tabindex="-1"><a class="header-anchor" href="#caption" aria-hidden="true">#</a> <strong>Caption</strong></h3><p>我们需要为每张图片写一句 prompt，用于 Lora 的训练。 Caption 过程中的一些小贴士：</p><ul><li>假设你要训练某男子的 Lora，那么所有的 prompt 中添加上相同的人物触发词，比如姓名或者拼音。触发词应该尽量独特，比如你要训练的人物名叫 coffee，那么训练时应尽量避免使用 coffee 这个容易混淆的词汇。</li><li>来自网友的意见：prompt 当中，应该避免提及到自定义人物的特点。比如自定义人物是某男子，那么就不要再 prompt 当中提到 man，black hair 等词。于此相反的，不属于该男子的特征应该尽量被提及（比如衣服的颜色，造型等等）。比如该男子平时穿衣风格丰富，但你提供的照片中，他总是穿着一件蓝色的 T shirt，那么 prompt 当中应该提及 blue T shirt。</li></ul><p>该过程可以人工写，当然也可以使用 CLIP 工具进行实现。比如 AUTO1111 里 img2img 一栏，提供了 Interrogate CLIP，可以针对每个图片进行 caption。</p><figure><img src="https://pic1.zhimg.com/80/v2-0b6e8e0648ed7b00470e32ef3b62caaa_1440w.png?source=d16d100b" alt="AUTO1111 webui Caption 操作截图" tabindex="0" loading="lazy"><figcaption>AUTO1111 webui Caption 操作截图</figcaption></figure><p>此外，可以采用如 kohya UI - utils 中的 BLIP Captioning。通常会将图像 img_1234.jpg 对应的 prompt 储存在 img_1234.txt 文件中。</p><h2 id="lora-训练" tabindex="-1"><a class="header-anchor" href="#lora-训练" aria-hidden="true">#</a> <strong>Lora 训练</strong></h2>',11),f={href:"https://github.com/bmaltais/kohya_ss",target:"_blank",rel:"noopener noreferrer"},b={href:"https://github.com/bmaltais/kohya_ss#installation",target:"_blank",rel:"noopener noreferrer"},y={href:"https://github.com/bmaltais/kohya_ss/blob/master/train_network.py",target:"_blank",rel:"noopener noreferrer"},C=n(`<h3 id="训练文件夹准备" tabindex="-1"><a class="header-anchor" href="#训练文件夹准备" aria-hidden="true">#</a> <strong>训练文件夹准备</strong></h3><div class="language-text line-numbers-mode" data-ext="text"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292e;">└── abcboy</span></span>
<span class="line"><span style="color:#24292e;">    └── 100_abcboy               # 文件夹命名为 {repeat}_{folder_name}</span></span>
<span class="line"><span style="color:#24292e;">        ├── image_01.png         # 文件夹下放置图片</span></span>
<span class="line"><span style="color:#24292e;">        ├── image_01.txt         # 还有图片对应的 prompt</span></span>
<span class="line"><span style="color:#24292e;">        └── ....</span></span>
<span class="line"><span style="color:#24292e;"></span></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>参考以上文件夹结构，其中我们需要将图片和储存 prompt 的 .txt 文件放在名为 {repeat}_{folder_name} 的文件夹下，其中 repeat 为每张图片重复的次数，比如设置 repeat 为 100 时，假设文件夹下一共有 16 张图片，那么训练过程中每个 epoch 会训练 16 * 100 张图片。</p><h3 id="开始训练" tabindex="-1"><a class="header-anchor" href="#开始训练" aria-hidden="true">#</a> <strong>开始训练</strong></h3><p>经过几次调参后，以下参数效果相对理想。其中：</p>`,5),v={href:"https://zhuanlan.zhihu.com/p/651809963/runwayml/stable-diffusion-v1-5",target:"_blank",rel:"noopener noreferrer"},k={href:"https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE",target:"_blank",rel:"noopener noreferrer"},w=e("li",null,"train_data_dir：选 abcboy 而不是 100_abcboy ，参考上文提到的训练文件夹准备。",-1),E=e("li",null,"network_alpha 和 network_dim：个人测试 alpha=128, dim=128 与 alpha=8, dim=8 VRAM 占用及训练时长差不多，但 128 效果好很多。",-1),x={href:"https://www.youtube.com/watch?v=k5imq01uvUY",target:"_blank",rel:"noopener noreferrer"},A=e("li",null,"clip_skip：个人测试，clip_skip=2 相对 clip_skip=1 效果好一点。",-1),q=e("li",null,"batch_size：batch size =2 算是一种权衡。由于我们数据集较小，batch size 过大会导致梯度更新速度太慢。",-1),T=n(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">accelerate</span><span style="color:#24292E;"> </span><span style="color:#032F62;">launch</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--num_cpu_threads_per_process=2</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;/workspace/kohya_ss/train_network.py&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--enable_bucket</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--min_bucket_reso=256</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--max_bucket_reso=2048</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--pretrained_model_name_or_path=</span><span style="color:#032F62;">&quot;runwayml/stable-diffusion-v1-5&quot;</span><span style="color:#24292E;">  </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--train_data_dir=</span><span style="color:#032F62;">&quot;/workspace/abcboy&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--resolution=</span><span style="color:#032F62;">&quot;512,512&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--output_dir=</span><span style="color:#032F62;">&quot;/workspace/stable-diffusion-webui/models/Lora&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--logging_dir=</span><span style="color:#032F62;">&quot;/workspace/logs&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--network_alpha=</span><span style="color:#032F62;">&quot;128&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--save_model_as=safetensors</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--network_module=networks.lora</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--text_encoder_lr=5e-05</span><span style="color:#24292E;">   </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--unet_lr=0.0001</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--network_dim=128</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--output_name=</span><span style="color:#032F62;">&quot;abcboy_v1.0&quot;</span><span style="color:#24292E;">             </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--lr_scheduler_num_cycles=</span><span style="color:#032F62;">&quot;1&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--no_half_vae</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--learning_rate=</span><span style="color:#032F62;">&quot;0.0001&quot;</span><span style="color:#24292E;">        </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--lr_scheduler=</span><span style="color:#032F62;">&quot;constant&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--train_batch_size=</span><span style="color:#032F62;">&quot;2&quot;</span><span style="color:#24292E;">        </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--max_train_steps=</span><span style="color:#032F62;">&quot;800&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--save_every_n_epochs=</span><span style="color:#032F62;">&quot;1&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--mixed_precision=</span><span style="color:#032F62;">&quot;bf16&quot;</span><span style="color:#24292E;">    </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--save_precision=</span><span style="color:#032F62;">&quot;bf16&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--seed=</span><span style="color:#032F62;">&quot;1234&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--caption_extension=</span><span style="color:#032F62;">&quot;.txt&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--cache_latents</span><span style="color:#24292E;">     </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--optimizer_type=</span><span style="color:#032F62;">&quot;AdamW8bit&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--max_data_loader_n_workers=</span><span style="color:#032F62;">&quot;1&quot;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--clip_skip=2</span><span style="color:#24292E;">     </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--bucket_reso_steps=64</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--xformers</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--bucket_no_upscale</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--noise_offset=0.0</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>个人在本地 4090 运行，整个训练过程大约耗时 90 秒，VRAM 占用 7GB。</p><h2 id="效果测试" tabindex="-1"><a class="header-anchor" href="#效果测试" aria-hidden="true">#</a> <strong>效果测试</strong></h2><p>我们将训练好的 lora 权重保存在 AUTO1111 的 lora 权重位置： stable-diffusion-webui/models/Lora 下，而后通过 AUTO1111 webui 来进行简单的生成测试。</p><p>以下是个人使用生成图片的配置：</p>`,5),I={href:"https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE",target:"_blank",rel:"noopener noreferrer"},L={href:"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.ckpt",target:"_blank",rel:"noopener noreferrer"},R=e("li",null,"sampling method 采用 DPM++ SDE Karras",-1),V=e("li",null,"Clip Skip=2，width=512, height=768",-1),F={href:"https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE",target:"_blank",rel:"noopener noreferrer"},S=e("li",null,"prompt: RAW photo, subject, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3",-1),U=e("li",null,"negative prompt： (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime), text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, UnrealisticDream",-1),D=n('<p>个人在本地测试，使用 16 张图片训练的 lora，能够生成与训练集人物相似的形象照：</p><figure><img src="https://picx.zhimg.com/80/v2-98ca5adbf325dd1b024c6383d1352efb_1440w.png?source=d16d100b" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>生成照片还有一些瑕疵（比如手，头发等），可以考虑使用一些其他 lora 或者 embedding 来完善。</p><h2 id="妙鸭的一些想法" tabindex="-1"><a class="header-anchor" href="#妙鸭的一些想法" aria-hidden="true">#</a> <strong>妙鸭的一些想法</strong></h2><p>如果使用 SD + Lora 训练方案，搭建一个类似妙鸭的服务，那么根据实验的信息： <strong>在单卡 4090 上用 16 张图片训练 LORA，只需要 90 秒不到，并且生成效果不错。</strong></p><p>而其他的图像处理流程，如图像生成，超分，人脸匹配，目标检测等，加起来也不到 90 秒。 因此我们可以判断，用一台 4090，单独服务一个客户只需要 3 分钟即可，大致一小时可以服务 20 个客户。</p><p>参考网友贴出的秒鸭排队情况：</p><figure><img src="https://picx.zhimg.com/80/v2-252d0c495c4f93d5d47d7e7d187d08c4_1440w.png?source=d16d100b" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>如果秒鸭采用了 LORA 训练的方案实现的话，部署 16 台 4090 ，8 小时可以服务约 2560 人。每个用户的训练和推理成本约为 0.15 元（参考 GPU 市场租赁价格，如 AUTODL 或恒源云，一小时 4090 价格约 3 元）。由于训练 LORA 以及推理过程中，实际只需要 7GB 的显存，因此每台 4090 可以部署多个实例。</p><h2 id="资源" tabindex="-1"><a class="header-anchor" href="#资源" aria-hidden="true">#</a> <strong>资源</strong></h2><p>可以考虑叠加其他风格的 lora 生成不同类型的艺术照：</p>',11),O={href:"https://civitai.com/images/1931055?period=AllTime&periodMode=published&sort=Newest&view=categories&modelVersionId=113479&modelId=25494&postId=474886",target:"_blank",rel:"noopener noreferrer"},z={href:"https://civitai.com/images/2001799?period=AllTime&periodMode=published&sort=Newest&view=categories&modelVersionId=113479&modelId=25494&postId=492440",target:"_blank",rel:"noopener noreferrer"},N={href:"https://civitai.com/images/1599751?period=AllTime&periodMode=published&sort=Newest&view=categories&modelVersionId=117437&modelId=109018&postId=403858",target:"_blank",rel:"noopener noreferrer"},M={href:"https://civitai.com/models/108815",target:"_blank",rel:"noopener noreferrer"},B={href:"https://civitai.com/images/1344910?period=AllTime&periodMode=published&sort=Newest&view=categories&modelVersionId=95103&modelId=89348&postId=347211",target:"_blank",rel:"noopener noreferrer"},G={href:"https://civitai.com/models/89348",target:"_blank",rel:"noopener noreferrer"},P={href:"https://civitai.com/images/1662448?period=AllTime&periodMode=published&sort=Newest&view=categories&username=y_tamura&withTags=false",target:"_blank",rel:"noopener noreferrer"},W={href:"https://civitai.com/models/23337",target:"_blank",rel:"noopener noreferrer"},j={href:"https://civitai.com/models/43331?modelVersionId=126470",target:"_blank",rel:"noopener noreferrer"},H={href:"https://civitai.com/models/25494/beautiful-realistic-asians",target:"_blank",rel:"noopener noreferrer"},X=e("h2",{id:"参考",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#参考","aria-hidden":"true"},"#"),s(),e("strong",null,"参考")],-1),K={href:"https://github.com/bmaltais/kohya_ss",target:"_blank",rel:"noopener noreferrer"},Y={href:"https://www.youtube.com/watch?v=N4_-fB62Hwk",target:"_blank",rel:"noopener noreferrer"},Q={href:"https://www.youtube.com/watch?v=k5imq01uvUY",target:"_blank",rel:"noopener noreferrer"},Z={href:"https://www.youtube.com/watch?v=TpuDOsuKIBo",target:"_blank",rel:"noopener noreferrer"},J={href:"https://youtu.be/AY6DMBCIZ3A",target:"_blank",rel:"noopener noreferrer"},$={href:"https://imgur.com/a/mrTteIt",target:"_blank",rel:"noopener noreferrer"};function ee(se,ae){const a=r("ExternalLinkIcon");return l(),i("div",null,[c,d,e("p",null,[s("推荐使用现有的 GUI "),e("a",h,[s("AUTO1111/stable-diffusion-webui"),o(a)]),s(" 。安装指南可以参考官方仓库下的安装方案，或者其他网友笔记，比如 "),e("a",u,[s("AUTOMATIC1111/stable-diffusion-webui 安装教程_咔！哈！的博客-CSDN 博客"),o(a)]),s(" 。")]),_,e("p",null,[s("对于 lora 训练，图像像素最好在 （512，512）以上。对于像素低的照片，可以使用一些算法来对图像进行优化（细节优化，分辨率提升等），如 ESRGAN，LDSR，R-ESRGAN 4x+等。我们采用 R-ESRGAN 4x+ 对图像进行优化。可以使用 "),e("a",m,[s("AUTO1111"),o(a)]),s(" 中 EXTRA 一栏下的 scale 工具，一键对图像进行处理：")]),g,e("p",null,[s("以下使用 "),e("a",f,[s("kohya_ss "),o(a)]),s(" 脚本进行训练（"),e("a",b,[s("安装参考"),o(a)]),s("）。 kohya_ss 提供了"),e("a",y,[s("训练代码"),o(a)]),s("，但使用代码前，需要先将图像储存在对应文件夹内：")]),C,e("ul",null,[e("li",null,[s("pretrained_model_name_or_path：建议选择 "),e("a",v,[s("runwayml/stable-diffusion-v1-5"),o(a)]),s("。有个 trick，个人尝试，如果使用 "),e("a",k,[s("realisticVision checkpoint"),o(a)]),s(" 进行图片生成，在 realisticVision checkpoint 上微调的 lora，效果不如在 SD-1.5 上微调的，不确定是什么原因。")]),w,E,e("li",null,[s("learning_rate 及 lr_scheduler：学习率参考了 "),e("a",x,[s("How to Create a LoRA Part 2: Training the Model"),o(a)])]),A,q]),T,e("ul",null,[e("li",null,[s("checkpoint 使用 "),e("a",I,[s("realisticVision checkpoint"),o(a)]),s(" 的效果，会比使用 SD1.5 效果好很多（尽管我们的 lora 是在 SD1.5 上训练的）")]),e("li",null,[s("SD VAE 采用 "),e("a",L,[s("vae-ft-mse-840000-ema-pruned.ckpt"),o(a)])]),R,V,e("li",null,[s("prompt 中，可以通过改动 prompt 来修改图片风格，建议添加上 "),e("a",F,[s("realisticVision checkpoint"),o(a)]),s(" 推荐的 prompt：")]),S,U]),D,e("ol",null,[e("li",null,[s("日常风景照："),e("a",O,[s("海边"),o(a)]),s("，"),e("a",z,[s("花海"),o(a)]),s("， "),e("a",N,[s("都市"),o(a)]),s("，居家")]),e("li",null,[s("传统服饰："),e("a",M,[s("汉服"),o(a)]),s("，"),e("a",B,[s("古装照"),o(a)]),s("，"),e("a",G,[s("花想容/Chinese style/古风/中国風です Lora"),o(a)])]),e("li",null,[s("其他 cos："),e("a",P,[s("歌手"),o(a)]),s("，"),e("a",W,[s("赛博朋克"),o(a)])])]),e("p",null,[s("当使用 realisticVision 效果不佳时，可以考虑换一下 checkpoint 比如 "),e("a",j,[s("majicMIX"),o(a)]),s(" 或 "),e("a",H,[s("Beautiful Realistic Asians"),o(a)])]),X,e("p",null,[e("a",K,[s("kohya_ss"),o(a)])]),e("p",null,[e("a",Y,[s("How to Create a LoRA Part 1: Dataset Preparation"),o(a)])]),e("p",null,[e("a",Q,[s("How to Create a LoRA Part 2: Training the Model"),o(a)])]),e("p",null,[e("a",Z,[s("Generate Studio Quality Realistic Photos By Kohya LoRA Stable Diffusion Training"),o(a)]),s(":")]),e("p",null,[e("a",J,[s("First Ever SDXL Training With Kohya LoRA - Stable Diffusion XL Training Will Replace Older Models"),o(a)]),s(":")]),e("p",null,[e("a",$,[s("DO FINE-TUNING WITH LORA"),o(a)])])])}const te=t(p,[["render",ee],["__file","笔记miaoya.html.vue"]]);export{te as default};
