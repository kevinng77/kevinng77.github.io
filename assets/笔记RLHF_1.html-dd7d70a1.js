const e=JSON.parse('{"key":"v-c4e4e6e4","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0RLHF_1.html","title":"RLHF 基础","lang":"zh-CN","frontmatter":{"title":"RLHF 基础","date":"2024-04-02T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC","LLM","Agent"],"description":"强化学习与 RLHF 本文基于 HuggingFace 推出的 Reinforcement Learning Course 进行了整理，旨在记录强化学习的基础知识，为理解 RLHF（Reinforcement Learning from Human Feedback）打下基础。需要强调的是，以下内容仅涵盖强化学习的基础概念，并非全面的强化学习教程。此外，这些内容已经过人工智能的润色处理。","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0RLHF_1.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"RLHF 基础"}],["meta",{"property":"og:description","content":"强化学习与 RLHF 本文基于 HuggingFace 推出的 Reinforcement Learning Course 进行了整理，旨在记录强化学习的基础知识，为理解 RLHF（Reinforcement Learning from Human Feedback）打下基础。需要强调的是，以下内容仅涵盖强化学习的基础概念，并非全面的强化学习教程。此外，这些内容已经过人工智能的润色处理。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:tag","content":"Agent"}],["meta",{"property":"article:published_time","content":"2024-04-02T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"RLHF 基础\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-04-02T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"强化学习基础","slug":"强化学习基础","link":"#强化学习基础","children":[{"level":3,"title":"奖励与折现","slug":"奖励与折现","link":"#奖励与折现","children":[]},{"level":3,"title":"探索与利用的权衡","slug":"探索与利用的权衡","link":"#探索与利用的权衡","children":[]},{"level":3,"title":"任务类型","slug":"任务类型","link":"#任务类型","children":[]},{"level":3,"title":"策略","slug":"策略","link":"#策略","children":[]},{"level":3,"title":"基于策略的方法","slug":"基于策略的方法","link":"#基于策略的方法","children":[]},{"level":3,"title":"基于值的方法","slug":"基于值的方法","link":"#基于值的方法","children":[]}]},{"level":2,"title":"强化学习进阶：Q 学习与深度 Q 网络（DQN）","slug":"强化学习进阶-q-学习与深度-q-网络-dqn","link":"#强化学习进阶-q-学习与深度-q-网络-dqn","children":[{"level":3,"title":"Q Learning","slug":"q-learning","link":"#q-learning","children":[]},{"level":3,"title":"深度 Q 网络（DQN）","slug":"深度-q-网络-dqn","link":"#深度-q-网络-dqn","children":[]},{"level":3,"title":"总结","slug":"总结","link":"#总结","children":[]}]},{"level":2,"title":"强化学习进阶：策略梯度（Policy Gradient）","slug":"强化学习进阶-策略梯度-policy-gradient","link":"#强化学习进阶-策略梯度-policy-gradient","children":[{"level":3,"title":"策略梯度方法","slug":"策略梯度方法","link":"#策略梯度方法","children":[]},{"level":3,"title":"策略梯度定理","slug":"策略梯度定理","link":"#策略梯度定理","children":[]},{"level":3,"title":"REINFORCE 算法","slug":"reinforce-算法","link":"#reinforce-算法","children":[]},{"level":3,"title":"总结","slug":"总结-1","link":"#总结-1","children":[]}]},{"level":2,"title":"Advantage Actor-Critic, (A2C）","slug":"advantage-actor-critic-a2c","link":"#advantage-actor-critic-a2c","children":[{"level":3,"title":"A2C 简介","slug":"a2c-简介","link":"#a2c-简介","children":[]},{"level":3,"title":"A2C 的目标","slug":"a2c-的目标","link":"#a2c-的目标","children":[]},{"level":3,"title":"A2C 的关键组件","slug":"a2c-的关键组件","link":"#a2c-的关键组件","children":[]},{"level":3,"title":"A2C 的算法流程","slug":"a2c-的算法流程","link":"#a2c-的算法流程","children":[]},{"level":3,"title":"A2C 的优势","slug":"a2c-的优势","link":"#a2c-的优势","children":[]},{"level":3,"title":"总结","slug":"总结-2","link":"#总结-2","children":[]}]},{"level":2,"title":"PPO","slug":"ppo","link":"#ppo","children":[{"level":3,"title":"PPO 简介","slug":"ppo-简介","link":"#ppo-简介","children":[]},{"level":3,"title":"PPO 的目标","slug":"ppo-的目标","link":"#ppo-的目标","children":[]},{"level":3,"title":"PPO 的算法流程","slug":"ppo-的算法流程","link":"#ppo-的算法流程","children":[]},{"level":3,"title":"PPO 的优势","slug":"ppo-的优势","link":"#ppo-的优势","children":[]},{"level":3,"title":"总结","slug":"总结-3","link":"#总结-3","children":[]}]},{"level":2,"title":"RLHF","slug":"rlhf","link":"#rlhf","children":[{"level":3,"title":"Step2 Train Reward Model","slug":"step2-train-reward-model","link":"#step2-train-reward-model","children":[]},{"level":3,"title":"Step 3 PPO","slug":"step-3-ppo","link":"#step-3-ppo","children":[]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":17.06,"words":5118},"filePathRelative":"posts/notes/articles/笔记RLHF_1.md","localizedDate":"2024年4月2日","excerpt":"<h1> 强化学习与 RLHF</h1>\\n<p>本文基于 HuggingFace 推出的  <a href=\\"https://huggingface.co/learn/deep-rl-course/unit0/introduction\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Reinforcement Learning Course</a>  进行了整理，旨在记录强化学习的基础知识，为理解 RLHF（Reinforcement Learning from Human Feedback）打下基础。需要强调的是，以下内容仅涵盖强化学习的基础概念，并非全面的强化学习教程。此外，这些内容已经过人工智能的润色处理。</p>","autoDesc":true}');export{e as data};
