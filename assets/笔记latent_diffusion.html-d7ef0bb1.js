import{_ as o,E as p,S as i,W as l,$ as n,a3 as s,Z as e,aS as t}from"./framework-d5c0d2cb.js";const c={},r=n("p",null,"相对于 DDIM, DDPM 以及 SDE，High-Resolution Image Synthesis with Latent Diffusion Models 一文重点在于 latent Space 和 Conditioning Cross Attention，而非 diffusion pipeline 流程。",-1),u={href:"https://zhuanlan.zhihu.com/p/659212489/huggingface/diffusers",target:"_blank",rel:"noopener noreferrer"},d=n("p",null,"系列笔记",-1),k={href:"https://zhuanlan.zhihu.com/p/650495280",target:"_blank",rel:"noopener noreferrer"},m={href:"https://zhuanlan.zhihu.com/p/650614674",target:"_blank",rel:"noopener noreferrer"},h={href:"https://zhuanlan.zhihu.com/p/655679978",target:"_blank",rel:"noopener noreferrer"},_=t(`<h2 id="latent-diffusion-model" tabindex="-1"><a class="header-anchor" href="#latent-diffusion-model" aria-hidden="true">#</a> Latent Diffusion Model</h2><p>论文：High-Resolution Image Synthesis with Latent Diffusion Models</p><figure><img src="https://pic3.zhimg.com/80/v2-da826549375793c4f8472a54a14c1616_1440w.webp" alt="LDM 架构图" tabindex="0" loading="lazy"><figcaption>LDM 架构图</figcaption></figure><h3 id="ldm-主要思想" tabindex="-1"><a class="header-anchor" href="#ldm-主要思想" aria-hidden="true">#</a> LDM 主要思想</h3><p>扩散模型（DMs）直接在像素领域工作，优化和推断都很费时。为了在有限的计算资源上训练它们，LDM 先使用一个预训练好的 AutoEncoder，将图片像素转换到了维度较小的 latent space 上，而后再进行传统的扩散模型推理与优化。这种训练方式使得 LDM 在算力和性能之间得到了平衡。</p><p>此外，通过引入交叉注意力，使得 DMs 能够在条件生成上有不错的效果，包括如文字生成图片，inpainting 等。</p><h3 id="ldm-使用示例" tabindex="-1"><a class="header-anchor" href="#ldm-使用示例" aria-hidden="true">#</a> <strong>LDM 使用示例</strong></h3><p>huggingface Diffusers 将各种 Diffusion Model Pipeline 都包装好了，使用 Diffusion model 就和使用 Transformers 一样地方便：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> diffusers <span class="token keyword">import</span> DiffusionPipeline

<span class="token comment"># load model and scheduler</span>
ldm <span class="token operator">=</span> DiffusionPipeline<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;CompVis/ldm-text2im-large-256&quot;</span><span class="token punctuation">)</span>
 <span class="token comment"># run pipeline in inference (sample random noise and denoise)</span>
prompt <span class="token operator">=</span> <span class="token string">&quot;A painting of a squirrel eating a burger&quot;</span>

images <span class="token operator">=</span> ldm<span class="token punctuation">(</span><span class="token punctuation">[</span>prompt<span class="token punctuation">]</span><span class="token punctuation">,</span> num_inference_steps<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> eta<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> guidance_scale<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">.</span>images

<span class="token comment"># save images</span>
<span class="token keyword">for</span> idx<span class="token punctuation">,</span> image <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>images<span class="token punctuation">)</span><span class="token punctuation">:</span>
    image<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;squirrel-</span><span class="token interpolation"><span class="token punctuation">{</span>idx<span class="token punctuation">}</span></span><span class="token string">.png&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="ldm-pipeline" tabindex="-1"><a class="header-anchor" href="#ldm-pipeline" aria-hidden="true">#</a> LDM Pipeline</h3><p>LDM 的 pipeline 可以简化表示为：<code>Pipeline(prompt, num_inference_steps, latents)</code>。我们暂时考虑没有 negative prompt 和 初始 latent 的输入，那么整个采样过程大致可以表示为：</p><ol><li>首先采用了 BERT 架构模型对 prompt 进行处理，生成 <code>text_hidden_state</code>；同时生成随机噪声 <code>latents</code>。</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>text_hidden_state <span class="token operator">=</span> LDMBERT<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span> <span class="token comment"># shape=[bs, len_seq, d_model] = [1, 77, 1280] </span>
latents <span class="token operator">=</span> randn_tensor<span class="token punctuation">(</span>latents_shape<span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,13),g=n("code",null,'"CompVis/ldm-text2im-large-256"',-1),f=n("code",null,"LDMBert",-1),v={href:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",target:"_blank",rel:"noopener noreferrer"},b=n("code",null,"LDMBert",-1),D=n("code",null,"[batch_size, 77, 1280]",-1),y=t(`<ol start="2"><li>之后进行传统的扩散模型 backward process：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>progress_bar<span class="token punctuation">(</span>self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>timesteps<span class="token punctuation">)</span><span class="token punctuation">:</span>
    noise_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>unet<span class="token punctuation">(</span>latents_input<span class="token punctuation">,</span> t<span class="token punctuation">,</span> encoder_hidden_states<span class="token operator">=</span>context<span class="token punctuation">)</span><span class="token punctuation">.</span>sample
    <span class="token comment"># compute the previous noisy sample x_t -&gt; x_t-1</span>
    latents <span class="token operator">=</span> self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>noise_pred<span class="token punctuation">,</span> t<span class="token punctuation">,</span> latents<span class="token punctuation">,</span> <span class="token operator">**</span>extra_kwargs<span class="token punctuation">)</span><span class="token punctuation">.</span>prev_sample
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中 UNET 为 <code>UNet2DConditionModel</code>，与传统 Unet 不同在于其应用了 Cross Attention 对文字以及图片信息进行综合处理，下文会对改模块做梳理。scheduler 可以选 DDIM 或者其他算法。</p><ol start="3"><li>最后对 latent hidden state 进行 decode，生成图片：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>latents <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>vqvae<span class="token punctuation">.</span>config<span class="token punctuation">.</span>scaling_factor <span class="token operator">*</span> latents
image <span class="token operator">=</span> self<span class="token punctuation">.</span>vqvae<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>latents<span class="token punctuation">)</span><span class="token punctuation">.</span>sample
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="ldm-中的-unet" tabindex="-1"><a class="header-anchor" href="#ldm-中的-unet" aria-hidden="true">#</a> LDM 中的 UNET</h4><p>backward process 中的 <code>self.unet(...)</code>，即 <code>UNET2DCondition(sample, timestep, encoder_hidden_state)</code> 前向推导可以看成五部分，（以下以 <code>CompVis/ldm-text2im-large-256</code> 为例介绍）：</p><ul><li><strong>准备 time steps</strong> ：Timesteps 编码信息是 diffusion 中 predict noise residual 模型的标配：</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 经过两次映射得到 timesteps 对应的 embedding</span>
t_emb <span class="token operator">=</span> self<span class="token punctuation">.</span>time_proj<span class="token punctuation">(</span>timesteps<span class="token punctuation">)</span>
emb <span class="token operator">=</span> self<span class="token punctuation">.</span>time_embedding<span class="token punctuation">(</span>t_emb<span class="token punctuation">,</span> timestep_cond<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>pre-process：</strong> LDM 只用了一个 2D 卷积对输入的 hidden state 进行处理</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>sample <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>
            in_channels<span class="token punctuation">,</span> block_out_channels<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>conv_in_kernel<span class="token punctuation">,</span> padding<span class="token operator">=</span>conv_in_padding
        <span class="token punctuation">)</span><span class="token punctuation">(</span>sample<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>down sampling</strong> ：down sampling 包括了 3 个 <code>CrossAttnDownBlock2D</code>, 和 1 个 <code>DownBlock2D</code>。</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># down sampling 大致前向推导</span>
down_block_res_samples <span class="token operator">=</span> <span class="token punctuation">(</span>sample<span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> downsample_block <span class="token keyword">in</span> self<span class="token punctuation">.</span>down_blocks<span class="token punctuation">:</span>
    sample<span class="token punctuation">,</span> res_samples <span class="token operator">=</span> downsample_block<span class="token punctuation">(</span>hidden_states<span class="token operator">=</span>sample<span class="token punctuation">,</span> temb<span class="token operator">=</span>emb<span class="token punctuation">,</span> scale<span class="token operator">=</span>lora_scale<span class="token punctuation">)</span>
    <span class="token comment"># 用于 UNET 的残差链接</span>
    down_block_res_samples <span class="token operator">+=</span> res_samples
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中每个 <code>CrossAttnDownBlock2D</code> 大概前向过程为：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># CrossAttnDownBlock2D</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">,</span> temb<span class="token punctuation">,</span> encoder_hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
	output_states <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> resnet<span class="token punctuation">,</span> attn <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>resnets<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attentions<span class="token punctuation">)</span><span class="token punctuation">:</span>
        hidden_states <span class="token operator">=</span> resnet<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> temb<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> attn<span class="token punctuation">(</span>
            hidden_states<span class="token punctuation">,</span>
            encoder_hidden_states<span class="token operator">=</span>encoder_hidden_states<span class="token punctuation">,</span>
            cross_attention_kwargs<span class="token operator">=</span>cross_attention_kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span><span class="token punctuation">.</span>sample
        output_states <span class="token operator">+=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

    <span class="token comment"># downsampler = Conv2D </span>
    hidden_states <span class="token operator">=</span> downsampler<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
    output_states <span class="token operator">+=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> hidden_states<span class="token punctuation">,</span> output_states
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在 <code>CompVis/ldm-text2im-large-256</code> 中，每个 <code>CrossAttnDownBlock2D</code> 包含了 2 个 <code>attn</code>（<code>Transformer2DModel</code>）以及 2 个 <code>resnet</code> （<code>ResnetBlock2D</code>）。</p>`,16),x=n("code",null,"Transformer2DModel",-1),w={href:"https://github.com/huggingface/diffusers/blob/16b9a57d29b6dbce4f97dbf439af1663d2c54588/src/diffusers/models/transformer_2d.py#L44C6-L44C6",target:"_blank",rel:"noopener noreferrer"},L=n("code",null,"(batch_size, channel, width, height)",-1),M=n("code",null,"(batch_size, num_image_vectors)",-1),C=n("code",null,"(batch_size, len_seq, hidden_size)",-1),B=n("code",null,"hidden_states",-1),A=n("code",null,"hidden_states",-1),S=n("code",null,"encoder_hidden_states",-1),T=t(`<ul><li><strong>mid processing:</strong></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>sample <span class="token operator">=</span> MidBlock2DCrossAttn<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>sample<span class="token punctuation">,</span> 
                              emb<span class="token punctuation">,</span>
                           encoder_hidden_states<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2),z=n("code",null,"CompVis/ldm-text2im-large-256",-1),q={href:"https://github.com/huggingface/diffusers/blob/16b9a57d29b6dbce4f97dbf439af1663d2c54588/src/diffusers/models/unet_2d_blocks.py#L572",target:"_blank",rel:"noopener noreferrer"},I=n("code",null,"MidBlock2DCrossAttn",-1),N=n("code",null,"Transformer2DModel",-1),U=n("code",null,"resnet",-1),E=n("code",null,"ResnetBlock2D",-1),R=t(`<ul><li><strong>upsampling</strong> ：upsampling 采用的模块 UpBlocks 包括了 <code> (&quot;UpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;)</code>，各个模块的架构与 down sampling 中的模块相似。</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># upsample_block</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> upsample_block <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>up_blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
    sample <span class="token operator">=</span> upsample_block<span class="token punctuation">(</span>
                    hidden_states<span class="token operator">=</span>sample<span class="token punctuation">,</span>
                    temb<span class="token operator">=</span>emb<span class="token punctuation">,</span>
                    res_hidden_states_tuple<span class="token operator">=</span>res_samples<span class="token punctuation">,</span>
                    upsample_size<span class="token operator">=</span>upsample_size<span class="token punctuation">,</span>
                    scale<span class="token operator">=</span>lora_scale<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>post-process</strong></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># GroupNorm</span>
sample <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_norm_out<span class="token punctuation">(</span>sample<span class="token punctuation">)</span>
<span class="token comment"># Silu</span>
sample <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_act<span class="token punctuation">(</span>sample<span class="token punctuation">)</span>
<span class="token comment"># Conv2d(320, 4, kernel=(3,3), s=(1,1), padding=(1,1))</span>
sample <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_out<span class="token punctuation">(</span>sample<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>总结起来，down sampling，midprocess，upsampling 三个步骤中都涉及到了 <code>Transformer2DModel</code> ，实现多模态的信息交互。</p><h3 id="ldm-super-resolution-pipeline" tabindex="-1"><a class="header-anchor" href="#ldm-super-resolution-pipeline" aria-hidden="true">#</a> LDM Super Resolution Pipeline</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>low_res_img <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>BytesIO<span class="token punctuation">(</span>response<span class="token punctuation">.</span>content<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">&quot;RGB&quot;</span><span class="token punctuation">)</span>
low_res_img <span class="token operator">=</span> low_res_img<span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>

upscaled_image <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>low_res_img<span class="token punctuation">,</span> num_inference_steps<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> eta<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>images<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
upscaled_image<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">&quot;ldm_generated_image.png&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>大致前项推导流程可以概括为：</p><ol><li>根据 输入图片大小，生成对应的 latent 噪音以及 time step embedding：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>latents <span class="token operator">=</span> randn_tensor<span class="token punctuation">(</span>latents_shape<span class="token punctuation">,</span> generator<span class="token operator">=</span>generator<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>latents_dtype<span class="token punctuation">)</span>  <span class="token comment"># shape 与输入图片相同</span>
latents <span class="token operator">=</span> latents <span class="token operator">*</span> self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>init_noise_sigma
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>将 latent 与原始图片拼接，然后进行 diffusion 反向推导：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>progress_bar<span class="token punctuation">(</span>timesteps_tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># concat latents and low resolution image in the channel dimension.</span>
    latents_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>latents<span class="token punctuation">,</span> image<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    latents_input <span class="token operator">=</span> self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>scale_model_input<span class="token punctuation">(</span>latents_input<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
    <span class="token comment"># predict the noise residual</span>
    noise_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>unet<span class="token punctuation">(</span>latents_input<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">.</span>sample
    <span class="token comment"># compute the previous noisy sample x_t -&gt; x_t-1</span>
    latents <span class="token operator">=</span> self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>noise_pred<span class="token punctuation">,</span> t<span class="token punctuation">,</span> latents<span class="token punctuation">,</span> <span class="token operator">**</span>extra_kwargs<span class="token punctuation">)</span><span class="token punctuation">.</span>prev_sample
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="3"><li>使用 vqvae 对 latent 进行解码，得到最终图片</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># decode the image latents with the VQVAE</span>
image <span class="token operator">=</span> self<span class="token punctuation">.</span>vqvae<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>latents<span class="token punctuation">)</span><span class="token punctuation">.</span>sample
image <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>image<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span>
image <span class="token operator">=</span> image <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">0.5</span>
image <span class="token operator">=</span> image<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="stable-diffusion" tabindex="-1"><a class="header-anchor" href="#stable-diffusion" aria-hidden="true">#</a> Stable diffusion</h2><h3 id="sd-v1-架构" tabindex="-1"><a class="header-anchor" href="#sd-v1-架构" aria-hidden="true">#</a> SD v1 架构</h3>`,16),P={href:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",target:"_blank",rel:"noopener noreferrer"},V=n("code",null,"stable-diffusion-v1-5",-1),F=n("ol",null,[n("li",null,[n("strong",null,"Text Encoder")])],-1),O=n("code",null,"CLIPTextModel",-1),j={href:"https://arxiv.org/pdf/2103.00020.pdf",target:"_blank",rel:"noopener noreferrer"},G=n("code",null,"[batch_size, 77, 768]",-1),H=t(`<ol start="2"><li><strong>Diffusion 反向采样过程</strong></li></ol><p>SD v1.5 采样过程与 LDM 相似，其中的 latents 大小为 <code>[bs, 4, 64, 64]</code>。对于 txt2img，latents 通过随机生成，对于 img2img，latents 通过 VAE 模型进行 encode。</p><p>Unet 配置与 LDM 相似：</p><ul><li><p>down sampling 采用 3 个 <code>CrossAttnDownBlock2D</code>, 和 1 个 <code>DownBlock2D</code>。</p></li><li><p>mid block 采用 1 个 <code>MidBlock2DCrossAttn</code>。hidden size = 1280</p></li><li><p>Up sampling 采用 1 个 <code>UpBlock2D</code> + 3 个 <code>CrossAttnUpBlock2D</code></p></li></ul><p>每个 CrossAttn 的 transformer 中， text embedding 大小为 768，但 Transformer 模块的 <code>hidden size</code> 随着 Unet 深入而增加。如 down sampling 采用的维度为 320, 640, 1280, 1280。那么 3 个 Transformer 模块中的 hidden size 就分别是 320, 640, 1280。</p><p>以 down sampling 为例，在进行 cross attention 时候，图像的 hidden state （latent）大小分别被映射到了 <code>[4096, 320]</code>，<code>[2014, 640]</code>，<code>[256, 1280]</code> ，而后与文字的 hidden state <code>[77, 768]</code> 进行 cross attention 计算。（以上张量维度省略了 batch size）</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># hidden size 为 320 时候的 cross attention 单元示例</span>
Attention<span class="token punctuation">(</span>
<span class="token punctuation">(</span>to_q<span class="token punctuation">)</span><span class="token punctuation">:</span> LoRACompatibleLinear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">320</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">320</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>to_k<span class="token punctuation">)</span><span class="token punctuation">:</span> LoRACompatibleLinear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">320</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>to_v<span class="token punctuation">)</span><span class="token punctuation">:</span> LoRACompatibleLinear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">320</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这也是 SD Unet 中 Transformer2DBlock 与传统 Transformer 主要的不同，SD Unet 中的 Transformer2DBlock 输入与输出维度是不一样的。</p><ol start="3"><li><strong>super resolution</strong></li></ol><p>生成后 latent 大小为 64 * 64， 通过 VQModel 解码为 512*512</p><h3 id="sd-v1-1-v1-5" tabindex="-1"><a class="header-anchor" href="#sd-v1-1-v1-5" aria-hidden="true">#</a> SD v1.1 - v1.5</h3>`,11),K={href:"https://github.com/runwayml/stable-diffusion#weights",target:"_blank",rel:"noopener noreferrer"},W={href:"https://huggingface.co/compvis",target:"_blank",rel:"noopener noreferrer"},Q=n("code",null,"sd-v1-1.ckpt",-1),Z=n("code",null,"256x256",-1),$={href:"https://huggingface.co/datasets/laion/laion2B-en",target:"_blank",rel:"noopener noreferrer"},J=n("code",null,"512x512",-1),X={href:"https://huggingface.co/datasets/laion/laion-high-resolution",target:"_blank",rel:"noopener noreferrer"},Y=n("code",null,">= 1024x1024",-1),nn={href:"https://huggingface.co/compvis",target:"_blank",rel:"noopener noreferrer"},sn=n("code",null,"sd-v1-2.ckpt",-1),an=n("code",null,"sd-v1-1.ckpt",-1),en=n("code",null,"512x512",-1),tn={href:"https://laion.ai/blog/laion-aesthetics/",target:"_blank",rel:"noopener noreferrer"},on=n("code",null,"> 5.0",-1),pn=n("code",null,">= 512x512",-1),ln=n("code",null,"< 0.5",-1),cn={href:"https://laion.ai/blog/laion-5b/",target:"_blank",rel:"noopener noreferrer"},rn={href:"https://github.com/christophschuhmann/improved-aesthetic-predictor",target:"_blank",rel:"noopener noreferrer"},un={href:"https://huggingface.co/compvis",target:"_blank",rel:"noopener noreferrer"},dn=n("code",null,"sd-v1-3.ckpt",-1),kn=n("code",null,"sd-v1-2.ckpt",-1),mn=n("code",null,"512x512",-1),hn={href:"https://arxiv.org/abs/2207.12598",target:"_blank",rel:"noopener noreferrer"},_n={href:"https://huggingface.co/compvis",target:"_blank",rel:"noopener noreferrer"},gn=n("code",null,"sd-v1-4.ckpt",-1),fn=n("code",null,"sd-v1-2.ckpt",-1),vn=n("code",null,"512x512",-1),bn={href:"https://arxiv.org/abs/2207.12598",target:"_blank",rel:"noopener noreferrer"},Dn={href:"https://huggingface.co/runwayml/stable-diffusion-v1-5",target:"_blank",rel:"noopener noreferrer"},yn=n("code",null,"sd-v1-5.ckpt",-1),xn=n("code",null,"sd-v1-2.ckpt",-1),wn=n("code",null,"512x512",-1),Ln={href:"https://arxiv.org/abs/2207.12598",target:"_blank",rel:"noopener noreferrer"},Mn={href:"https://huggingface.co/runwayml/stable-diffusion-inpainting",target:"_blank",rel:"noopener noreferrer"},Cn=n("code",null,"sd-v1-5-inpainting.ckpt",-1),Bn=n("code",null,"sd-v1-5.ckpt",-1),An=n("code",null,"512x512",-1),Sn={href:"https://arxiv.org/abs/2207.12598",target:"_blank",rel:"noopener noreferrer"},Tn=n("h3",{id:"sd-v2",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#sd-v2","aria-hidden":"true"},"#"),s(" SD v2")],-1),zn={href:"https://github.com/Stability-AI/stablediffusion",target:"_blank",rel:"noopener noreferrer"},qn=n("p",null,"架构方面 SD v2 系列：",-1),In={href:"https://github.com/mlfoundations/open_clip",target:"_blank",rel:"noopener noreferrer"},Nn=n("li",null,[s("Unet 架构改变：其中 Transformer 模块中的 "),n("code",null,"attention_head_dim"),s(" 变为了 "),n("code",null,"5,10,20,20"),s("，SD v1 中为 "),n("code",null,"8,8,8,8"),s("。"),n("code",null,"cross_attention_dim"),s(" 从 768 变为 1280。同时在 latent hidden state 进入 cross attention 之前，额外采用了 "),n("code",null,"linear_projection"),s(" 进行 latent hidden state 的处理，SD v1 中为卷积层处理。")],-1),Un=n("p",null,"训练方面 SD v2 系列，（以下拷贝了 huggingface 中 SD 模型 model card 的介绍） ：",-1),En={href:"https://huggingface.co/stabilityai/stable-diffusion-2-base",target:"_blank",rel:"noopener noreferrer"},Rn=n("code",null,"256x256",-1),Pn={href:"https://laion.ai/blog/laion-5b/",target:"_blank",rel:"noopener noreferrer"},Vn={href:"https://github.com/LAION-AI/CLIP-based-NSFW-Detector",target:"_blank",rel:"noopener noreferrer"},Fn=n("code",null,"punsafe=0.1",-1),On={href:"https://github.com/christophschuhmann/improved-aesthetic-predictor",target:"_blank",rel:"noopener noreferrer"},jn=n("code",null,"4.5",-1),Gn=n("code",null,"512x512",-1),Hn=n("code",null,">= 512x512",-1),Kn={href:"https://huggingface.co/stabilityai/stable-diffusion-2",target:"_blank",rel:"noopener noreferrer"},Wn=n("code",null,"stable-diffusion-2",-1),Qn={href:"https://huggingface.co/stabilityai/stable-diffusion-2-base",target:"_blank",rel:"noopener noreferrer"},Zn=n("code",null,"512-base-ema.ckpt",-1),$n={href:"https://arxiv.org/abs/2202.00512",target:"_blank",rel:"noopener noreferrer"},Jn=n("code",null,"768x768",-1),Xn={href:"https://huggingface.co/stabilityai/stable-diffusion-2-1",target:"_blank",rel:"noopener noreferrer"},Yn=n("code",null,"stable-diffusion-2-1",-1),ns={href:"https://huggingface.co/stabilityai/stable-diffusion-2",target:"_blank",rel:"noopener noreferrer"},ss=n("code",null,"768-v-ema.ckpt",-1),as=n("code",null,"punsafe=0.1",-1),es=n("code",null,"punsafe=0.98",-1),ts=n("h2",{id:"lora",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#lora","aria-hidden":"true"},"#"),s(" Lora")],-1),os={href:"https://link.zhihu.com/?target=https%3A//huggingface.co/docs/diffusers/training/lora",target:"_blank",rel:"noopener noreferrer"},ps=n("p",null,[s("Diffusers 中，SD 采用 Lora 的部分位于 Unet 当中，大部分的 Lora 在 Transformer 模块当中，SD 的 lora 与 NLP Lora 实现方式基本相同， "),n("strong",null,"一个较大的区别在于，SD 中的 Lora 除了对线性层进行 Lora 叠加外，也对卷积层进行了 Lora 改造"),s(" 。")],-1);function is(ls,cs){const a=p("ExternalLinkIcon");return i(),l("div",null,[r,n("p",null,[s("以此不同于前几份笔记，本文主要参考 "),n("a",u,[s("huggingface/diffusers"),e(a)]),s(" 中 Latent Diffusion Model 及 Stable Diffusion 的实现，对 LDM 架构及其中的 Conditioning Cross Attention 做梳理。")]),d,n("ul",null,[n("li",null,[n("a",k,[s("Kevin 吴嘉文：Diffusion|DDPM 理解、数学、代码"),e(a)])]),n("li",null,[n("a",m,[s("Kevin 吴嘉文：DIFFUSION 系列笔记|DDIM 数学、思考与 ppdiffuser 代码探索"),e(a)])]),n("li",null,[n("a",h,[s("Kevin 吴嘉文：DIFFUSION 系列笔记| SDE（上）"),e(a)])])]),_,n("p",null,[s("对于 "),g,s("，其中使用了 "),f,s("， 参考 "),n("a",v,[s("huggignface 的 LDMBert"),e(a)]),s(" 实现，"),b,s(" 与传统 BERT 架构相似，规模不同，LDMBert 采用 32 层， hidden_size 为 1280，属实比 bert-base 大上不少。同时文本被 padding 到了固定的 77 长度，以此来保证文字的 hidden state 格式为 "),D,s("。")]),y,n("p",null,[s("文字与图像的交互就发生在 "),x,s(" 当中。每个 "),n("a",w,[s("Transformer2DModel"),e(a)]),s(" 先对输入的图像数据进行预处理，将图片格式从如 "),L,s(" 或 "),M,s(" 转换为 "),C,s("，而后将 "),B,s(" 传入 1 层传统 Transformer layer（非 bert 或 GPT 类型），先对图像 "),A,s(" 进行 self-attention，而后结合 "),S,s(" 进行 cross attention 处理。")]),T,n("p",null,[s("在 "),z,s(" 中，upsampling 和 down sampling 之间采用 "),n("a",q,[s("MidBlock2DCrossAttn"),e(a)]),s(" 连接，"),I,s(" 包括了 1 个 1 层的 "),N,s(" 以及 1 个 "),U,s(),E,s("。")]),R,n("p",null,[s("参考 "),n("a",P,[s("hugging face diffuser 的 SD pipeline 实现"),e(a)]),s("。以 "),V,s(" 为例。")]),F,n("p",null,[s("采用 "),O,s("，来自于 "),n("a",j,[s("CLIP"),e(a)]),s(" 的 Text Encoder 部分。相比于其他传统的 Transformer 语言模型，CLIP 在预训练时，在 text-image pair 数据集上进行了对比学习预训练。prompt_embeds, negative_prompt_embeds 在经过编码后，shape 都为 "),G]),H,n("p",null,[s("stable diffusion 1.1-1.5 的模型架构相同，以下搬运 "),n("a",K,[s("runwayml"),e(a)]),s(" 的 stable diffusion weights 总结：")]),n("ul",null,[n("li",null,[n("p",null,[n("a",W,[Q,e(a)]),s(": 237k steps at resolution "),Z,s(" on "),n("a",$,[s("laion2B-en"),e(a)]),s(". 194k steps at resolution "),J,s(" on "),n("a",X,[s("laion-high-resolution"),e(a)]),s(" (170M examples from LAION-5B with resolution "),Y,s(").")])]),n("li",null,[n("p",null,[n("a",nn,[sn,e(a)]),s(": Resumed from "),an,s(". 515k steps at resolution "),en,s(" on "),n("a",tn,[s("laion-aesthetics v2 5+"),e(a)]),s(" (a subset of laion2B-en with estimated aesthetics score "),on,s(", and additionally filtered to images with an original size "),pn,s(", and an estimated watermark probability "),ln,s(". The watermark estimate is from the "),n("a",cn,[s("LAION-5B"),e(a)]),s(" metadata, the aesthetics score is estimated using the "),n("a",rn,[s("LAION-Aesthetics Predictor V2"),e(a)]),s(").")])]),n("li",null,[n("p",null,[n("a",un,[dn,e(a)]),s(": Resumed from "),kn,s(". 195k steps at resolution "),mn,s(' on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve '),n("a",hn,[s("classifier-free guidance sampling"),e(a)]),s(".")])]),n("li",null,[n("p",null,[n("a",_n,[gn,e(a)]),s(": Resumed from "),fn,s(". 225k steps at resolution "),vn,s(' on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve '),n("a",bn,[s("classifier-free guidance sampling"),e(a)]),s(".")])]),n("li",null,[n("p",null,[n("a",Dn,[yn,e(a)]),s(": Resumed from "),xn,s(". 595k steps at resolution "),wn,s(' on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve '),n("a",Ln,[s("classifier-free guidance sampling"),e(a)]),s(".")])]),n("li",null,[n("p",null,[n("a",Mn,[Cn,e(a)]),s(": Resumed from "),Bn,s(". 440k steps of inpainting training at resolution "),An,s(' on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve '),n("a",Sn,[s("classifier-free guidance sampling"),e(a)]),s(". For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.")])])]),Tn,n("p",null,[s("参考 "),n("a",zn,[s("stability-AI 仓库"),e(a)]),s("，SD v2 相对 v1 系列改动较大：")]),qn,n("ul",null,[n("li",null,[s("采用了 "),n("a",In,[s("OpenCLIP-ViT/H"),e(a)]),s(" 作为 text encoder。")]),Nn]),Un,n("ul",null,[n("li",null,[n("a",En,[s("SD 2.0-base"),e(a)]),s("：The model is trained from scratch 550k steps at resolution "),Rn,s(" on a subset of "),n("a",Pn,[s("LAION-5B"),e(a)]),s(" filtered for explicit pornographic material, using the "),n("a",Vn,[s("LAION-NSFW classifier"),e(a)]),s(" with "),Fn,s(" and an "),n("a",On,[s("aesthetic score"),e(a)]),s(" >= "),jn,s(". Then it is further trained for 850k steps at resolution "),Gn,s(" on the same dataset on images with resolution "),Hn,s(".")]),n("li",null,[n("a",Kn,[s("SD v2.0"),e(a)]),s("：This "),Wn,s(" model is resumed from "),n("a",Qn,[s("stable-diffusion-2-base"),e(a)]),s(" ("),Zn,s(") and trained for 150k steps using a "),n("a",$n,[s("v-objective"),e(a)]),s(" on the same dataset. Resumed for another 140k steps on "),Jn,s(" images.")]),n("li",null,[n("a",Xn,[s("SD v2.1"),e(a)]),s("：This "),Yn,s(" model is fine-tuned from "),n("a",ns,[s("stable-diffusion-2"),e(a)]),s(" ("),ss,s(") with an additional 55k steps on the same dataset (with "),as,s("), and then fine-tuned for another 155k extra steps with "),es,s(".")])]),ts,n("p",null,[s("huggingface diffuser 中 Lora 的实现与 huggingface/PEFT 实现方法相似，添加 Lora 只需要通过撰写规则，锁定需要改动的 layer，并替换为 LoRACompatibleLayer 实现，huggingface 也提供好了 "),n("a",os,[s("lora 训练代码"),e(a)]),s("，和 SD lora 推理方法。")]),ps])}const us=o(c,[["render",is],["__file","笔记latent_diffusion.html.vue"]]);export{us as default};
