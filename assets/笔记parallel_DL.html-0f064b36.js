import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as p,o as t,c as r,a as s,b as a,d as e,f as l}from"./app-c58985cf.js";const c={},i=l(`<h2 id="单机单卡" tabindex="-1"><a class="header-anchor" href="#单机单卡" aria-hidden="true">#</a> 单机单卡</h2><h3 id="梯度累加" tabindex="-1"><a class="header-anchor" href="#梯度累加" aria-hidden="true">#</a> 梯度累加</h3><p>单机单卡下可以通过梯度累加来增大训练时候的理论 batch size。如：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> i, (inputs, labels) </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">enumerate</span><span style="color:#24292E;">(training_set):</span></span>
<span class="line"><span style="color:#24292E;">  loss </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> model(inputs, labels)                   </span></span>
<span class="line"><span style="color:#24292E;">  loss </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> loss </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> accumulation_steps                </span></span>
<span class="line"><span style="color:#24292E;">  loss.backward()                                 </span></span>
<span class="line"><span style="color:#24292E;">  </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> (i</span><span style="color:#D73A49;">+</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">) </span><span style="color:#D73A49;">%</span><span style="color:#24292E;"> accumulation_steps </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">: </span></span>
<span class="line"><span style="color:#24292E;">      global_step </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span></span>
<span class="line"><span style="color:#24292E;">      optimizer.step()                            </span></span>
<span class="line"><span style="color:#24292E;">      optimizer.clean_grad() </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>假设目标复现论文采用 <code>batch size=64</code>，<code>lr=4e-5</code>。复现过程中，在单机单卡上训练时的<code>batch size=16</code>。此时有两种方案达到论文复现效果：</p><ul><li>方案一：小<code>batch=16</code>，小学习率 <code>lr=1e-5</code>，高频率 <code>accumulation_steps=1</code>。这种情况下，假设每个样本平均损失为 <code>m</code> ，并且我们对每个 batch 的 loss 取平均，那么 64 个样本训练完后，模型得以更新的损失为 <code>64/16 * 1e-5 * m= m*4e-5</code></li><li>方案二：小 <code>batch=16</code>， 大学习率 <code>lr=4e-5</code>，低频率<code>accumulation_steps=4</code>。这种情况下，每个 step 处理 16 个样本后，得到的损失为 <code>loss/accumulation_steps=m/4 * 4e-5</code>；每 64 个样本提供的损失为 <code>4e-5 * m</code></li></ul><h3 id="混合精度训练" tabindex="-1"><a class="header-anchor" href="#混合精度训练" aria-hidden="true">#</a> 混合精度训练</h3><p>混合精度训练可以为计算密集型任务带来效率上的提升，若任务为 IO 密集型（Batch 小），那么提升注定是有限的。</p><p>amp 库下采用混合精度训练，大致流程为：</p><p><strong>FP32 权重 -&gt; FP16 权重 -&gt; FP16 计算前向 -&gt; FP32 的 loss，扩大 -&gt; 转为 FP16 -&gt; FP16 反向计算梯度 -&gt; 缩放为 FP32 的梯度更新权重</strong></p>`,10),d={href:"https://zhuanlan.zhihu.com/p/84219777",target:"_blank",rel:"noopener noreferrer"},u=l(`<h2 id="单机多卡" tabindex="-1"><a class="header-anchor" href="#单机多卡" aria-hidden="true">#</a> 单机多卡</h2><h3 id="多-gpu-操作总结" tabindex="-1"><a class="header-anchor" href="#多-gpu-操作总结" aria-hidden="true">#</a> 多 GPU 操作总结</h3><blockquote><p>以下为单卡多 GPU，数据并行计算总结</p></blockquote><p>启动训练：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">python</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-m</span><span style="color:#24292E;"> </span><span style="color:#032F62;">paddle.distributed.launch</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--gpus=0,1,2,3</span><span style="color:#24292E;"> </span><span style="color:#032F62;">run_train.py</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>代码中调用训练函数：</p><ul><li>可以通过 spawn 启动，</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">n_gpu </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">len</span><span style="color:#24292E;">(os.getenv(</span><span style="color:#032F62;">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;&quot;</span><span style="color:#24292E;">).split(</span><span style="color:#032F62;">&quot;,&quot;</span><span style="color:#24292E;">))</span></span>
<span class="line"><span style="color:#24292E;">paddle.distributed.spawn(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.train, </span><span style="color:#E36209;">nprocs</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">n_gpu)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>在训练中需要初始化平行训练</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">if</span><span style="color:#24292E;"> paddle.distributed.get_world_size() </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">    paddle.distributed.init_parallel_env()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>模型加载 <code>model = paddle.DataParellel(model)</code>，在模型加载好权重之后运行。torch 下采用 <code>DDP</code> 加载。</p><p>模型加载权重方式不变，同样可以采用 <code>state_dict</code> 方式加载</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">if</span><span style="color:#24292E;"> paddle.distributed.get_world_size() </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">    paddle.distributed.init_parallel_env()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>distrubutedbatchsampler 在多进程启动的时候，会将数据集根据进程数量进行划分。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">state_dict </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> emb.state_dict()</span></span>
<span class="line"><span style="color:#24292E;">paddle.save(state_dict, </span><span style="color:#032F62;">&quot;paddle_dy.pdparams&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">para_state_dict </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> paddle.load(</span><span style="color:#032F62;">&quot;paddle_dy.pdparams&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">emb.set_state_dict(para_state_dict)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在并行训练下，每个 dataloader 的大小都会变为 <code>total_samples/n_gpu</code> 因此总的训练 step 也会相应的减少，对于 <code>lr_scheduler</code> 会有所影响。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">lr_scheduler </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> paddlenlp.transformers.LinearDecayWithWarmup(</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">learning_rate</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.learning_rate,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">total_steps</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">num_train_steps,</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#E36209;">warmup</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">int</span><span style="color:#24292E;">(num_train_steps </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.warmup_proportion))</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="进程间通信" tabindex="-1"><a class="header-anchor" href="#进程间通信" aria-hidden="true">#</a> 进程间通信</h3><p>进程间通信包括 all_gather, all_reduce 等方法。</p><p>如以下通过 all_gather 方式通信后，每个卡上会有一份所有 GPU 上 loss 的数据，此时仅需要在一个 GPU 上进行操作，如打印日志等。避免重复操作。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 合并多进程数据，进行 logging loss_list=[] losses=paddle.tensor</span></span>
<span class="line"><span style="color:#24292E;">loss_list </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> []</span></span>
<span class="line"><span style="color:#D73A49;">if</span><span style="color:#24292E;"> (paddle.distributed.get_world_size() </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    paddle.distributed.all_gather(loss_list, losses)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> paddle.distributed.get_rank() </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">        losses </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> paddle.stack(loss_list).sum() </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">len</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">            loss_list)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="多卡训练提示" tabindex="-1"><a class="header-anchor" href="#多卡训练提示" aria-hidden="true">#</a> 多卡训练提示</h3><ul><li>batch size 大小</li></ul><p>由于异步训练与同步训练对梯度的更新方式不同，因此两者训练时候的理论 bathc size 也是不同的，在每张卡获取部分数据-&gt;获取模型参数-&gt;各自前项传播-&gt;各自反向传播计算梯度后：</p><ul><li>异步训练：采取每张卡直接对模型参数进行更新（问：每张卡储存的模型参数一直都是一致的吗？）</li><li>同步训练：采取汇总所有的梯度后，一起同步更新，因此同步训练理论上的 batch size 是更大的。</li></ul><p>异步训练相当于采用了更频繁的梯度更新，以及较小的 batch size，同时训练中的 loss 波动更大。而同步训练更新较慢，同时还会受到 gpu 间通信同步的影响，但该方案比异步训练更流行。</p><p>在 torch 中，DDP 默认采用同步训练。在 paddle 中的 dataparellel 相似，若需要异步训练可以考虑使用 <code>DataParallel</code> 下的 <code>no_sync()</code> 。</p><h2 id="其他参考" tabindex="-1"><a class="header-anchor" href="#其他参考" aria-hidden="true">#</a> 其他参考</h2>`,28),h={href:"https://zhuanlan.zhihu.com/p/74792767",target:"_blank",rel:"noopener noreferrer"},y={href:"https://zhuanlan.zhihu.com/p/56991108",target:"_blank",rel:"noopener noreferrer"},b={href:"https://blog.csdn.net/qq_37668436/article/details/124293378",target:"_blank",rel:"noopener noreferrer"},m={href:"https://support.huaweicloud.com/develop-modelarts/modelarts-distributed-0007.html",target:"_blank",rel:"noopener noreferrer"},_={href:"https://zhuanlan.zhihu.com/p/72939003",target:"_blank",rel:"noopener noreferrer"},v={href:"https://link.zhihu.com/?target=https%3A//github.com/nvidia/apex",target:"_blank",rel:"noopener noreferrer"},g={href:"https://zhuanlan.zhihu.com/p/467103734",target:"_blank",rel:"noopener noreferrer"};function E(f,D){const n=p("ExternalLinkIcon");return t(),r("div",null,[i,s("p",null,[s("a",d,[a("一文搞懂神经网络混合精度训练"),e(n)])]),u,s("p",null,[s("a",h,[a("单机多卡的正确打开方式（三）：PyTorch"),e(n)])]),s("p",null,[s("a",y,[a("tensorflow 分布式训练必备知识"),e(n)])]),s("p",null,[s("a",b,[a("深度学习多机多卡 batchsize 和学习率的关系"),e(n)])]),s("p",null,[s("a",m,[a("单机多卡数据并行-DataParallel(DP)"),e(n)])]),s("p",null,[s("a",_,[a("单机多卡理论"),e(n)])]),s("p",null,[s("a",v,[a("NVIDIA/apex"),e(n)]),a("：封装了 DistributedDataParallel，AllReduce 架构，建议")]),s("p",null,[s("a",g,[a("较详细的 torch DDP 解说"),e(n)])])])}const k=o(c,[["render",E],["__file","笔记parallel_DL.html.vue"]]);export{k as default};
