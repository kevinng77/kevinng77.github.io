const e=JSON.parse('{"key":"v-1a0ad0ea","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0nlu_pretrain.html","title":"NLP 预训练小综述","lang":"zh-CN","frontmatter":{"title":"NLP 预训练小综述","date":"2022-03-22T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP"],"mathjax":true,"toc":true,"comments":"笔记","description":"预训练模型的前世今生 看完了 Pre-Trained Models: Past, Present and Future。对目前主流 NLP 预训练模型、预训练方式做个小结与梳理。 自从 ELMO，GPT，BERT 问世，基于大规模预料的预训练模型便开始流行起来。学者们的注意力渐渐从模型架构转移到了预训练上。预训练+微调的方式也创造了不少下游任务 SOTA。","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0nlu_pretrain.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"NLP 预训练小综述"}],["meta",{"property":"og:description","content":"预训练模型的前世今生 看完了 Pre-Trained Models: Past, Present and Future。对目前主流 NLP 预训练模型、预训练方式做个小结与梳理。 自从 ELMO，GPT，BERT 问世，基于大规模预料的预训练模型便开始流行起来。学者们的注意力渐渐从模型架构转移到了预训练上。预训练+微调的方式也创造了不少下游任务 SOTA。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-02-22T13:54:51.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:published_time","content":"2022-03-22T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-02-22T13:54:51.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"NLP 预训练小综述\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2022-03-22T00:00:00.000Z\\",\\"dateModified\\":\\"2023-02-22T13:54:51.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":3,"title":"深度神经网络","slug":"深度神经网络","link":"#深度神经网络","children":[]},{"level":2,"title":"预训练模型的发展","slug":"预训练模型的发展","link":"#预训练模型的发展","children":[{"level":3,"title":"从 BERT 的 MLM 开始","slug":"从-bert-的-mlm-开始","link":"#从-bert-的-mlm-开始","children":[]},{"level":3,"title":"更好的学习字符知识","slug":"更好的学习字符知识","link":"#更好的学习字符知识","children":[]},{"level":3,"title":"字符之上","slug":"字符之上","link":"#字符之上","children":[]},{"level":3,"title":"预训练模型目前情况","slug":"预训练模型目前情况","link":"#预训练模型目前情况","children":[]},{"level":3,"title":"未来的预训练模型","slug":"未来的预训练模型","link":"#未来的预训练模型","children":[]}]},{"level":2,"title":"预训练模型关键点总结","slug":"预训练模型关键点总结","link":"#预训练模型关键点总结","children":[]},{"level":2,"title":"论文","slug":"论文","link":"#论文","children":[]}],"git":{"createdTime":1676542179000,"updatedTime":1677074091000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":2}]},"readingTime":{"minutes":16.71,"words":5014},"filePathRelative":"posts/notes/articles/笔记nlu_pretrain.md","localizedDate":"2022年3月22日","excerpt":"<h1> 预训练模型的前世今生</h1>\\n<blockquote>\\n<p>看完了 <a href=\\"http://arxiv.org/abs/2106.07139\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Pre-Trained Models: Past, Present and Future</a>。对目前主流 NLP 预训练模型、预训练方式做个小结与梳理。</p>\\n</blockquote>\\n<p>自从 ELMO，GPT，BERT 问世，基于大规模预料的预训练模型便开始流行起来。学者们的注意力渐渐从模型架构转移到了预训练上。预训练+微调的方式也创造了不少下游任务 SOTA。</p>","autoDesc":true}');export{e as data};
