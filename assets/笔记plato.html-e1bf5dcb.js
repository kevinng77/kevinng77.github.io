const e=JSON.parse('{"key":"v-19225dfa","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0plato.html","title":"对话模型 PLATO 系列论文笔记","lang":"zh-CN","frontmatter":{"title":"对话模型 PLATO 系列论文笔记","date":"2022-10-05T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP"],"mathjax":true,"description":"对话模型 PLATO 系列论文笔记 最近又开始着迷对话系统了，于是花时间看了以下几个中文比较有意思的模型。本文对 PLATO，PLATO=2， PLATO-XL，PLATO-KAG 四篇论文进行笔记梳理与总结。 PLATO 论文：PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable 概述 比较早的论文了，论文一上来提出，如果直接使用 Bert 在小规模的对话数据集上微调对话任务，其效果是很差的，基于这些问题，论文给出了几点原因猜测：","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0plato.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"对话模型 PLATO 系列论文笔记"}],["meta",{"property":"og:description","content":"对话模型 PLATO 系列论文笔记 最近又开始着迷对话系统了，于是花时间看了以下几个中文比较有意思的模型。本文对 PLATO，PLATO=2， PLATO-XL，PLATO-KAG 四篇论文进行笔记梳理与总结。 PLATO 论文：PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable 概述 比较早的论文了，论文一上来提出，如果直接使用 Bert 在小规模的对话数据集上微调对话任务，其效果是很差的，基于这些问题，论文给出了几点原因猜测："}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-03-26T07:48:04.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:published_time","content":"2022-10-05T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-03-26T07:48:04.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"对话模型 PLATO 系列论文笔记\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2022-10-05T00:00:00.000Z\\",\\"dateModified\\":\\"2023-03-26T07:48:04.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"PLATO","slug":"plato","link":"#plato","children":[{"level":3,"title":"概述","slug":"概述","link":"#概述","children":[]},{"level":3,"title":"预训练方法","slug":"预训练方法","link":"#预训练方法","children":[]},{"level":3,"title":"推理过程","slug":"推理过程","link":"#推理过程","children":[]},{"level":3,"title":"模型使用","slug":"模型使用","link":"#模型使用","children":[]}]},{"level":2,"title":"PLATO-2","slug":"plato-2","link":"#plato-2","children":[{"level":3,"title":"预训练过程","slug":"预训练过程","link":"#预训练过程","children":[]},{"level":3,"title":"推理","slug":"推理","link":"#推理","children":[]},{"level":3,"title":"任务式对话","slug":"任务式对话","link":"#任务式对话","children":[]},{"level":3,"title":"其他","slug":"其他","link":"#其他","children":[]}]},{"level":2,"title":"PLATO-XL","slug":"plato-xl","link":"#plato-xl","children":[{"level":3,"title":"模型训练","slug":"模型训练","link":"#模型训练","children":[]}]},{"level":2,"title":"PLATO-KAG","slug":"plato-kag","link":"#plato-kag","children":[{"level":3,"title":"模型训练","slug":"模型训练-1","link":"#模型训练-1","children":[]}]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1679816884000,"updatedTime":1679816884000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":9.83,"words":2950},"filePathRelative":"posts/notes/articles/笔记plato.md","localizedDate":"2022年10月5日","excerpt":"<h1> 对话模型 PLATO 系列论文笔记</h1>\\n<p>最近又开始着迷对话系统了，于是花时间看了以下几个中文比较有意思的模型。本文对 PLATO，PLATO=2， PLATO-XL，PLATO-KAG 四篇论文进行笔记梳理与总结。</p>\\n<h2> PLATO</h2>\\n<p>论文：PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable</p>\\n<h3> 概述</h3>\\n<p>比较早的论文了，论文一上来提出，如果直接使用 Bert 在小规模的对话数据集上微调对话任务，其效果是很差的，基于这些问题，论文给出了几点原因猜测：</p>","autoDesc":true}');export{e as data};
