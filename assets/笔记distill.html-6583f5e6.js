const e=JSON.parse('{"key":"v-60c5eb12","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0distill.html","title":"bert 蒸馏小综述","lang":"zh-CN","frontmatter":{"title":"bert 蒸馏小综述","date":"2021-10-09T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","论文笔记"],"mathjax":true,"toc":true,"comments":"笔记","description":"主要参考了 BERT 蒸馏完全指南｜原理/技巧/代码 一文，其中对部分论文的笔记有些抽象，因此遍对提及的蒸馏论文进行了阅读与笔记补充总结。同时总结了 textbrewer 相关使用说明。先来对所有介绍到的模型做个总结： Distilled BiLSTM - 将 BERT LARGE 蒸馏到了 BiLSTM 上，采用 MST 目标函数与数据增强。 BERT-PKD - 不同于前者，BERT-PKD 加入了对中间层 [CLS] 位置上隐状态的拟合。 DistilBERT - 在预训练阶段进行蒸馏，6 层保留了 97% BERT 的表现 TinyBERT - 结合预训练蒸馏与微调蒸馏，提出注意力矩阵蒸馏。速度 x9.4 的 4 层 BERT 达到 96.8%BERT 效果。 MobileBERT - 保持层数，使用 bottlenect 减少维度。速度 x5.5，各种任务上与 BERT 相差约 1%。 MINILM - 提出 Value-Relation Transfer 与助教机制。同样参数数量下，效果由于 TinyBERT，DistillBERT。 TextBrewer - 实用的蒸馏框架。 蒸馏方式对比","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0distill.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"bert 蒸馏小综述"}],["meta",{"property":"og:description","content":"主要参考了 BERT 蒸馏完全指南｜原理/技巧/代码 一文，其中对部分论文的笔记有些抽象，因此遍对提及的蒸馏论文进行了阅读与笔记补充总结。同时总结了 textbrewer 相关使用说明。先来对所有介绍到的模型做个总结： Distilled BiLSTM - 将 BERT LARGE 蒸馏到了 BiLSTM 上，采用 MST 目标函数与数据增强。 BERT-PKD - 不同于前者，BERT-PKD 加入了对中间层 [CLS] 位置上隐状态的拟合。 DistilBERT - 在预训练阶段进行蒸馏，6 层保留了 97% BERT 的表现 TinyBERT - 结合预训练蒸馏与微调蒸馏，提出注意力矩阵蒸馏。速度 x9.4 的 4 层 BERT 达到 96.8%BERT 效果。 MobileBERT - 保持层数，使用 bottlenect 减少维度。速度 x5.5，各种任务上与 BERT 相差约 1%。 MINILM - 提出 Value-Relation Transfer 与助教机制。同样参数数量下，效果由于 TinyBERT，DistillBERT。 TextBrewer - 实用的蒸馏框架。 蒸馏方式对比"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-03-26T07:48:04.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"论文笔记"}],["meta",{"property":"article:published_time","content":"2021-10-09T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-03-26T07:48:04.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"bert 蒸馏小综述\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2021-10-09T00:00:00.000Z\\",\\"dateModified\\":\\"2023-03-26T07:48:04.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"最基础的蒸馏","slug":"最基础的蒸馏","link":"#最基础的蒸馏","children":[]},{"level":2,"title":"bert 蒸馏","slug":"bert-蒸馏","link":"#bert-蒸馏","children":[{"level":3,"title":"Distilled BiLSTM","slug":"distilled-bilstm","link":"#distilled-bilstm","children":[]},{"level":3,"title":"BERT-PKD","slug":"bert-pkd","link":"#bert-pkd","children":[]},{"level":3,"title":"DistilBERT","slug":"distilbert","link":"#distilbert","children":[]},{"level":3,"title":"TinyBERT","slug":"tinybert","link":"#tinybert","children":[]},{"level":3,"title":"MobileBERT","slug":"mobilebert","link":"#mobilebert","children":[]},{"level":3,"title":"MINILM","slug":"minilm","link":"#minilm","children":[]},{"level":3,"title":"TextBrewer","slug":"textbrewer","link":"#textbrewer","children":[]}]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1679816884000,"updatedTime":1679816884000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":16.97,"words":5090},"filePathRelative":"posts/notes/articles/笔记distill.md","localizedDate":"2021年10月9日","excerpt":"<blockquote>\\n<p>主要参考了 <a href=\\"https://mp.weixin.qq.com/s/tKfHq49heakvjM0EVQPgHw\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">BERT 蒸馏完全指南｜原理/技巧/代码</a> 一文，其中对部分论文的笔记有些抽象，因此遍对提及的蒸馏论文进行了阅读与笔记补充总结。同时总结了 textbrewer 相关使用说明。先来对所有介绍到的模型做个总结：</p>\\n<p><a href=\\"#distilled-bilstm\\">Distilled BiLSTM</a> - 将 BERT LARGE 蒸馏到了 BiLSTM 上，采用 MST 目标函数与数据增强。\\n<a href=\\"#BERT-PKD\\">BERT-PKD</a> - 不同于前者，BERT-PKD 加入了对中间层 <code>[CLS]</code>  位置上隐状态的拟合。\\n<a href=\\"#DistilBERT\\">DistilBERT</a> - 在预训练阶段进行蒸馏，6 层保留了 97% BERT 的表现\\n<a href=\\"#TinyBERT\\">TinyBERT</a> - 结合预训练蒸馏与微调蒸馏，提出注意力矩阵蒸馏。速度 x9.4 的 4 层 BERT 达到 96.8%BERT 效果。\\n<a href=\\"#MobileBERT\\">MobileBERT</a> - 保持层数，使用 bottlenect 减少维度。速度 x5.5，各种任务上与 BERT 相差约 1%。\\n<a href=\\"#MINILM\\">MINILM</a> - 提出 Value-Relation Transfer 与助教机制。同样参数数量下，效果由于 TinyBERT，DistillBERT。</p>\\n<p><a href=\\"#TextBrewer\\">TextBrewer</a> - 实用的蒸馏框架。</p>\\n<p><a href=\\"#%E7%94%A8%E4%BB%80%E4%B9%88%E8%92%B8%E9%A6%8F%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%EF%BC%9F\\">蒸馏方式对比</a></p>\\n</blockquote>","autoDesc":true}');export{e as data};
