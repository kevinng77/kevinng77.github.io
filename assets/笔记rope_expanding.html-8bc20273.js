import{_ as l}from"./plugin-vue_export-helper-c27b6911.js";import{r as o,o as i,c as r,a as s,b as a,d as n,f as t}from"./app-337c1ddd.js";const p="/assets/img/rope_expanding/image-20231216141021010.png",m="/assets/img/rope_expanding/image-20231226194402099.png",c="/assets/img/rope_expanding/image-20231226130049273.png",g="/assets/img/rope_expanding/image-20231226134926803.png",d="/assets/img/rope_expanding/image-20231226140020823.png",h="/assets/img/rope_expanding/image-20231225173340833.png",u="/assets/img/rope_expanding/image-20231225173930623.png",f="/assets/img/rope_expanding/image-20231225175229316.png",_="/assets/img/rope_expanding/image-20231225175238791.png",y="/assets/img/rope_expanding/image-20231225185035672.png",b="/assets/img/rope_expanding/image-20231225190028114.png",x="/assets/img/rope_expanding/image-20231226220616504.png",L="/assets/img/rope_expanding/image-20231226220814654.png",E="/assets/img/rope_expanding/image-20231226221034156.png",A="/assets/img/rope_expanding/image-20231226221328972.png",v="/assets/img/rope_expanding/image-20231226221507768.png",k="/assets/img/rope_expanding/image-20231226221807623.png",M={},T=t('<p>大模型上下文在前段时间有点火，TODO 里堆积的论文也越来越多（。</p><p>本文记录了 LLM 在长度外推方面的文章笔记，包括位置编码相关的 <strong>ALiBi</strong> 、 <strong>ROPE</strong> 和 <strong>线性插值（PI）</strong> ， <strong>NTK</strong> ；注意力相关的 <strong>GQA</strong> , <strong>SWA</strong> ， <strong>LM-INFINITE</strong> ， <strong>StreamingLLM</strong> ；以及 meta 的综述 Effective Long-Context Scaling of Foundation Models 记录。</p><h3 id="位置编码" tabindex="-1"><a class="header-anchor" href="#位置编码" aria-hidden="true">#</a> 位置编码</h3>',3),N={href:"https://arxiv.org/pdf/2108.12409.pdf",target:"_blank",rel:"noopener noreferrer"},I=s("p",null,"将原先的 attention score 计算方式改为：",-1),z=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"softmax"),s("mo",null,"⁡"),s("mrow",null,[s("mo",{fence:"true"},"("),s("msub",null,[s("mi",{mathvariant:"bold"},"q"),s("mi",null,"i")]),s("msup",null,[s("mi",{mathvariant:"bold"},"K"),s("mi",{mathvariant:"normal"},"⊤")]),s("mo",null,"+"),s("mi",null,"m"),s("mo",null,"⋅"),s("mo",{stretchy:"false"},"["),s("mo",null,"−"),s("mo",{stretchy:"false"},"("),s("mi",null,"i"),s("mo",null,"−"),s("mn",null,"1"),s("mo",{stretchy:"false"},")"),s("mo",{separator:"true"},","),s("mo",null,"…"),s("mo",{separator:"true"},","),s("mo",null,"−"),s("mn",null,"2"),s("mo",{separator:"true"},","),s("mo",null,"−"),s("mn",null,"1"),s("mo",{separator:"true"},","),s("mn",null,"0"),s("mo",{stretchy:"false"},"]"),s("mo",{fence:"true"},")")])]),s("annotation",{encoding:"application/x-tex"}," \\operatorname{softmax}\\left(\\mathbf{q}_i \\mathbf{K}^{\\top}+m \\cdot[-(i-1), \\ldots,-2,-1,0]\\right) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.2491em","vertical-align":"-0.35em"}}),s("span",{class:"mop"},[s("span",{class:"mord mathrm"},"softmax")]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"minner"},[s("span",{class:"mopen delimcenter",style:{top:"0em"}},[s("span",{class:"delimsizing size1"},"(")]),s("span",{class:"mord"},[s("span",{class:"mord mathbf"},"q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},[s("span",{class:"mord mathbf"},"K"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8991em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"⊤")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord mathnormal"},"m"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"⋅"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord"},"−"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mclose"},")"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"minner"},"…"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},"−"),s("span",{class:"mord"},"2"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},"−"),s("span",{class:"mord"},"1"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},"0"),s("span",{class:"mclose"},"]"),s("span",{class:"mclose delimcenter",style:{top:"0em"}},[s("span",{class:"delimsizing size1"},")")])])])])])])],-1),w=s("p",null,[a("其中 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"m")]),s("annotation",{encoding:"application/x-tex"},"m")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"m")])])]),a(" 为超参数。从下图可以看出，ALiBi 限定了注意力的范围。")],-1),S=s("figure",null,[s("img",{src:p,alt:"image-20231216141021010",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20231216141021010")],-1),C={href:"https://arxiv.org/abs/2104.09864",target:"_blank",rel:"noopener noreferrer"},O=s("p",null,"对比 Transformer 绝对位置编码，ROPE 通过将向量以复数方式推理， 巧妙地实现了以添加绝对位置编码的方式，在 attention 中计算了相对位置信息。关于 ROPE 详细可以参考笔者的另一篇笔记：",-1),G=s("p",null,"https://kevinng77.github.io/posts/notes/articles/%E7%AC%94%E8%AE%B0rope.html",-1),R={href:"https://arxiv.org/pdf/2306.15595.pdf",target:"_blank",rel:"noopener noreferrer"},P=s("p",null,[a("文中提出一种 Linear 的 position Interpolation （线性内插）方案，该方案可以使得原先上下文长度为 2000 左右的 llama 模型，再仅通过 1000 steps 的微调之后，获得 32768 的窗口长度。并且在原先 2000 窗口长度的任务上，不会降低模型回复质量。具体方法是将 ROPE "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"f")]),s("annotation",{encoding:"application/x-tex"},"f")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f")])])]),a(" 替换为：")],-1),D=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"f"),s("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")]),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{separator:"true"},","),s("mi",null,"m"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mi",null,"f"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{separator:"true"},","),s("mfrac",null,[s("mrow",null,[s("mi",null,"m"),s("mi",null,"L")]),s("msup",null,[s("mi",null,"L"),s("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"}," f'(x,m) = f(x, \\frac {mL}{L'}) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0519em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8019em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"′")])])])])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"m"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.0463em","vertical-align":"-0.686em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3603em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"L"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6779em"}},[s("span",{style:{top:"-2.989em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"′")])])])])])])])])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"m"),s("span",{class:"mord mathnormal"},"L")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.686em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mclose"},")")])])])])],-1),F=s("p",null,[a("其中 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"L"),s("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),s("annotation",{encoding:"application/x-tex"},"L'")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7519em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"L"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7519em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"′")])])])])])])])])])])]),a(" 为目标的 context window 大小，比如我们希望模型 context window 大小为 4096，那么 ROPE 的转变如下图：")],-1),H=s("figure",null,[s("img",{src:m,alt:"image-20231226194402099",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20231226194402099")],-1),B=s("p",null,"除了 position embedding 的变换，还需要使用 next token prediction 进行额外的微调来确保模型的效果。",-1),Q={href:"https://github.com/huggingface/text-generation-inference/issues/512",target:"_blank",rel:"noopener noreferrer"},U=s("p",null,"相对于前一篇文中使用的线性插值，Neural Tangent Kerne 通过非线性插值方案，改变 RoPE 的 base，可以取得更好的效果。",-1),K={href:"https://colab.research.google.com/drive/1VI2nhlyKvd5cw4-zHvAIk00cAVj2lCCC#scrollTo=e431d2cd",target:"_blank",rel:"noopener noreferrer"},W=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> transformers</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">old_init </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.</span><span style="color:#005CC5;">__init__</span></span>
<span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">ntk_scaled_init</span><span style="color:#24292E;">(self, dim, max_position_embeddings</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">2048</span><span style="color:#24292E;">, base</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">10000</span><span style="color:#24292E;">, device</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">):</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;">#The method is just these three lines</span></span>
<span class="line"><span style="color:#24292E;">    max_position_embeddings </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">16384</span></span>
<span class="line"><span style="color:#24292E;">    a </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">8</span><span style="color:#24292E;"> </span><span style="color:#6A737D;">#Alpha value</span></span>
<span class="line"><span style="color:#24292E;">    base </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> base </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> a </span><span style="color:#D73A49;">**</span><span style="color:#24292E;"> (dim </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> (dim</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">2</span><span style="color:#24292E;">)) </span><span style="color:#6A737D;">#Base change formula</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    old_init(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">, dim, max_position_embeddings, base, device)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,1),q={href:"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/",target:"_blank",rel:"noopener noreferrer"},V=s("h3",{id:"transformer-注意力的优化",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#transformer-注意力的优化","aria-hidden":"true"},"#"),a(" Transformer 注意力的优化")],-1),j={href:"https://arxiv.org/abs/2305.13245",target:"_blank",rel:"noopener noreferrer"},X=s("figure",null,[s("img",{src:c,alt:"image-20231226130049273",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20231226130049273")],-1),Y={href:"https://arxiv.org/pdf/1911.02150.pdf",target:"_blank",rel:"noopener noreferrer"},Z=s("p",null,"以下为 GQA 文中的实验结果，值得注意的是论文中使用原 MHA checkpoint 转换为 GQA 权重后，还进行了额外的预训练：",-1),J=s("figure",null,[s("img",{src:g,alt:"image-20231226134926803",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20231226134926803")],-1),$=s("p",null,"此外 Mistral，Llama2 的部分模型使用 GQA 时，采用的 kv head 数量似乎都是 8。",-1),ss={href:"https://zhuanlan.zhihu.com/p/647130255",target:"_blank",rel:"noopener noreferrer"},as=s("p",null,[s("strong",null,"Sliding Window Attention")],-1),es=s("p",null,"以 Mistral 的 SWA 为例，Mistral 采用的 window size 为 4096，而后一共有 32 层 layer，那么采用 SWA 之后，理论上在进行 attention 的时候，可以收集到约 131K tokens 的信息。",-1),ns=s("figure",null,[s("img",{src:d,alt:"image-20231226140020823",height:"400",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20231226140020823")],-1),ts=s("p",null,[a("由于代用了固定的 attention 窗口大小，因此我们只需要一个大小为 "),s("code",null,"W=window size"),a(" 的 cache ，在计算第 i 个 token 的 cache 的时候，只需要覆盖 cache 中 "),s("code",null,"i mod M"),a(" 位置上的 hidden state 即可。")],-1),ls={href:"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2308.16137",target:"_blank",rel:"noopener noreferrer"},os=t('<p>文中讨论了导致长度外推失败的几个因素：</p><p><strong>UNSEEN DISTANCES</strong></p><p>直观上来说推理时候的长度，远超出了训练集的长度，那么采样的效果一定不好。文中也证明了，随着长度的增加，如果我们要区分出新的 <strong>距离因素</strong> ，那么，注意力的 logit 就必须增长到无限大。</p><p><strong>UNSEEN NUMBER OF TOKENS</strong></p><p>即便我们不考虑距离因素，那么当推理长度增加时，attention score 的分布会趋于平坦，导致信息的丢失。论文也通过 Attention Entropy 会趋于无限大进行了观点验证。</p><p><strong>IMPLICITLY-ENCODED ABSOLUTE POSITION</strong></p><p>文中发现：即使在网络中没有明确编码绝对位置信息，注意力机制仍然能够隐式地编码它。</p><p>论文对 LLaMa 第一层的 hidden state 进行 PCA 后发现：在图 c 中，头部的 token 为蓝色点，尾部的 token 为红色点。尾部的 token 在空间分布上，已经逐渐地失去了区分性。</p><figure><img src="'+h+'" alt="image-20231225173340833" tabindex="0" loading="lazy"><figcaption>image-20231225173340833</figcaption></figure><p><strong>LM-INFINITE 方法</strong></p><figure><img src="'+u+'" alt="image-20231225173930623" tabindex="0" loading="lazy"><figcaption>image-20231225173930623</figcaption></figure><p>因此，LM-INFINITE 尝试了移动采样。对于注意力部分的计算，如果整个序列长度为 <code>i</code> 的话，那么我们只针对序列开始的 <code>n_global</code> 个 token 以及序列末尾的 <code>n_local</code> 个 token 进行注意力计算（参考上图 b 中的 <code>start tokens</code> 和 <code>rear tokens</code> 部分）。当 <code>n_global=0</code> 时，采样效果是非常差的。</p><p>文中对采用了相对位置编码的主流模型（LlaMa, MPT, GPT），在 ArXiv 和 OpenWebText2 上计算了测试。</p><p>Perplexity 指标：</p><figure><img src="'+f+'" alt="image-20231225175229316" height="400" tabindex="0" loading="lazy"><figcaption>image-20231225175229316</figcaption></figure><p>和 BLEU, ROUGE 指标：</p><figure><img src="'+_+'" alt="image-20231225175238791" height="400" tabindex="0" loading="lazy"><figcaption>image-20231225175238791</figcaption></figure>',17),is={href:"https://arxiv.org/pdf/2309.17453.pdf",target:"_blank",rel:"noopener noreferrer"},rs=t('<p>StreamingLLM 的想法与 LM-infinite 相似，在计算 attention 时候，只计算头部和尾部的 token，省略掉重点部分的 attention。</p><figure><img src="'+y+'" alt="image-20231225185035672" height="300" tabindex="0" loading="lazy"><figcaption>image-20231225185035672</figcaption></figure><p>StreamingLLM 试验后提出，保留开头的 4 个 token 是个不错的选择。</p><p>在设置 <code>position_id</code> 时，输入的是连续不间断的 ID，如上图中，生成第 9 个 token 时，使用的 position id 为 <code>[0,1,2,3,4,5,6,7]</code></p><p>对于为什么开头的 tokens 不能从 KV 中去除，文中以下实验：</p><figure><img src="'+b+'" alt="img" height="450" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>结果发现开头用 4 个 <code>\\n</code> 替换原始文本后，PPL 增长了一点而已。</p><p>StreamLLM 官方仓库中特别指明：该方法并不会扩展上下文长度：</p><blockquote><p><strong>Can I input an extensive text, like a book, into StreamingLLM for summarization?</strong></p><p>While you can input a lengthy text, the model will only recognize the latest tokens. Thus, if a book is an input, StreamingLLM might only summarize the concluding paragraphs, which might not be very insightful. As emphasized earlier, we neither expand the LLMs&#39; context window nor enhance their long-term memory. StreamingLLM&#39;s strength lies in generating fluent text from recent tokens without needing a cache refresh.</p></blockquote>',9),ps={href:"https://arxiv.org/abs/2309.16039",target:"_blank",rel:"noopener noreferrer"},ms=t('<p>Meta 提出的一篇对于扩展模型上下文能力的详细分析，主要讨论了长上下文模型的连续预训练方法、位置编码的设计、数据集的长度分布以及训练方案（Training Curriculum）对最终性能的影响。</p><p>方案：</p><p>首先进行额外的预训练，预训练过程中需要：</p><ul><li>调整 position encoding 策略：meta 在实验中对 base 进行了调整，从 10000 改成了 500000。文中实验表示 RoPE ABF（adjusted base frequency，即仅对 base 进行修改）效果会好与下图中展示的其他方案。</li></ul><figure><img src="'+x+'" alt="image-20231226220616504" height="400" tabindex="0" loading="lazy"><figcaption>image-20231226220616504</figcaption></figure><ul><li>更换数据分布：Meta 表示对于长文本的预训练，数据的质量十分关键。并且一味地增加长文本的比重并不能持续地提高训练效果。</li></ul><figure><img src="'+L+'" alt="image-20231226220814654 " tabindex="0" loading="lazy"><figcaption>image-20231226220814654 </figcaption></figure><p>在下面地结果中展示了：即使我们在有限长度地文本下继续预训练，那么我们也能提高不少地模型效果。</p><figure><img src="'+E+'" alt="image-20231226221034156" tabindex="0" loading="lazy"><figcaption>image-20231226221034156</figcaption></figure><ul><li>在优化过程中，采用了 FlashAttention，同时保持了与预训练时同样的 token per batch。对于 7/13B 模型采用 2e-5 学习率+ 2000 warm-up steps 的 cosine learning rate schedule.</li></ul><p>而后进行指令微调：</p><p>Meta 表示，指令微调环节主要关注点在于 QA 数据集的构造。文中先从预训练语料（似乎是 LLAMA 2 CHAT 用到的 RLHF 语料）中选出长文档，而后让 LLAMA 2 CHAT 生成一些 QA 训练对。同时用 LLAMA 2 CHAT 进行答案验证。</p><p>对于生成的训练数据，如果过短的话，会被 padding 到 16384 token。</p><figure><img src="'+A+'" alt="image-20231226221328972" tabindex="0" loading="lazy"><figcaption>image-20231226221328972</figcaption></figure><p><strong>训练方案：</strong></p><p>文中尝试了先从 4096 长度开始训练，而后换到 32768 训练。看文中的实验，似乎感觉从 20%-40% 的部分就换到 32k 训练效果会好一点。</p><figure><img src="'+v+'" alt="image-20231226221507768" tabindex="0" loading="lazy"><figcaption>image-20231226221507768</figcaption></figure><p>下表中表示，每当长度从 4K 切换到 32k，模型地 loss 都会有进一步地减少。</p><figure><img src="'+k+'" alt="image-20231226221807623" height="400" tabindex="0" loading="lazy"><figcaption>image-20231226221807623</figcaption></figure><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考" aria-hidden="true">#</a> 参考</h2>',20),cs={href:"https://zhuanlan.zhihu.com/p/619703849",target:"_blank",rel:"noopener noreferrer"},gs={href:"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2308.16137",target:"_blank",rel:"noopener noreferrer"},ds={href:"https://arxiv.org/pdf/2309.17453.pdf",target:"_blank",rel:"noopener noreferrer"},hs={href:"https://kexue.fm/archives/9431",target:"_blank",rel:"noopener noreferrer"},us={href:"https://kexue.fm/archives/9444",target:"_blank",rel:"noopener noreferrer"},fs={href:"https://arxiv.org/pdf/2108.12409.pdf",target:"_blank",rel:"noopener noreferrer"},_s={href:"https://arxiv.org/pdf/2212.10554.pdf",target:"_blank",rel:"noopener noreferrer"},ys={href:"https://zhuanlan.zhihu.com/p/647130255",target:"_blank",rel:"noopener noreferrer"},bs={href:"https://yaofu.notion.site/Understanding-data-influence-on-context-scaling-a-close-look-at-baseline-solution-eb17eab795dd4132b1a1ffe73f5e850a",target:"_blank",rel:"noopener noreferrer"},xs={href:"https://arxiv.org/abs/2309.16039",target:"_blank",rel:"noopener noreferrer"},Ls={href:"https://zhuanlan.zhihu.com/p/660073229",target:"_blank",rel:"noopener noreferrer"},Es={href:"https://arxiv.org/pdf/2306.15595.pdf",target:"_blank",rel:"noopener noreferrer"},As={href:"https://zhuanlan.zhihu.com/p/664212322",target:"_blank",rel:"noopener noreferrer"},vs={href:"https://yaofu.notion.site/Understanding-data-influence-on-context-scaling-a-close-look-at-baseline-solution-eb17eab795dd4132b1a1ffe73f5e850a",target:"_blank",rel:"noopener noreferrer"};function ks(Ms,Ts){const e=o("ExternalLinkIcon");return i(),r("div",null,[T,s("p",null,[s("a",N,[a("ALIBI: TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION"),n(e)])]),I,z,w,S,s("p",null,[s("a",C,[a("ROPE: Rotary Position Embedding"),n(e)])]),O,G,s("p",null,[s("a",R,[a("Extending Context Window of Large Language Models via Positional Interpolation"),n(e)])]),P,D,F,H,B,s("p",null,[s("a",Q,[a("NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"),n(e)])]),U,s("p",null,[a("提出 NTK 的网友也提供了对应的"),s("a",K,[a("代码"),n(e)]),a("，核心部分即更改 RoPE 的 base：")]),W,s("p",null,[a("该方案也在后续逐渐在 LLM 上被采用，如 "),s("a",q,[a("Code Llama: Open Foundation Models for Code"),n(e)]),a(" 。")]),V,s("p",null,[s("a",j,[a("GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"),n(e)])]),X,s("p",null,[a("grouped-query attention 指出，"),s("a",Y,[a("Multi-Query Attention"),n(e)]),a(" 提高了推理速度的同时，却可能极大地降低回复质量。因此根据上图，GQA 在推理速度和质量之间作了权衡。")]),Z,J,$,s("blockquote",null,[s("p",null,[s("a",ss,[a("为什么现在大家都在用 MQA 和 GQA？"),n(e)]),a(" 文中提到 MQA 和 GQA 能获得巨大加速的一个点在于：GPU 内存强的限制。由于 MQA 和 GQA 都降低了内存中数据的读取量，减少了计算单元的等待时间，因此推理速度的提高比想象中的要快更多。")])]),as,es,ns,ts,s("p",null,[s("a",ls,[a("LM-INFINITE: SIMPLE ON-THE-FLY LENGTH GENERALIZATION FOR LARGE LANGUAGE MODELS"),n(e)])]),os,s("p",null,[s("a",is,[a("StreamingLLM: EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS"),n(e)])]),rs,s("p",null,[s("a",ps,[a("LlaMA2 Long: Effective Long-Context Scaling of Foundation Models"),n(e)])]),ms,s("p",null,[s("a",cs,[a("Perpetual Sampling with LLaMA-30B"),n(e)])]),s("p",null,[s("a",gs,[a("LM-INFINITE: SIMPLE ON-THE-FLY LENGTH GENERALIZATION FOR LARGE LANGUAGE MODELS"),n(e)])]),s("p",null,[s("a",ds,[a("StreamingLLM: EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS"),n(e)])]),s("p",null,[s("a",hs,[a("Transformer 升级之路：7、长度外推性与局部注意力"),n(e)])]),s("p",null,[s("a",us,[a("Transformer 升级之路：8、长度外推性与位置鲁棒性"),n(e)])]),s("p",null,[s("a",fs,[a("TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION"),n(e)])]),s("p",null,[s("a",_s,[a("A Length-Extrapolatable Transformer"),n(e)])]),s("p",null,[s("a",ys,[a("为什么现在大家都在用 MQA 和 GQA？"),n(e)])]),s("p",null,[s("a",bs,[a("Understanding data influence on context scaling: a close look at baseline solution"),n(e)])]),s("p",null,[s("a",xs,[a("Effective Long-Context Scaling of Foundation Models"),n(e)])]),s("p",null,[s("a",Ls,[a("RoPE 外推的缩放法则 —— 尝试外推 RoPE 至 1M 上下文"),n(e)])]),s("p",null,[s("a",Es,[a("Extending Context Window of Large Language Models via Positional Interpolation"),n(e)])]),s("p",null,[s("a",As,[a("LLM 时代探秘 100K 上下文背后的密码"),n(e)])]),s("p",null,[s("a",vs,[a("Understanding data influence on context scaling: a close look at baseline solution"),n(e)])])])}const zs=l(M,[["render",ks],["__file","笔记rope_expanding.html.vue"]]);export{zs as default};
