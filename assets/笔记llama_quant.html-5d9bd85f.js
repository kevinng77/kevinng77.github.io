const t=JSON.parse('{"key":"v-0af42f41","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0llama_quant.html","title":"LLaMa 量化部署方案测试","lang":"zh-CN","frontmatter":{"title":"LLaMa 量化部署方案测试","date":"2023-07-05T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC"],"description":"本文导论部署 LLaMa 系列模型常用的几种方案，并作速度测试。包括 Huggingface 自带的 LLM.int8()，AutoGPTQ, GPTQ-for-LLaMa, exllama。 总结来看，对 7B 级别的 LLaMa 系列模型，经过 GPTQ 量化后，在 4090 上可以达到 140+ tokens/s 的推理速度。在 3070 上可以达到 40 tokens/s 的推理速度。 LM.int8() 来自论文：LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0llama_quant.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"LLaMa 量化部署方案测试"}],["meta",{"property":"og:description","content":"本文导论部署 LLaMa 系列模型常用的几种方案，并作速度测试。包括 Huggingface 自带的 LLM.int8()，AutoGPTQ, GPTQ-for-LLaMa, exllama。 总结来看，对 7B 级别的 LLaMa 系列模型，经过 GPTQ 量化后，在 4090 上可以达到 140+ tokens/s 的推理速度。在 3070 上可以达到 40 tokens/s 的推理速度。 LM.int8() 来自论文：LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-07-27T15:28:00.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:published_time","content":"2023-07-05T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-07-27T15:28:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLaMa 量化部署方案测试\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-07-05T00:00:00.000Z\\",\\"dateModified\\":\\"2023-07-27T15:28:00.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"LM.int8()","slug":"lm-int8","link":"#lm-int8","children":[]},{"level":2,"title":"GPTQ","slug":"gptq","link":"#gptq","children":[{"level":3,"title":"GPTQ-for-LLaMa","slug":"gptq-for-llama","link":"#gptq-for-llama","children":[]},{"level":3,"title":"AutoGPTQ","slug":"autogptq","link":"#autogptq","children":[]},{"level":3,"title":"exllama","slug":"exllama","link":"#exllama","children":[]},{"level":3,"title":"gptq","slug":"gptq-1","link":"#gptq-1","children":[]}]},{"level":2,"title":"GGML","slug":"ggml","link":"#ggml","children":[]},{"level":2,"title":"推理部署","slug":"推理部署","link":"#推理部署","children":[{"level":3,"title":"一些备注","slug":"一些备注","link":"#一些备注","children":[]}]}],"git":{"createdTime":1690471680000,"updatedTime":1690471680000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":9.39,"words":2818},"filePathRelative":"posts/notes/articles/笔记llama_quant.md","localizedDate":"2023年7月5日","excerpt":"<p>本文导论部署 LLaMa 系列模型常用的几种方案，并作速度测试。包括 Huggingface 自带的 LLM.int8()，AutoGPTQ, GPTQ-for-LLaMa, exllama。</p>\\n<p>总结来看，对 7B 级别的 LLaMa 系列模型，经过 GPTQ 量化后，在 4090 上可以达到 140+ tokens/s 的推理速度。在 3070 上可以达到 40 tokens/s 的推理速度。</p>\\n<h2> <strong>LM.int8()</strong></h2>\\n<p>来自论文：<a href=\\"https://arxiv.org/pdf/2208.07339.pdf\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a></p>","autoDesc":true}');export{t as data};
