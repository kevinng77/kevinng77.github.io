const e=JSON.parse('{"key":"v-49ba5d2d","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0rlhf_2.html","title":"RLHF|DPO, GRPO","lang":"zh-CN","frontmatter":{"title":"RLHF|DPO, GRPO","date":"2024-07-03T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC","LLM","Agent"],"description":"本文梳理了 DPO，GRPO 的主要特点、亮点以及相关资源链接。 DPO 论文：Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023 先来回顾以下 PPO，采用 PPO 的 RLHF 会经过 reward model tuning 和 Reinforcement Learning 2 个步骤：","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0rlhf_2.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"RLHF|DPO, GRPO"}],["meta",{"property":"og:description","content":"本文梳理了 DPO，GRPO 的主要特点、亮点以及相关资源链接。 DPO 论文：Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023 先来回顾以下 PPO，采用 PPO 的 RLHF 会经过 reward model tuning 和 Reinforcement Learning 2 个步骤："}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-01-29T12:24:45.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:tag","content":"Agent"}],["meta",{"property":"article:published_time","content":"2024-07-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-01-29T12:24:45.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"RLHF|DPO, GRPO\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-07-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-01-29T12:24:45.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"DPO","slug":"dpo","link":"#dpo","children":[]},{"level":2,"title":"GRPO","slug":"grpo","link":"#grpo","children":[]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1738153485000,"updatedTime":1738153485000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":3.83,"words":1148},"filePathRelative":"posts/notes/articles/笔记rlhf_2.md","localizedDate":"2024年7月3日","excerpt":"<p>本文梳理了 DPO，GRPO 的主要特点、亮点以及相关资源链接。</p>\\n<h2> DPO</h2>\\n<p>论文：<a href=\\"https://arxiv.org/abs/2305.18290\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023</a></p>\\n<p>先来回顾以下 PPO，采用 PPO 的 RLHF 会经过 reward model tuning 和 Reinforcement Learning 2 个步骤：</p>","autoDesc":true}');export{e as data};
