import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as t,o as p,c as i,a as s,b as n,d as a,f as l}from"./app-1dfbb1d3.js";const r={},c=s("p",null,"相对于 DDIM, DDPM 以及 SDE，High-Resolution Image Synthesis with Latent Diffusion Models 一文重点在于 latent Space 和 Conditioning Cross Attention，而非 diffusion pipeline 流程。",-1),d={href:"https://zhuanlan.zhihu.com/p/659212489/huggingface/diffusers",target:"_blank",rel:"noopener noreferrer"},u=s("p",null,"系列笔记",-1),h={href:"https://zhuanlan.zhihu.com/p/650495280",target:"_blank",rel:"noopener noreferrer"},y={href:"https://zhuanlan.zhihu.com/p/650614674",target:"_blank",rel:"noopener noreferrer"},m={href:"https://zhuanlan.zhihu.com/p/655679978",target:"_blank",rel:"noopener noreferrer"},_=l(`<h2 id="latent-diffusion-model" tabindex="-1"><a class="header-anchor" href="#latent-diffusion-model" aria-hidden="true">#</a> Latent Diffusion Model</h2><p>论文：High-Resolution Image Synthesis with Latent Diffusion Models</p><figure><img src="https://pic3.zhimg.com/80/v2-da826549375793c4f8472a54a14c1616_1440w.webp" alt="LDM 架构图" tabindex="0" loading="lazy"><figcaption>LDM 架构图</figcaption></figure><h3 id="ldm-主要思想" tabindex="-1"><a class="header-anchor" href="#ldm-主要思想" aria-hidden="true">#</a> LDM 主要思想</h3><p>扩散模型（DMs）直接在像素领域工作，优化和推断都很费时。为了在有限的计算资源上训练它们，LDM 先使用一个预训练好的 AutoEncoder，将图片像素转换到了维度较小的 latent space 上，而后再进行传统的扩散模型推理与优化。这种训练方式使得 LDM 在算力和性能之间得到了平衡。</p><p>此外，通过引入交叉注意力，使得 DMs 能够在条件生成上有不错的效果，包括如文字生成图片，inpainting 等。</p><h3 id="ldm-使用示例" tabindex="-1"><a class="header-anchor" href="#ldm-使用示例" aria-hidden="true">#</a> <strong>LDM 使用示例</strong></h3><p>huggingface Diffusers 将各种 Diffusion Model Pipeline 都包装好了，使用 Diffusion model 就和使用 Transformers 一样地方便：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> diffusers </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> DiffusionPipeline</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># load model and scheduler</span></span>
<span class="line"><span style="color:#24292E;">ldm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> DiffusionPipeline.from_pretrained(</span><span style="color:#032F62;">&quot;CompVis/ldm-text2im-large-256&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;"> </span><span style="color:#6A737D;"># run pipeline in inference (sample random noise and denoise)</span></span>
<span class="line"><span style="color:#24292E;">prompt </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;A painting of a squirrel eating a burger&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">images </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> ldm([prompt], </span><span style="color:#E36209;">num_inference_steps</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">50</span><span style="color:#24292E;">, </span><span style="color:#E36209;">eta</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0.3</span><span style="color:#24292E;">, </span><span style="color:#E36209;">guidance_scale</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">6</span><span style="color:#24292E;">).images</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># save images</span></span>
<span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> idx, image </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">enumerate</span><span style="color:#24292E;">(images):</span></span>
<span class="line"><span style="color:#24292E;">    image.save(</span><span style="color:#D73A49;">f</span><span style="color:#032F62;">&quot;squirrel-</span><span style="color:#005CC5;">{</span><span style="color:#24292E;">idx</span><span style="color:#005CC5;">}</span><span style="color:#032F62;">.png&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="ldm-pipeline" tabindex="-1"><a class="header-anchor" href="#ldm-pipeline" aria-hidden="true">#</a> LDM Pipeline</h3><p>LDM 的 pipeline 可以简化表示为：<code>Pipeline(prompt, num_inference_steps, latents)</code>。我们暂时考虑没有 negative prompt 和 初始 latent 的输入，那么整个采样过程大致可以表示为：</p><ol><li>首先采用了 BERT 架构模型对 prompt 进行处理，生成 <code>text_hidden_state</code>；同时生成随机噪声 <code>latents</code>。</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">text_hidden_state </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> LDMBERT(prompt) </span><span style="color:#6A737D;"># shape=[bs, len_seq, d_model] = [1, 77, 1280] </span></span>
<span class="line"><span style="color:#24292E;">latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> randn_tensor(latents_shape) </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,13),f=s("code",null,'"CompVis/ldm-text2im-large-256"',-1),g=s("code",null,"LDMBert",-1),b={href:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",target:"_blank",rel:"noopener noreferrer"},v=s("code",null,"LDMBert",-1),E=s("code",null,"[batch_size, 77, 1280]",-1),D=l(`<ol start="2"><li>之后进行传统的扩散模型 backward process：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> t </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.progress_bar(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.timesteps):</span></span>
<span class="line"><span style="color:#24292E;">    noise_pred </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.unet(latents_input, t, </span><span style="color:#E36209;">encoder_hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">context).sample</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># compute the previous noisy sample x_t -&gt; x_t-1</span></span>
<span class="line"><span style="color:#24292E;">    latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.step(noise_pred, t, latents, </span><span style="color:#D73A49;">**</span><span style="color:#24292E;">extra_kwargs).prev_sample</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中 UNET 为 <code>UNet2DConditionModel</code>，与传统 Unet 不同在于其应用了 Cross Attention 对文字以及图片信息进行综合处理，下文会对改模块做梳理。scheduler 可以选 DDIM 或者其他算法。</p><ol start="3"><li>最后对 latent hidden state 进行 decode，生成图片：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.vqvae.config.scaling_factor </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> latents</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.vqvae.decode(latents).sample</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="ldm-中的-unet" tabindex="-1"><a class="header-anchor" href="#ldm-中的-unet" aria-hidden="true">#</a> LDM 中的 UNET</h4><p>backward process 中的 <code>self.unet(...)</code>，即 <code>UNET2DCondition(sample, timestep, encoder_hidden_state)</code> 前向推导可以看成五部分，（以下以 <code>CompVis/ldm-text2im-large-256</code> 为例介绍）：</p><ul><li><strong>准备 time steps</strong> ：Timesteps 编码信息是 diffusion 中 predict noise residual 模型的标配：</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 经过两次映射得到 timesteps 对应的 embedding</span></span>
<span class="line"><span style="color:#24292E;">t_emb </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.time_proj(timesteps)</span></span>
<span class="line"><span style="color:#24292E;">emb </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.time_embedding(t_emb, timestep_cond)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>pre-process：</strong> LDM 只用了一个 2D 卷积对输入的 hidden state 进行处理</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Conv2d(</span></span>
<span class="line"><span style="color:#24292E;">            in_channels, block_out_channels[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">], </span><span style="color:#E36209;">kernel_size</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">conv_in_kernel, </span><span style="color:#E36209;">padding</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">conv_in_padding</span></span>
<span class="line"><span style="color:#24292E;">        )(sample)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>down sampling</strong> ：down sampling 包括了 3 个 <code>CrossAttnDownBlock2D</code>, 和 1 个 <code>DownBlock2D</code>。</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># down sampling 大致前向推导</span></span>
<span class="line"><span style="color:#24292E;">down_block_res_samples </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> (sample,)</span></span>
<span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> downsample_block </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.down_blocks:</span></span>
<span class="line"><span style="color:#24292E;">    sample, res_samples </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> downsample_block(</span><span style="color:#E36209;">hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">sample, </span><span style="color:#E36209;">temb</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">emb, </span><span style="color:#E36209;">scale</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">lora_scale)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 用于 UNET 的残差链接</span></span>
<span class="line"><span style="color:#24292E;">    down_block_res_samples </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> res_samples</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中每个 <code>CrossAttnDownBlock2D</code> 大概前向过程为：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># CrossAttnDownBlock2D</span></span>
<span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, hidden_states, temb, encoder_hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">	output_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> ()</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> resnet, attn </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">zip</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.resnets, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.attentions):</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> resnet(hidden_states, temb)</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> attn(</span></span>
<span class="line"><span style="color:#24292E;">            hidden_states,</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#E36209;">encoder_hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">encoder_hidden_states,</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#E36209;">cross_attention_kwargs</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">cross_attention_kwargs,</span></span>
<span class="line"><span style="color:#24292E;">        ).sample</span></span>
<span class="line"><span style="color:#24292E;">        output_states </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> (hidden_states,)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># downsampler = Conv2D </span></span>
<span class="line"><span style="color:#24292E;">    hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> downsampler(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">    output_states </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> (hidden_states,)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> hidden_states, output_states</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在 <code>CompVis/ldm-text2im-large-256</code> 中，每个 <code>CrossAttnDownBlock2D</code> 包含了 2 个 <code>attn</code>（<code>Transformer2DModel</code>）以及 2 个 <code>resnet</code> （<code>ResnetBlock2D</code>）。</p>`,16),C=s("code",null,"Transformer2DModel",-1),A={href:"https://github.com/huggingface/diffusers/blob/16b9a57d29b6dbce4f97dbf439af1663d2c54588/src/diffusers/models/transformer_2d.py#L44C6-L44C6",target:"_blank",rel:"noopener noreferrer"},k=s("code",null,"(batch_size, channel, width, height)",-1),x=s("code",null,"(batch_size, num_image_vectors)",-1),w=s("code",null,"(batch_size, len_seq, hidden_size)",-1),L=s("code",null,"hidden_states",-1),M=s("code",null,"hidden_states",-1),B=s("code",null,"encoder_hidden_states",-1),S=l(`<ul><li><strong>mid processing:</strong></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MidBlock2DCrossAttn()(sample, </span></span>
<span class="line"><span style="color:#24292E;">                              emb,</span></span>
<span class="line"><span style="color:#24292E;">                           encoder_hidden_states)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2),T=s("code",null,"CompVis/ldm-text2im-large-256",-1),z={href:"https://github.com/huggingface/diffusers/blob/16b9a57d29b6dbce4f97dbf439af1663d2c54588/src/diffusers/models/unet_2d_blocks.py#L572",target:"_blank",rel:"noopener noreferrer"},q=s("code",null,"MidBlock2DCrossAttn",-1),I=s("code",null,"Transformer2DModel",-1),N=s("code",null,"resnet",-1),U=s("code",null,"ResnetBlock2D",-1),R=l(`<ul><li><strong>upsampling</strong> ：upsampling 采用的模块 UpBlocks 包括了 <code> (&quot;UpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;)</code>，各个模块的架构与 down sampling 中的模块相似。</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># upsample_block</span></span>
<span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> i, upsample_block </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">enumerate</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.up_blocks):</span></span>
<span class="line"><span style="color:#24292E;">    sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> upsample_block(</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">sample,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">temb</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">emb,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">res_hidden_states_tuple</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">res_samples,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">upsample_size</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">upsample_size,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">scale</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">lora_scale,</span></span>
<span class="line"><span style="color:#24292E;">                )</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>post-process</strong></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># GroupNorm</span></span>
<span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.conv_norm_out(sample)</span></span>
<span class="line"><span style="color:#6A737D;"># Silu</span></span>
<span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.conv_act(sample)</span></span>
<span class="line"><span style="color:#6A737D;"># Conv2d(320, 4, kernel=(3,3), s=(1,1), padding=(1,1))</span></span>
<span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.conv_out(sample)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>总结起来，down sampling，midprocess，upsampling 三个步骤中都涉及到了 <code>Transformer2DModel</code> ，实现多模态的信息交互。</p><h3 id="ldm-super-resolution-pipeline" tabindex="-1"><a class="header-anchor" href="#ldm-super-resolution-pipeline" aria-hidden="true">#</a> LDM Super Resolution Pipeline</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">low_res_img </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> Image.open(BytesIO(response.content)).convert(</span><span style="color:#032F62;">&quot;RGB&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">low_res_img </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> low_res_img.resize((</span><span style="color:#005CC5;">128</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">128</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E36209;">upscaled_image</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> pipeline(low_res_img, </span><span style="color:#E36209;">num_inference_steps</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">100</span><span style="color:#24292E;">, </span><span style="color:#E36209;">eta</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">).images[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">upscaled_image.save(</span><span style="color:#032F62;">&quot;ldm_generated_image.png&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>大致前项推导流程可以概括为：</p><ol><li>根据 输入图片大小，生成对应的 latent 噪音以及 time step embedding：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> randn_tensor(latents_shape, </span><span style="color:#E36209;">generator</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">generator, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.device, </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">latents_dtype)  </span><span style="color:#6A737D;"># shape 与输入图片相同</span></span>
<span class="line"><span style="color:#24292E;">latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> latents </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.init_noise_sigma</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>将 latent 与原始图片拼接，然后进行 diffusion 反向推导：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> t </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.progress_bar(timesteps_tensor):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># concat latents and low resolution image in the channel dimension.</span></span>
<span class="line"><span style="color:#24292E;">    latents_input </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.cat([latents, image], </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">    latents_input </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.scale_model_input(latents_input, t)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># predict the noise residual</span></span>
<span class="line"><span style="color:#24292E;">    noise_pred </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.unet(latents_input, t).sample</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># compute the previous noisy sample x_t -&gt; x_t-1</span></span>
<span class="line"><span style="color:#24292E;">    latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.step(noise_pred, t, latents, </span><span style="color:#D73A49;">**</span><span style="color:#24292E;">extra_kwargs).prev_sample</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="3"><li>使用 vqvae 对 latent 进行解码，得到最终图片</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># decode the image latents with the VQVAE</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.vqvae.decode(latents).sample</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.clamp(image, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1.0</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1.0</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> image </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0.5</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> image.cpu().permute(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">).numpy()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="stable-diffusion" tabindex="-1"><a class="header-anchor" href="#stable-diffusion" aria-hidden="true">#</a> Stable diffusion</h2><h3 id="sd-v1-架构" tabindex="-1"><a class="header-anchor" href="#sd-v1-架构" aria-hidden="true">#</a> SD v1 架构</h3>`,16),F={href:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",target:"_blank",rel:"noopener noreferrer"},P=s("code",null,"stable-diffusion-v1-5",-1),V=s("ol",null,[s("li",null,[s("strong",null,"Text Encoder")])],-1),O=s("code",null,"CLIPTextModel",-1),j={href:"https://arxiv.org/pdf/2103.00020.pdf",target:"_blank",rel:"noopener noreferrer"},G=s("code",null,"[batch_size, 77, 768]",-1),H=l(`<ol start="2"><li><strong>Diffusion 反向采样过程</strong></li></ol><p>SD v1.5 采样过程与 LDM 相似，其中的 latents 大小为 <code>[bs, 4, 64, 64]</code>。对于 txt2img，latents 通过随机生成，对于 img2img，latents 通过 VAE 模型进行 encode。</p><p>Unet 配置与 LDM 相似：</p><ul><li><p>down sampling 采用 3 个 <code>CrossAttnDownBlock2D</code>, 和 1 个 <code>DownBlock2D</code>。</p></li><li><p>mid block 采用 1 个 <code>MidBlock2DCrossAttn</code>。hidden size = 1280</p></li><li><p>Up sampling 采用 1 个 <code>UpBlock2D</code> + 3 个 <code>CrossAttnUpBlock2D</code></p></li></ul><p>每个 CrossAttn 的 transformer 中， text embedding 大小为 768，但 Transformer 模块的 <code>hidden size</code> 随着 Unet 深入而增加。如 down sampling 采用的维度为 320, 640, 1280, 1280。那么 3 个 Transformer 模块中的 hidden size 就分别是 320, 640, 1280。</p><p>以 down sampling 为例，在进行 cross attention 时候，图像的 hidden state （latent）大小分别被映射到了 <code>[4096, 320]</code>，<code>[2014, 640]</code>，<code>[256, 1280]</code> ，而后与文字的 hidden state <code>[77, 768]</code> 进行 cross attention 计算。（以上张量维度省略了 batch size）</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># hidden size 为 320 时候的 cross attention 单元示例</span></span>
<span class="line"><span style="color:#24292E;">Attention(</span></span>
<span class="line"><span style="color:#24292E;">(to_q): LoRACompatibleLinear(</span><span style="color:#E36209;">in_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">320</span><span style="color:#24292E;">, </span><span style="color:#E36209;">out_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">320</span><span style="color:#24292E;">, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">(to_k): LoRACompatibleLinear(</span><span style="color:#E36209;">in_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">768</span><span style="color:#24292E;">, </span><span style="color:#E36209;">out_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">320</span><span style="color:#24292E;">, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">(to_v): LoRACompatibleLinear(</span><span style="color:#E36209;">in_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">768</span><span style="color:#24292E;">, </span><span style="color:#E36209;">out_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">320</span><span style="color:#24292E;">, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这也是 SD Unet 中 Transformer2DBlock 与传统 Transformer 主要的不同，SD Unet 中的 Transformer2DBlock 输入与输出维度是不一样的。</p><ol start="3"><li><strong>super resolution</strong></li></ol><p>生成后 latent 大小为 64 * 64， 通过 VQModel 解码为 512*512</p><h3 id="sd-v1-1-v1-5" tabindex="-1"><a class="header-anchor" href="#sd-v1-1-v1-5" aria-hidden="true">#</a> SD v1.1 - v1.5</h3>`,11),K={href:"https://github.com/runwayml/stable-diffusion#weights",target:"_blank",rel:"noopener noreferrer"},Q={href:"https://huggingface.co/compvis",target:"_blank",rel:"noopener noreferrer"},W=s("code",null,"sd-v1-1.ckpt",-1),J=s("code",null,"256x256",-1),X={href:"https://huggingface.co/datasets/laion/laion2B-en",target:"_blank",rel:"noopener noreferrer"},Y=s("code",null,"512x512",-1),Z={href:"https://huggingface.co/datasets/laion/laion-high-resolution",target:"_blank",rel:"noopener noreferrer"},$=s("code",null,">= 1024x1024",-1),ss={href:"https://huggingface.co/compvis",target:"_blank",rel:"noopener noreferrer"},ns=s("code",null,"sd-v1-2.ckpt",-1),es=s("code",null,"sd-v1-1.ckpt",-1),as=s("code",null,"512x512",-1),ls={href:"https://laion.ai/blog/laion-aesthetics/",target:"_blank",rel:"noopener noreferrer"},os=s("code",null,"> 5.0",-1),ts=s("code",null,">= 512x512",-1),ps=s("code",null,"< 0.5",-1),is={href:"https://laion.ai/blog/laion-5b/",target:"_blank",rel:"noopener noreferrer"},rs={href:"https://github.com/christophschuhmann/improved-aesthetic-predictor",target:"_blank",rel:"noopener noreferrer"},cs={href:"https://huggingface.co/compvis",target:"_blank",rel:"noopener noreferrer"},ds=s("code",null,"sd-v1-3.ckpt",-1),us=s("code",null,"sd-v1-2.ckpt",-1),hs=s("code",null,"512x512",-1),ys={href:"https://arxiv.org/abs/2207.12598",target:"_blank",rel:"noopener noreferrer"},ms={href:"https://huggingface.co/compvis",target:"_blank",rel:"noopener noreferrer"},_s=s("code",null,"sd-v1-4.ckpt",-1),fs=s("code",null,"sd-v1-2.ckpt",-1),gs=s("code",null,"512x512",-1),bs={href:"https://arxiv.org/abs/2207.12598",target:"_blank",rel:"noopener noreferrer"},vs={href:"https://huggingface.co/runwayml/stable-diffusion-v1-5",target:"_blank",rel:"noopener noreferrer"},Es=s("code",null,"sd-v1-5.ckpt",-1),Ds=s("code",null,"sd-v1-2.ckpt",-1),Cs=s("code",null,"512x512",-1),As={href:"https://arxiv.org/abs/2207.12598",target:"_blank",rel:"noopener noreferrer"},ks={href:"https://huggingface.co/runwayml/stable-diffusion-inpainting",target:"_blank",rel:"noopener noreferrer"},xs=s("code",null,"sd-v1-5-inpainting.ckpt",-1),ws=s("code",null,"sd-v1-5.ckpt",-1),Ls=s("code",null,"512x512",-1),Ms={href:"https://arxiv.org/abs/2207.12598",target:"_blank",rel:"noopener noreferrer"},Bs=s("h3",{id:"sd-v2",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#sd-v2","aria-hidden":"true"},"#"),n(" SD v2")],-1),Ss={href:"https://github.com/Stability-AI/stablediffusion",target:"_blank",rel:"noopener noreferrer"},Ts=s("p",null,"架构方面 SD v2 系列：",-1),zs={href:"https://github.com/mlfoundations/open_clip",target:"_blank",rel:"noopener noreferrer"},qs=s("li",null,[n("Unet 架构改变：其中 Transformer 模块中的 "),s("code",null,"attention_head_dim"),n(" 变为了 "),s("code",null,"5,10,20,20"),n("，SD v1 中为 "),s("code",null,"8,8,8,8"),n("。"),s("code",null,"cross_attention_dim"),n(" 从 768 变为 1280。同时在 latent hidden state 进入 cross attention 之前，额外采用了 "),s("code",null,"linear_projection"),n(" 进行 latent hidden state 的处理，SD v1 中为卷积层处理。")],-1),Is=s("p",null,"训练方面 SD v2 系列，（以下拷贝了 huggingface 中 SD 模型 model card 的介绍） ：",-1),Ns={href:"https://huggingface.co/stabilityai/stable-diffusion-2-base",target:"_blank",rel:"noopener noreferrer"},Us=s("code",null,"256x256",-1),Rs={href:"https://laion.ai/blog/laion-5b/",target:"_blank",rel:"noopener noreferrer"},Fs={href:"https://github.com/LAION-AI/CLIP-based-NSFW-Detector",target:"_blank",rel:"noopener noreferrer"},Ps=s("code",null,"punsafe=0.1",-1),Vs={href:"https://github.com/christophschuhmann/improved-aesthetic-predictor",target:"_blank",rel:"noopener noreferrer"},Os=s("code",null,"4.5",-1),js=s("code",null,"512x512",-1),Gs=s("code",null,">= 512x512",-1),Hs={href:"https://huggingface.co/stabilityai/stable-diffusion-2",target:"_blank",rel:"noopener noreferrer"},Ks=s("code",null,"stable-diffusion-2",-1),Qs={href:"https://huggingface.co/stabilityai/stable-diffusion-2-base",target:"_blank",rel:"noopener noreferrer"},Ws=s("code",null,"512-base-ema.ckpt",-1),Js={href:"https://arxiv.org/abs/2202.00512",target:"_blank",rel:"noopener noreferrer"},Xs=s("code",null,"768x768",-1),Ys={href:"https://huggingface.co/stabilityai/stable-diffusion-2-1",target:"_blank",rel:"noopener noreferrer"},Zs=s("code",null,"stable-diffusion-2-1",-1),$s={href:"https://huggingface.co/stabilityai/stable-diffusion-2",target:"_blank",rel:"noopener noreferrer"},sn=s("code",null,"768-v-ema.ckpt",-1),nn=s("code",null,"punsafe=0.1",-1),en=s("code",null,"punsafe=0.98",-1),an=s("h2",{id:"lora",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#lora","aria-hidden":"true"},"#"),n(" Lora")],-1),ln={href:"https://link.zhihu.com/?target=https%3A//huggingface.co/docs/diffusers/training/lora",target:"_blank",rel:"noopener noreferrer"},on=s("p",null,[n("Diffusers 中，SD 采用 Lora 的部分位于 Unet 当中，大部分的 Lora 在 Transformer 模块当中，SD 的 lora 与 NLP Lora 实现方式基本相同， "),s("strong",null,"一个较大的区别在于，SD 中的 Lora 除了对线性层进行 Lora 叠加外，也对卷积层进行了 Lora 改造"),n(" 。")],-1);function tn(pn,rn){const e=t("ExternalLinkIcon");return p(),i("div",null,[c,s("p",null,[n("以此不同于前几份笔记，本文主要参考 "),s("a",d,[n("huggingface/diffusers"),a(e)]),n(" 中 Latent Diffusion Model 及 Stable Diffusion 的实现，对 LDM 架构及其中的 Conditioning Cross Attention 做梳理。")]),u,s("ul",null,[s("li",null,[s("a",h,[n("Kevin 吴嘉文：Diffusion|DDPM 理解、数学、代码"),a(e)])]),s("li",null,[s("a",y,[n("Kevin 吴嘉文：DIFFUSION 系列笔记|DDIM 数学、思考与 ppdiffuser 代码探索"),a(e)])]),s("li",null,[s("a",m,[n("Kevin 吴嘉文：DIFFUSION 系列笔记| SDE（上）"),a(e)])])]),_,s("p",null,[n("对于 "),f,n("，其中使用了 "),g,n("， 参考 "),s("a",b,[n("huggignface 的 LDMBert"),a(e)]),n(" 实现，"),v,n(" 与传统 BERT 架构相似，规模不同，LDMBert 采用 32 层， hidden_size 为 1280，属实比 bert-base 大上不少。同时文本被 padding 到了固定的 77 长度，以此来保证文字的 hidden state 格式为 "),E,n("。")]),D,s("p",null,[n("文字与图像的交互就发生在 "),C,n(" 当中。每个 "),s("a",A,[n("Transformer2DModel"),a(e)]),n(" 先对输入的图像数据进行预处理，将图片格式从如 "),k,n(" 或 "),x,n(" 转换为 "),w,n("，而后将 "),L,n(" 传入 1 层传统 Transformer layer（非 bert 或 GPT 类型），先对图像 "),M,n(" 进行 self-attention，而后结合 "),B,n(" 进行 cross attention 处理。")]),S,s("p",null,[n("在 "),T,n(" 中，upsampling 和 down sampling 之间采用 "),s("a",z,[n("MidBlock2DCrossAttn"),a(e)]),n(" 连接，"),q,n(" 包括了 1 个 1 层的 "),I,n(" 以及 1 个 "),N,n(),U,n("。")]),R,s("p",null,[n("参考 "),s("a",F,[n("hugging face diffuser 的 SD pipeline 实现"),a(e)]),n("。以 "),P,n(" 为例。")]),V,s("p",null,[n("采用 "),O,n("，来自于 "),s("a",j,[n("CLIP"),a(e)]),n(" 的 Text Encoder 部分。相比于其他传统的 Transformer 语言模型，CLIP 在预训练时，在 text-image pair 数据集上进行了对比学习预训练。prompt_embeds, negative_prompt_embeds 在经过编码后，shape 都为 "),G]),H,s("p",null,[n("stable diffusion 1.1-1.5 的模型架构相同，以下搬运 "),s("a",K,[n("runwayml"),a(e)]),n(" 的 stable diffusion weights 总结：")]),s("ul",null,[s("li",null,[s("p",null,[s("a",Q,[W,a(e)]),n(": 237k steps at resolution "),J,n(" on "),s("a",X,[n("laion2B-en"),a(e)]),n(". 194k steps at resolution "),Y,n(" on "),s("a",Z,[n("laion-high-resolution"),a(e)]),n(" (170M examples from LAION-5B with resolution "),$,n(").")])]),s("li",null,[s("p",null,[s("a",ss,[ns,a(e)]),n(": Resumed from "),es,n(". 515k steps at resolution "),as,n(" on "),s("a",ls,[n("laion-aesthetics v2 5+"),a(e)]),n(" (a subset of laion2B-en with estimated aesthetics score "),os,n(", and additionally filtered to images with an original size "),ts,n(", and an estimated watermark probability "),ps,n(". The watermark estimate is from the "),s("a",is,[n("LAION-5B"),a(e)]),n(" metadata, the aesthetics score is estimated using the "),s("a",rs,[n("LAION-Aesthetics Predictor V2"),a(e)]),n(").")])]),s("li",null,[s("p",null,[s("a",cs,[ds,a(e)]),n(": Resumed from "),us,n(". 195k steps at resolution "),hs,n(' on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve '),s("a",ys,[n("classifier-free guidance sampling"),a(e)]),n(".")])]),s("li",null,[s("p",null,[s("a",ms,[_s,a(e)]),n(": Resumed from "),fs,n(". 225k steps at resolution "),gs,n(' on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve '),s("a",bs,[n("classifier-free guidance sampling"),a(e)]),n(".")])]),s("li",null,[s("p",null,[s("a",vs,[Es,a(e)]),n(": Resumed from "),Ds,n(". 595k steps at resolution "),Cs,n(' on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve '),s("a",As,[n("classifier-free guidance sampling"),a(e)]),n(".")])]),s("li",null,[s("p",null,[s("a",ks,[xs,a(e)]),n(": Resumed from "),ws,n(". 440k steps of inpainting training at resolution "),Ls,n(' on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve '),s("a",Ms,[n("classifier-free guidance sampling"),a(e)]),n(". For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.")])])]),Bs,s("p",null,[n("参考 "),s("a",Ss,[n("stability-AI 仓库"),a(e)]),n("，SD v2 相对 v1 系列改动较大：")]),Ts,s("ul",null,[s("li",null,[n("采用了 "),s("a",zs,[n("OpenCLIP-ViT/H"),a(e)]),n(" 作为 text encoder。")]),qs]),Is,s("ul",null,[s("li",null,[s("a",Ns,[n("SD 2.0-base"),a(e)]),n("：The model is trained from scratch 550k steps at resolution "),Us,n(" on a subset of "),s("a",Rs,[n("LAION-5B"),a(e)]),n(" filtered for explicit pornographic material, using the "),s("a",Fs,[n("LAION-NSFW classifier"),a(e)]),n(" with "),Ps,n(" and an "),s("a",Vs,[n("aesthetic score"),a(e)]),n(" >= "),Os,n(". Then it is further trained for 850k steps at resolution "),js,n(" on the same dataset on images with resolution "),Gs,n(".")]),s("li",null,[s("a",Hs,[n("SD v2.0"),a(e)]),n("：This "),Ks,n(" model is resumed from "),s("a",Qs,[n("stable-diffusion-2-base"),a(e)]),n(" ("),Ws,n(") and trained for 150k steps using a "),s("a",Js,[n("v-objective"),a(e)]),n(" on the same dataset. Resumed for another 140k steps on "),Xs,n(" images.")]),s("li",null,[s("a",Ys,[n("SD v2.1"),a(e)]),n("：This "),Zs,n(" model is fine-tuned from "),s("a",$s,[n("stable-diffusion-2"),a(e)]),n(" ("),sn,n(") with an additional 55k steps on the same dataset (with "),nn,n("), and then fine-tuned for another 155k extra steps with "),en,n(".")])]),an,s("p",null,[n("huggingface diffuser 中 Lora 的实现与 huggingface/PEFT 实现方法相似，添加 Lora 只需要通过撰写规则，锁定需要改动的 layer，并替换为 LoRACompatibleLayer 实现，huggingface 也提供好了 "),s("a",ln,[n("lora 训练代码"),a(e)]),n("，和 SD lora 推理方法。")]),on])}const un=o(r,[["render",tn],["__file","笔记latent_diffusion.html.vue"]]);export{un as default};
