import{_ as t,P as p,U as r,Y as n,a1 as a,X as e,aQ as o,E as c}from"./framework-6cee4965.js";const i={},l=n("h1",{id:"huggingface-training",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#huggingface-training","aria-hidden":"true"},"#"),a(" Huggingface Training")],-1),u={href:"https://huggingface.co/docs/transformers/perf_train_gpu_one",target:"_blank",rel:"noopener noreferrer"},d=o(`<p>经过几年的发展，transformers 的训练框架也变得成熟，在 bert 时代我们可能需要手写许多优化过程，当初 huggingface 快速上手中的示例代码，大部分还是类似以下的操作：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">for</span> _ <span class="token keyword">in</span> num_epochs<span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
	loss <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果要采用混合精度训练，gradient accumulate 等策略时需要手动添加。而今我们只需要配置 <code>Trainier</code> 和 <code>TrainingArgument</code> 即可：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments<span class="token punctuation">,</span> Trainer<span class="token punctuation">,</span> logging

logging<span class="token punctuation">.</span>set_verbosity_error<span class="token punctuation">(</span><span class="token punctuation">)</span>


training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>per_device_train_batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">**</span>default_args<span class="token punctuation">)</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>model<span class="token operator">=</span>model<span class="token punctuation">,</span> args<span class="token operator">=</span>training_args<span class="token punctuation">,</span> train_dataset<span class="token operator">=</span>ds<span class="token punctuation">)</span>
result <span class="token operator">=</span> trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
print_summary<span class="token punctuation">(</span>result<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="训练超参梳理" tabindex="-1"><a class="header-anchor" href="#训练超参梳理" aria-hidden="true">#</a> 训练超参梳理</h3><p>以下对 <code>tranformers.training_args.TrainingArguments</code> 中，部分较使用的方法进行清点：</p><p>训练策略相关 strategy 类型</p><ul><li><p><code>evaluation_strategy</code>：evaluation 的方式，可选择 <code>no</code>, <code>step</code>, <code>epoch</code>。通常我们算 <code>step</code> 或 <code>no</code>。</p></li><li><p><code>logging_dir</code>：日志文件夹</p></li></ul><h3 id="关键训练配置" tabindex="-1"><a class="header-anchor" href="#关键训练配置" aria-hidden="true">#</a> 关键训练配置</h3><p>对于 batch size，可配置的参数有：</p><ul><li><p><code>per_device_train_batch_size</code>：每个设备上的 batch_size。</p></li><li><p><code>gradient_accumulation_steps</code>：累计 gradient 的步数，</p></li></ul><div class="hint-container info"><p class="hint-container-title">相关信息</p><p>如你有 4 张显卡，显卡最多放入 4 个 batch，那么设置 <code>gradient_accumulation_steps=4</code>， <code>per_device_train_batch_size=4</code> 近似于全局 64 batch size 的效果，当然这也与你是否使用 batch norm，以及多卡状态下所使用的同步策略有关。</p></div><p>对于 learning_rate，相关参数有：</p><ul><li><p><code>learning_rate</code>：通常结合 batch size 进行缩放调整。</p></li><li><p><code>lr_scheduler_type</code>：默认采用 <code>&quot;linear&quot;</code>。</p></li><li><p><code>warmup_steps</code>/<code>warmup_ratio</code>：这两者的默认值为 0， 可以参考其他 LLM 训练配置，如 alpaca 配置 <code>warmup_ratio=0.03</code></p></li></ul><p>其他训练配置：</p><ul><li><p><code>num_train_epochs</code>/<code>max_steps</code>：训练时长，默认是 <code>num_train_epochs=3</code></p></li><li><p><code>gradient_checkpointing</code>: 是否采用 gradient checkpoint 来对显存进行优化，使用之后可以提高 <code>per_device_train_batch_size</code></p></li><li><p><code>optim</code>：优化器类型，比较常用的如 <code>adamw_torch</code></p></li><li><p><code>max_grad_norm</code>：在 clip gradient 时候采用的参数，可以防止梯度爆炸。</p></li><li><p><code>group_by_length</code>：将长度低的样本放到一个 batch 当中训练，这样能够尽量控制 padding 数量，提高 transformers 的训练速度。</p></li></ul><p>保存模型时的部分配置：</p>`,17),_=n("li",null,[n("p",null,[n("code",null,"save_steps"),a("：保存模型的周期（以 step 为单位）。如果想要通过 epoch 保存的话，需要额外调整 "),n("code",null,"save_strategy=epoch"),a("。")])],-1),k=n("li",null,[n("p",null,[n("code",null,"save_total_limit"),a("：限制保存 checkpoint 的数量，在本地磁盘空间不够时候可以用得上。")])],-1),m=n("code",null,"save_safetensors",-1),v={href:"https://huggingface.co/docs/safetensors/index",target:"_blank",rel:"noopener noreferrer"},g=n("ul",null,[n("li",null,[n("code",null,"metric_for_best_model"),a("：默认是使用 "),n("code",null,"loss"),a(" 来判断模型好坏。")])],-1),h=n("p",null,"加速训练主要参数",-1),b=n("li",null,[n("p",null,[n("code",null,"jit_mode_eval"),a("：通常，我们使用静态图进行推理时，速度会是动态图的 2 倍以上。使用 "),n("code",null,"jit_mode_eval"),a(" 能够提高 eval 时候的推理速度。")])],-1),f=n("li",null,[n("p",null,[n("code",null,"fp16"),a("：是否采用混合精度训练，通过调整 "),n("code",null,"fp16_opt_level"),a(" 混合精度训练能够在减少显存占用，提高训练速度的同时，尽可能高地保留训练效果。一般在对 softmax 等 activation 层应用 fp16 后，训练时间可以缩短 2/3，但训练过程中，出现数值溢出的可能性高于 bf16 或者 tf32。")])],-1),y=n("code",null,"bf16",-1),x=n("code",null,"tf32",-1),z={href:"https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/",target:"_blank",rel:"noopener noreferrer"},w=o(`<figure><img src="https://developer-blogs.nvidia.com/wp-content/uploads/2021/01/AI_training_TF32_tensor_cores_F2-625x371.png" alt="Breakdowns of sign, range and mantissa bits for common DL precision formats." tabindex="0" loading="lazy"><figcaption>Breakdowns of sign, range and mantissa bits for common DL precision formats.</figcaption></figure><h2 id="完成一次训练" tabindex="-1"><a class="header-anchor" href="#完成一次训练" aria-hidden="true">#</a> 完成一次训练</h2><p>在代码中定义：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>parser <span class="token operator">=</span> transformers<span class="token punctuation">.</span>HfArgumentParser<span class="token punctuation">(</span><span class="token punctuation">(</span>ModelArguments<span class="token punctuation">,</span> DataArguments<span class="token punctuation">,</span> TrainingArguments<span class="token punctuation">)</span><span class="token punctuation">)</span>
model_args<span class="token punctuation">,</span> data_args<span class="token punctuation">,</span> training_args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args_into_dataclasses<span class="token punctuation">(</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> transformers<span class="token punctuation">.</span>AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>xxx<span class="token punctuation">)</span>

tokenizer <span class="token operator">=</span> transformers<span class="token punctuation">.</span>AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>xxx<span class="token punctuation">)</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>model<span class="token operator">=</span>model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> args<span class="token operator">=</span>training_args<span class="token punctuation">,</span> <span class="token operator">**</span>data_module<span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>save_state<span class="token punctuation">(</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span>output_dir<span class="token operator">=</span>training_args<span class="token punctuation">.</span>output_dir<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在启动训练文件时，传入超参进行配置：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>torchrun <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">4</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token operator">&lt;</span>your_random_port<span class="token operator">&gt;</span> train<span class="token punctuation">.</span>py \\
    <span class="token operator">-</span><span class="token operator">-</span>model_name_or_path <span class="token operator">&lt;</span>your_path_to_hf_converted_llama_ckpt_and_tokenizer<span class="token operator">&gt;</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>data_path <span class="token punctuation">.</span><span class="token operator">/</span>alpaca_data<span class="token punctuation">.</span>json \\
    <span class="token operator">-</span><span class="token operator">-</span>bf16 <span class="token boolean">True</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>output_dir <span class="token operator">&lt;</span>your_output_dir<span class="token operator">&gt;</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>num_train_epochs <span class="token number">3</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>per_device_train_batch_size <span class="token number">4</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>per_device_eval_batch_size <span class="token number">4</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>gradient_accumulation_steps <span class="token number">8</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>evaluation_strategy <span class="token string">&quot;no&quot;</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>save_strategy <span class="token string">&quot;steps&quot;</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>save_steps <span class="token number">2000</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>save_total_limit <span class="token number">1</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>learning_rate <span class="token number">2e-5</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>weight_decay <span class="token number">0.</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>warmup_ratio <span class="token number">0.03</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>lr_scheduler_type <span class="token string">&quot;cosine&quot;</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>logging_steps <span class="token number">1</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>fsdp <span class="token string">&quot;full_shard auto_wrap&quot;</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>fsdp_transformer_layer_cls_to_wrap <span class="token string">&#39;LlamaDecoderLayer&#39;</span> \\
    <span class="token operator">-</span><span class="token operator">-</span>tf32 <span class="token boolean">True</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,6);function T(A,q){const s=c("ExternalLinkIcon");return p(),r("div",null,[l,n("blockquote",null,[n("p",null,[a("本文参考 transformers 官方指南 "),n("a",u,[a("link"),e(s)])])]),d,n("ul",null,[_,k,n("li",null,[n("p",null,[m,a("：是否通过 safetensors 格式保存，对于该格式，可以查看 "),n("a",v,[a("官方指南"),e(s)])])])]),g,h,n("ul",null,[b,f,n("li",null,[n("p",null,[y,a(" 及 "),x,a("：这两者是 NVIDIA Ampere 架构才支持的数值格式，在 "),n("a",z,[a("NVIDIA 博客"),e(s)]),a(" 上可以看到详细的介绍。总结来说，部分显卡对于 bf16 以及 tf32 格式有很好的支持，相对于 fp16，有着更好的精度以及更快的速度。")])])]),w])}const I=t(i,[["render",T],["__file","笔记hf_trainer.html.vue"]]);export{I as default};
