const e=JSON.parse('{"key":"v-aeb2b792","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0moe.html","title":"近期 MOE 模型整理","lang":"zh-CN","frontmatter":{"title":"近期 MOE 模型整理","date":"2024-07-28T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["AIGC","LLM"],"description":"在本文中，我们梳理了近期 （24 年 7 月前）部分 MOE 大模型的关键信息，包括它们的主要特点、亮点以及相关资源链接。涉及模型 Mixtral 8x7B，Mixtral 8x22B，DeepSeek-MoE，Qwen1.5-MoE，DeepSeek-V2 混合专家模型的 Transformer 模型 对于 MOE 的基础，相比 dense model，MOE 的预训练速度更快，推理速度更快，但需要大量的显存。此外，MOE 的训练也有一些独有的 tips，详细的 MOE 混合专家模型基础，推荐参考： 混合专家模型基础（推荐）","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0moe.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"近期 MOE 模型整理"}],["meta",{"property":"og:description","content":"在本文中，我们梳理了近期 （24 年 7 月前）部分 MOE 大模型的关键信息，包括它们的主要特点、亮点以及相关资源链接。涉及模型 Mixtral 8x7B，Mixtral 8x22B，DeepSeek-MoE，Qwen1.5-MoE，DeepSeek-V2 混合专家模型的 Transformer 模型 对于 MOE 的基础，相比 dense model，MOE 的预训练速度更快，推理速度更快，但需要大量的显存。此外，MOE 的训练也有一些独有的 tips，详细的 MOE 混合专家模型基础，推荐参考： 混合专家模型基础（推荐）"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:published_time","content":"2024-07-28T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"近期 MOE 模型整理\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-07-28T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"混合专家模型的 Transformer 模型","slug":"混合专家模型的-transformer-模型","link":"#混合专家模型的-transformer-模型","children":[]},{"level":2,"title":"Mixtral 8*7B","slug":"mixtral-8-7b","link":"#mixtral-8-7b","children":[]},{"level":2,"title":"DeepSeek-MoE","slug":"deepseek-moe","link":"#deepseek-moe","children":[]},{"level":2,"title":"Qwen1.5-MoE","slug":"qwen1-5-moe","link":"#qwen1-5-moe","children":[]},{"level":2,"title":"DeepSeek-V2","slug":"deepseek-v2","link":"#deepseek-v2","children":[]},{"level":2,"title":"Mixtral 8*22B","slug":"mixtral-8-22b","link":"#mixtral-8-22b","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":11.48,"words":3445},"filePathRelative":"posts/notes/articles/笔记moe.md","localizedDate":"2024年7月28日","excerpt":"<p>在本文中，我们梳理了近期 （24 年 7 月前）部分 MOE 大模型的关键信息，包括它们的主要特点、亮点以及相关资源链接。涉及模型  Mixtral 8x7B，Mixtral 8x22B，DeepSeek-MoE，Qwen1.5-MoE，DeepSeek-V2</p>\\n<h2> 混合专家模型的 Transformer 模型</h2>\\n<p>对于 MOE 的基础，相比 dense model，MOE 的预训练速度更快，推理速度更快，但需要大量的显存。此外，MOE 的训练也有一些独有的 tips，详细的 MOE 混合专家模型基础，推荐参考：</p>\\n<p><a href=\\"https://huggingface.co/blog/zh/moe\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">混合专家模型基础（推荐）</a></p>","autoDesc":true}');export{e as data};
