import{_ as n}from"./plugin-vue_export-helper-c27b6911.js";import{r as l,o,c as i,a as s,b as e,d as a,f as r}from"./app-2598c59c.js";const c="/assets/img/speed_sd/image-20231216210637171.png",p="/assets/img/speed_sd/image-20231216222100682.png",d={},h=s("h2",{id:"cm-lcm-lcm-lora",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#cm-lcm-lcm-lora","aria-hidden":"true"},"#"),e(" CM，LCM，LCM-Lora")],-1),m=s("p",null,"对于 Consistency Model，Latent Consistency Model 及 LCM-LoRA 的原理解读，十分推荐这篇文章：",-1),u=s("p",null,"https://wrong.wang/blog/20231111-consistency-is-all-you-need/",-1),_=s("p",null,"具体细节建议参考上面推荐的文章链接，以下对大致思路进行总结：",-1),g=s("ul",null,[s("li",null,[e("Consistency Model 基于扩散模型，增加了一个推导约束：每个样本到噪声的加噪轨迹上的每个点都可以通过一个函数 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"f"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"x"),s("mi",null,"t")]),s("mo",{separator:"true"},","),s("mi",null,"t"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"f(x_t, t)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mclose"},")")])])]),e(" 映射回轨迹的起点。同时 CM 也对模型训练时的损失、sample 方案等进行了改动，以允许我们使用 2-4 个 step 来生成高质量图片。")])],-1),f=r('<figure><img src="'+c+'" alt="image-20231216210637171" tabindex="0" loading="lazy"><figcaption>image-20231216210637171</figcaption></figure><ul><li><p>LCM 在 SD 基础上进行蒸馏，在蒸馏过程中加入了 CM 中的 Consistency 约束。官方的用词是 <code>distill</code>：</p><blockquote><p>“LCMs can be distilled from any pre-trained Stable Diffusion (SD) in only 4,000 training steps，</p></blockquote><p>但经过比较 stabilityai/stable-diffusion-xl-base-1.0 和 latent-consistency/lcm-sdxl 的 UNET 参数配置。 LCM 蒸馏似乎并未改变模型的大小。经过蒸馏后，我们可以使用 2-4 个 step 来生成媲美教师模型的图片。</p></li></ul><ul><li>LCM-Lora：因为 LCM 也可以看做一种 finetune，因此我们也可以通过 lora 的方式进行高效参数微调，来蒸馏出一个 LCM-Lora。使用任何 SD 模型配合该 LCM-Lora，均能实现 2-4 step 的高质量成图。</li></ul><p>::: important</p>',4),L={href:"https://huggingface.co/docs/diffusers/main/en/using-diffusers/lcm",target:"_blank",rel:"noopener noreferrer"},M=s("code",null,"guidance_scale",-1),x=s("code",null,"[3., 13.]",-1),b=s("p",null,":::",-1),y={href:"https://huggingface.co/spaces/latent-consistency/Real-Time-LCM-Text-to-Image-Lora-SD1.5",target:"_blank",rel:"noopener noreferrer"},C=s("code",null,"LCMScheduler",-1),v=s("h2",{id:"sdxl-turbo",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#sdxl-turbo","aria-hidden":"true"},"#"),e(" SDXL Turbo")],-1),S={href:"https://static1.squarespace.com/static/6213c340453c3f502425776e/t/65663480a92fba51d0e1023f/1701197769659/adversarial_diffusion_distillation.pdf",target:"_blank",rel:"noopener noreferrer"},D={href:"https://huggingface.co/docs/diffusers/using-diffusers/sdxl_turbo",target:"_blank",rel:"noopener noreferrer"},k=s("p",null,"对于原理解析，可以参考文章：",-1),w=s("p",null,"https://zhuanlan.zhihu.com/p/669353808",-1),z=s("p",null,[e("SDXL Turbo 通过蒸馏 SDXL，根据官方描述，在 A100 上，生成一张 "),s("code",null,"512 * 512"),e(" 图片只需要 207ms，其中 UNET 推理用了 67 ms。")],-1),A=s("p",null,"以下为大致训练方式：",-1),T=s("figure",null,[s("img",{src:p,alt:"image-20231216222100682",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20231216222100682")],-1),N=s("p",null,[e("其中 ADD-student 的输入由 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"x"),s("mn",null,"0")])]),s("annotation",{encoding:"application/x-tex"},"x_0")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"0")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),e(" 经过 forward diffusion process 生成，而 DM-teacher 的输入由 ADD-Student 的输出经过 forward diffusion process 生成。")],-1),R=s("p",null,"与 LCM 类似，SDXL-turbo 似乎也能像 LCM 一样出一个 LORA 版。",-1),E=s("h2",{id:"参考",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#参考","aria-hidden":"true"},"#"),e(" 参考")],-1),I=s("p",null,"https://wrong.wang/blog/20231111-consistency-is-all-you-need/",-1),X={href:"https://arxiv.org/pdf/2303.01469.pdf",target:"_blank",rel:"noopener noreferrer"},V={href:"https://arxiv.org/pdf/2310.04378.pdf",target:"_blank",rel:"noopener noreferrer"},q={href:"https://arxiv.org/pdf/2311.05556.pdf",target:"_blank",rel:"noopener noreferrer"};function B(U,P){const t=l("ExternalLinkIcon");return o(),i("div",null,[h,m,u,_,g,f,s("p",null,[e("参考 "),s("a",L,[e("Performing inference with LCM"),a(t)]),e("。"),M,e(" 对于 LCM-Lora，建议设置为 0（也可以试试 1-2）。对于 LCM 建议取值范围 "),x]),b,s("p",null,[e("笔者测试了官方提供的 "),s("a",y,[e("Real-Time Latent Consistency Model SDv1.5"),a(t)]),e("，将其中的 LCMScheduler 改为 DPMSolverMultistepScheduler，似乎效果更好（从 CM 算法角度看，应该是 LCM Scheduler 效果更好？）。但在本地尝试 SDXL LCM Lora 时，"),C,e(" 的效果还是会好于其他 schedular 的。")]),v,s("p",null,[e("参考 "),s("a",S,[e("技术报告： Adversarial Diffusion Distillation"),a(t)]),e(" 和 "),s("a",D,[e("huggingface diffusors 代码"),a(t)]),e("。")]),k,w,z,A,T,N,R,E,I,s("p",null,[s("a",X,[e("Consistency Models"),a(t)])]),s("p",null,[s("a",V,[e("Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference"),a(t)])]),s("p",null,[s("a",q,[e("LCM-LoRA: A Universal Stable-Diffusion Acceleration Module"),a(t)])])])}const O=n(d,[["render",B],["__file","笔记speed_sd.html.vue"]]);export{O as default};
