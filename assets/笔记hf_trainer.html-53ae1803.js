import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as p,o as t,c as r,a as s,b as n,d as l,f as e}from"./app-4ea08187.js";const c={},i=s("h1",{id:"huggingface-training",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#huggingface-training","aria-hidden":"true"},"#"),n(" Huggingface Training")],-1),d={href:"https://huggingface.co/docs/transformers/perf_train_gpu_one",target:"_blank",rel:"noopener noreferrer"},y=e(`<p>经过几年的发展，transformers 的训练框架也变得成熟，在 bert 时代我们可能需要手写许多优化过程，当初 huggingface 快速上手中的示例代码，大部分还是类似以下的操作：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> _ </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> num_epochs:</span></span>
<span class="line"><span style="color:#24292E;">    optimizer.zero_grad()</span></span>
<span class="line"><span style="color:#24292E;">	loss </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> model(</span><span style="color:#D73A49;">**</span><span style="color:#24292E;">inputs)</span></span>
<span class="line"><span style="color:#24292E;">    loss.backward()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果要采用混合精度训练，gradient accumulate 等策略时需要手动添加。而今我们只需要配置 <code>Trainier</code> 和 <code>TrainingArgument</code> 即可：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> transformers </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> TrainingArguments, Trainer, logging</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">logging.set_verbosity_error()</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">training_args </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> TrainingArguments(</span><span style="color:#E36209;">per_device_train_batch_size</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">**</span><span style="color:#24292E;">default_args)</span></span>
<span class="line"><span style="color:#24292E;">trainer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> Trainer(</span><span style="color:#E36209;">model</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">model, </span><span style="color:#E36209;">args</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">training_args, </span><span style="color:#E36209;">train_dataset</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">ds)</span></span>
<span class="line"><span style="color:#24292E;">result </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> trainer.train()</span></span>
<span class="line"><span style="color:#24292E;">print_summary(result)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="训练超参梳理" tabindex="-1"><a class="header-anchor" href="#训练超参梳理" aria-hidden="true">#</a> 训练超参梳理</h3><p>以下对 <code>tranformers.training_args.TrainingArguments</code> 中，部分较使用的方法进行清点：</p><p>训练策略相关 strategy 类型</p><ul><li><p><code>evaluation_strategy</code>：evaluation 的方式，可选择 <code>no</code>, <code>step</code>, <code>epoch</code>。通常我们算 <code>step</code> 或 <code>no</code>。</p></li><li><p><code>logging_dir</code>：日志文件夹</p></li></ul><h3 id="关键训练配置" tabindex="-1"><a class="header-anchor" href="#关键训练配置" aria-hidden="true">#</a> 关键训练配置</h3><p>对于 batch size，可配置的参数有：</p><ul><li><p><code>per_device_train_batch_size</code>：每个设备上的 batch_size。</p></li><li><p><code>gradient_accumulation_steps</code>：累计 gradient 的步数，</p></li></ul><div class="hint-container info"><p class="hint-container-title">相关信息</p><p>如你有 4 张显卡，显卡最多放入 4 个 batch，那么设置 <code>gradient_accumulation_steps=4</code>， <code>per_device_train_batch_size=4</code> 近似于全局 64 batch size 的效果，当然这也与你是否使用 batch norm，以及多卡状态下所使用的同步策略有关。</p></div><p>对于 learning_rate，相关参数有：</p><ul><li><p><code>learning_rate</code>：通常结合 batch size 进行缩放调整。</p></li><li><p><code>lr_scheduler_type</code>：默认采用 <code>&quot;linear&quot;</code>。</p></li><li><p><code>warmup_steps</code>/<code>warmup_ratio</code>：这两者的默认值为 0， 可以参考其他 LLM 训练配置，如 alpaca 配置 <code>warmup_ratio=0.03</code></p></li></ul><p>其他训练配置：</p><ul><li><p><code>num_train_epochs</code>/<code>max_steps</code>：训练时长，默认是 <code>num_train_epochs=3</code></p></li><li><p><code>gradient_checkpointing</code>: 是否采用 gradient checkpoint 来对显存进行优化，使用之后可以提高 <code>per_device_train_batch_size</code></p></li><li><p><code>optim</code>：优化器类型，比较常用的如 <code>adamw_torch</code></p></li><li><p><code>max_grad_norm</code>：在 clip gradient 时候采用的参数，可以防止梯度爆炸。</p></li><li><p><code>group_by_length</code>：将长度低的样本放到一个 batch 当中训练，这样能够尽量控制 padding 数量，提高 transformers 的训练速度。</p></li></ul><p>保存模型时的部分配置：</p>`,17),u=s("li",null,[s("p",null,[s("code",null,"save_steps"),n("：保存模型的周期（以 step 为单位）。如果想要通过 epoch 保存的话，需要额外调整 "),s("code",null,"save_strategy=epoch"),n("。")])],-1),_=s("li",null,[s("p",null,[s("code",null,"save_total_limit"),n("：限制保存 checkpoint 的数量，在本地磁盘空间不够时候可以用得上。")])],-1),m=s("code",null,"save_safetensors",-1),v={href:"https://huggingface.co/docs/safetensors/index",target:"_blank",rel:"noopener noreferrer"},g=s("ul",null,[s("li",null,[s("code",null,"metric_for_best_model"),n("：默认是使用 "),s("code",null,"loss"),n(" 来判断模型好坏。")])],-1),E=s("p",null,"加速训练主要参数",-1),h=s("li",null,[s("p",null,[s("code",null,"jit_mode_eval"),n("：通常，我们使用静态图进行推理时，速度会是动态图的 2 倍以上。使用 "),s("code",null,"jit_mode_eval"),n(" 能够提高 eval 时候的推理速度。")])],-1),f=s("li",null,[s("p",null,[s("code",null,"fp16"),n("：是否采用混合精度训练，通过调整 "),s("code",null,"fp16_opt_level"),n(" 混合精度训练能够在减少显存占用，提高训练速度的同时，尽可能高地保留训练效果。一般在对 softmax 等 activation 层应用 fp16 后，训练时间可以缩短 2/3，但训练过程中，出现数值溢出的可能性高于 bf16 或者 tf32。")])],-1),b=s("code",null,"bf16",-1),D=s("code",null,"tf32",-1),A={href:"https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/",target:"_blank",rel:"noopener noreferrer"},x=e(`<figure><img src="https://developer-blogs.nvidia.com/wp-content/uploads/2021/01/AI_training_TF32_tensor_cores_F2-625x371.png" alt="Breakdowns of sign, range and mantissa bits for common DL precision formats." tabindex="0" loading="lazy"><figcaption>Breakdowns of sign, range and mantissa bits for common DL precision formats.</figcaption></figure><h2 id="完成一次训练" tabindex="-1"><a class="header-anchor" href="#完成一次训练" aria-hidden="true">#</a> 完成一次训练</h2><p>在代码中定义：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">parser </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))</span></span>
<span class="line"><span style="color:#24292E;">model_args, data_args, training_args </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> parser.parse_args_into_dataclasses()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">model </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> transformers.AutoModelForCausalLM.from_pretrained(xxx)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">tokenizer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> transformers.AutoTokenizer.from_pretrained(xxx)</span></span>
<span class="line"><span style="color:#24292E;">trainer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> Trainer(</span><span style="color:#E36209;">model</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">model, </span><span style="color:#E36209;">tokenizer</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">tokenizer, </span><span style="color:#E36209;">args</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">training_args, </span><span style="color:#D73A49;">**</span><span style="color:#24292E;">data_module)</span></span>
<span class="line"><span style="color:#24292E;">trainer.train()</span></span>
<span class="line"><span style="color:#24292E;">trainer.save_state()</span></span>
<span class="line"><span style="color:#24292E;">trainer.save_model(</span><span style="color:#E36209;">output_dir</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">training_args.output_dir)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在启动训练文件时，传入超参进行配置：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">torchrun </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">nproc_per_node</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">4</span><span style="color:#24292E;"> </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">master_port</span><span style="color:#D73A49;">=&lt;</span><span style="color:#24292E;">your_random_port</span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> train.py \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">model_name_or_path </span><span style="color:#D73A49;">&lt;</span><span style="color:#24292E;">your_path_to_hf_converted_llama_ckpt_and_tokenizer</span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">data_path .</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">alpaca_data.json \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">bf16 </span><span style="color:#005CC5;">True</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">output_dir </span><span style="color:#D73A49;">&lt;</span><span style="color:#24292E;">your_output_dir</span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">num_train_epochs </span><span style="color:#005CC5;">3</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">per_device_train_batch_size </span><span style="color:#005CC5;">4</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">per_device_eval_batch_size </span><span style="color:#005CC5;">4</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">gradient_accumulation_steps </span><span style="color:#005CC5;">8</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">evaluation_strategy </span><span style="color:#032F62;">&quot;no&quot;</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">save_strategy </span><span style="color:#032F62;">&quot;steps&quot;</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">save_steps </span><span style="color:#005CC5;">2000</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">save_total_limit </span><span style="color:#005CC5;">1</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">learning_rate </span><span style="color:#005CC5;">2e-5</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">weight_decay </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">. \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">warmup_ratio </span><span style="color:#005CC5;">0.03</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">lr_scheduler_type </span><span style="color:#032F62;">&quot;cosine&quot;</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">logging_steps </span><span style="color:#005CC5;">1</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">fsdp </span><span style="color:#032F62;">&quot;full_shard auto_wrap&quot;</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">fsdp_transformer_layer_cls_to_wrap </span><span style="color:#032F62;">&#39;LlamaDecoderLayer&#39;</span><span style="color:#24292E;"> \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#B31D28;font-style:italic;">--</span><span style="color:#24292E;">tf32 </span><span style="color:#005CC5;">True</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,6);function C(k,B){const a=p("ExternalLinkIcon");return t(),r("div",null,[i,s("blockquote",null,[s("p",null,[n("本文参考 transformers 官方指南 "),s("a",d,[n("link"),l(a)])])]),y,s("ul",null,[u,_,s("li",null,[s("p",null,[m,n("：是否通过 safetensors 格式保存，对于该格式，可以查看 "),s("a",v,[n("官方指南"),l(a)])])])]),g,E,s("ul",null,[h,f,s("li",null,[s("p",null,[b,n(" 及 "),D,n("：这两者是 NVIDIA Ampere 架构才支持的数值格式，在 "),s("a",A,[n("NVIDIA 博客"),l(a)]),n(" 上可以看到详细的介绍。总结来说，部分显卡对于 bf16 以及 tf32 格式有很好的支持，相对于 fp16，有着更好的精度以及更快的速度。")])])]),x])}const w=o(c,[["render",C],["__file","笔记hf_trainer.html.vue"]]);export{w as default};
