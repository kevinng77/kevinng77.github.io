const t=JSON.parse('{"key":"v-4a15ca9d","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0rope_expanding.html","title":"论文笔记 | 探索 LLM 的上下文长度外推","lang":"zh-CN","frontmatter":{"title":"论文笔记 | 探索 LLM 的上下文长度外推","date":"2023-12-25T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC","LLM"],"description":"大模型上下文在前段时间有点火，TODO 里堆积的论文也越来越多（。 本文记录了 LLM 在长度外推方面的文章笔记，包括位置编码相关的 ALiBi 、 ROPE 和 线性插值（PI） ， NTK ；注意力相关的 GQA , SWA ， LM-INFINITE ， StreamingLLM ；以及 meta 的综述 Effective Long-Context Scaling of Foundation Models 记录。","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0rope_expanding.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"论文笔记 | 探索 LLM 的上下文长度外推"}],["meta",{"property":"og:description","content":"大模型上下文在前段时间有点火，TODO 里堆积的论文也越来越多（。 本文记录了 LLM 在长度外推方面的文章笔记，包括位置编码相关的 ALiBi 、 ROPE 和 线性插值（PI） ， NTK ；注意力相关的 GQA , SWA ， LM-INFINITE ， StreamingLLM ；以及 meta 的综述 Effective Long-Context Scaling of Foundation Models 记录。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-12-26T15:42:10.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:published_time","content":"2023-12-25T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-12-26T15:42:10.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"论文笔记 | 探索 LLM 的上下文长度外推\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-12-25T00:00:00.000Z\\",\\"dateModified\\":\\"2023-12-26T15:42:10.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":3,"title":"位置编码","slug":"位置编码","link":"#位置编码","children":[]},{"level":3,"title":"Transformer 注意力的优化","slug":"transformer-注意力的优化","link":"#transformer-注意力的优化","children":[]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1703605330000,"updatedTime":1703605330000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":8.04,"words":2412},"filePathRelative":"posts/notes/articles/笔记rope_expanding.md","localizedDate":"2023年12月25日","excerpt":"<p>大模型上下文在前段时间有点火，TODO 里堆积的论文也越来越多（。</p>\\n<p>本文记录了 LLM 在长度外推方面的文章笔记，包括位置编码相关的  <strong>ALiBi</strong> 、 <strong>ROPE</strong> 和 <strong>线性插值（PI）</strong> ，  <strong>NTK</strong> ；注意力相关的  <strong>GQA</strong> ,  <strong>SWA</strong> ， <strong>LM-INFINITE</strong> ， <strong>StreamingLLM</strong>  ；以及 meta 的综述 Effective Long-Context Scaling of Foundation Models 记录。</p>","autoDesc":true}');export{t as data};
