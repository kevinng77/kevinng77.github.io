const e=JSON.parse('{"key":"v-6e610ac0","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0gat.html","title":"图注意力网络 GAT","lang":"zh-CN","frontmatter":{"title":"图注意力网络 GAT","date":"2021-08-23T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["图网络"],"mathjax":true,"toc":true,"comments":"笔记","description":"GCN 的提出打开了网络算法的新世界，但仍有部分局限性。传统注意力机制包括 Attention for image Captioning 和 for Machine Translation。但是传统的注意力目前较少人使用。近期的 seq2seq, transformers 使用的自注意力越来越多人使用。GAT 加入自注意力机制，大大提高了各方面的效果，但其计算资源消耗也大幅度的提高了。在实际应用中，基本上当 GCN 准确率在 70%、80%遇到瓶颈的时候，如果硬件有条件，可以考虑使用 GAT。","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0gat.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"图注意力网络 GAT"}],["meta",{"property":"og:description","content":"GCN 的提出打开了网络算法的新世界，但仍有部分局限性。传统注意力机制包括 Attention for image Captioning 和 for Machine Translation。但是传统的注意力目前较少人使用。近期的 seq2seq, transformers 使用的自注意力越来越多人使用。GAT 加入自注意力机制，大大提高了各方面的效果，但其计算资源消耗也大幅度的提高了。在实际应用中，基本上当 GCN 准确率在 70%、80%遇到瓶颈的时候，如果硬件有条件，可以考虑使用 GAT。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-02-16T10:09:39.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"图网络"}],["meta",{"property":"article:published_time","content":"2021-08-23T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-02-16T10:09:39.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"图注意力网络 GAT\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2021-08-23T00:00:00.000Z\\",\\"dateModified\\":\\"2023-02-16T10:09:39.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"GAT 架构","slug":"gat-架构","link":"#gat-架构","children":[{"level":3,"title":"测试与结果","slug":"测试与结果","link":"#测试与结果","children":[]},{"level":3,"title":"可视化","slug":"可视化","link":"#可视化","children":[]}]},{"level":2,"title":"相关代码","slug":"相关代码","link":"#相关代码","children":[]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1676542179000,"updatedTime":1676542179000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":4.31,"words":1293},"filePathRelative":"posts/notes/articles/笔记gat.md","localizedDate":"2021年8月23日","excerpt":"<blockquote>\\n<p><a href=\\"http://wujiawen.xyz/2021/08/18/gcn/#more\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">GCN</a> 的提出打开了网络算法的新世界，但仍有部分局限性。传统注意力机制包括 Attention for image Captioning 和 for Machine Translation。但是传统的注意力目前较少人使用。近期的 seq2seq, transformers 使用的自注意力越来越多人使用。GAT 加入自注意力机制，大大提高了各方面的效果，但其计算资源消耗也大幅度的提高了。在实际应用中，基本上当 GCN 准确率在 70%、80%遇到瓶颈的时候，如果硬件有条件，可以考虑使用 GAT。</p>\\n</blockquote>","autoDesc":true}');export{e as data};
