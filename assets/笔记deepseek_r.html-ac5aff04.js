const e=JSON.parse('{"key":"v-7558a341","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0deepseek_r.html","title":"Deepseek 相关模型整理","lang":"zh-CN","frontmatter":{"title":"Deepseek 相关模型整理","date":"2025-05-29T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["AIGC","LLM"],"description":"DeepSeek-MoE 相关资源：github， 论文 DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0deepseek_r.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"Deepseek 相关模型整理"}],["meta",{"property":"og:description","content":"DeepSeek-MoE 相关资源：github， 论文 DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-16T10:19:36.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:published_time","content":"2025-05-29T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-16T10:19:36.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Deepseek 相关模型整理\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-05-29T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-16T10:19:36.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"DeepSeek-MoE","slug":"deepseek-moe","link":"#deepseek-moe","children":[]},{"level":2,"title":"DeepSeek-V2","slug":"deepseek-v2","link":"#deepseek-v2","children":[]},{"level":2,"title":"Deepseek V3","slug":"deepseek-v3","link":"#deepseek-v3","children":[{"level":3,"title":"架构","slug":"架构","link":"#架构","children":[]},{"level":3,"title":"训练策略","slug":"训练策略","link":"#训练策略","children":[]},{"level":3,"title":"预训练","slug":"预训练","link":"#预训练","children":[]},{"level":3,"title":"Long Context Extension","slug":"long-context-extension","link":"#long-context-extension","children":[]},{"level":3,"title":"后训练","slug":"后训练","link":"#后训练","children":[]},{"level":3,"title":"探索","slug":"探索","link":"#探索","children":[]}]},{"level":2,"title":"Deepseek R1","slug":"deepseek-r1","link":"#deepseek-r1","children":[]},{"level":2,"title":"OpenAI o1","slug":"openai-o1","link":"#openai-o1","children":[]},{"level":2,"title":"DeepSeek 系列","slug":"deepseek-系列","link":"#deepseek-系列","children":[]}],"git":{"createdTime":1752661176000,"updatedTime":1752661176000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":15.33,"words":4600},"filePathRelative":"posts/notes/articles/笔记deepseek_r.md","localizedDate":"2025年5月29日","excerpt":"<h2> DeepSeek-MoE</h2>\\n<p>相关资源：<a href=\\"https://github.com/deepseek-ai/DeepSeek-MoE\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">github</a>， <a href=\\"https://arxiv.org/pdf/2401.06066\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">论文 DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</a></p>","autoDesc":true}');export{e as data};
