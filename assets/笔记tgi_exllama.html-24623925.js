const e=JSON.parse('{"key":"v-9c1ec8fe","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0tgi_exllama.html","title":"TGI + exllama llama 量化部署方案","lang":"zh-CN","frontmatter":{"title":"TGI + exllama llama 量化部署方案","date":"2023-07-29T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC"],"description":"本文对 Text generation inference + exllama 的 LLaMa 量化服务方案进行单卡 4090 部署测试。 上期内容：vllm vs TGI 部署 llama v2 7B 踩坑笔记 在上期中我们提到了 TGI 和 vllm 的对比测试，在使用 vllm 和 TGI 对 float16 模型进行部署后，我们能够在单卡 4090 上达到 3.5+ request/秒的吞吐量。","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0tgi_exllama.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"TGI + exllama llama 量化部署方案"}],["meta",{"property":"og:description","content":"本文对 Text generation inference + exllama 的 LLaMa 量化服务方案进行单卡 4090 部署测试。 上期内容：vllm vs TGI 部署 llama v2 7B 踩坑笔记 在上期中我们提到了 TGI 和 vllm 的对比测试，在使用 vllm 和 TGI 对 float16 模型进行部署后，我们能够在单卡 4090 上达到 3.5+ request/秒的吞吐量。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-08-17T09:00:07.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:published_time","content":"2023-07-29T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-08-17T09:00:07.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"TGI + exllama llama 量化部署方案\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-07-29T00:00:00.000Z\\",\\"dateModified\\":\\"2023-08-17T09:00:07.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"TGI + EXLLAMA 测试","slug":"tgi-exllama-测试","link":"#tgi-exllama-测试","children":[]}],"git":{"createdTime":1692262807000,"updatedTime":1692262807000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":2.7,"words":809},"filePathRelative":"posts/notes/articles/笔记tgi_exllama.md","localizedDate":"2023年7月29日","excerpt":"<p>本文对 Text generation inference + exllama 的 LLaMa 量化服务方案进行单卡 4090 部署测试。</p>\\n<p>上期内容：<a href=\\"https://zhuanlan.zhihu.com/p/645732302\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">vllm vs TGI 部署 llama v2 7B 踩坑笔记</a></p>\\n<p>在上期中我们提到了 TGI 和 vllm 的对比测试，在使用 vllm 和 TGI 对 float16 模型进行部署后，我们能够在单卡 4090 上达到 3.5+ request/秒的吞吐量。</p>","autoDesc":true}');export{e as data};
