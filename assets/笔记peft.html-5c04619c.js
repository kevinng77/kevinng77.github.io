const e=JSON.parse('{"key":"v-795b8e56","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0peft.html","title":"LLM 高效训练方案整理","lang":"zh-CN","frontmatter":{"title":"LLM 高效训练方案整理","date":"2023-04-25T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC"],"description":"LLM 高效训练方案整理 本文基于 Huggingface PEFT，回顾整理常见的 LLM 高效训练方式，包括 prefix-tuning, p-tuning, lora, prompt tuning。对于 PEFT 中的模型，如 PeftModelForSequenceClassification。可以分为以下四种方式进行讨论： Prefix-Tuning (P-Tuning v2) 论文：Prefix-tuning- Optimizing continuous prompts for generation","head":[["meta",{"property":"og:url","content":"http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0peft.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"LLM 高效训练方案整理"}],["meta",{"property":"og:description","content":"LLM 高效训练方案整理 本文基于 Huggingface PEFT，回顾整理常见的 LLM 高效训练方式，包括 prefix-tuning, p-tuning, lora, prompt tuning。对于 PEFT 中的模型，如 PeftModelForSequenceClassification。可以分为以下四种方式进行讨论： Prefix-Tuning (P-Tuning v2) 论文：Prefix-tuning- Optimizing continuous prompts for generation"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:published_time","content":"2023-04-25T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM 高效训练方案整理\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-04-25T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"Prefix-Tuning (P-Tuning v2)","slug":"prefix-tuning-p-tuning-v2","link":"#prefix-tuning-p-tuning-v2","children":[{"level":3,"title":"Prefix-tuning 的直觉","slug":"prefix-tuning-的直觉","link":"#prefix-tuning-的直觉","children":[]},{"level":3,"title":"参考 PEFT 实现","slug":"参考-peft-实现","link":"#参考-peft-实现","children":[]},{"level":3,"title":"PrefixEncoder","slug":"prefixencoder","link":"#prefixencoder","children":[]}]},{"level":2,"title":"Prompt tuning","slug":"prompt-tuning","link":"#prompt-tuning","children":[]},{"level":2,"title":"P-Tuning","slug":"p-tuning","link":"#p-tuning","children":[]},{"level":2,"title":"Lora","slug":"lora","link":"#lora","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":6.41,"words":1922},"filePathRelative":"posts/notes/articles/笔记peft.md","localizedDate":"2023年4月25日","excerpt":"<h1> LLM 高效训练方案整理</h1>\\n<p>本文基于 Huggingface PEFT，回顾整理常见的 LLM 高效训练方式，包括 prefix-tuning, p-tuning, lora, prompt tuning。对于 PEFT 中的模型，如 <code>PeftModelForSequenceClassification</code>。可以分为以下四种方式进行讨论：</p>\\n<h2> Prefix-Tuning (P-Tuning v2)</h2>\\n<p>论文：Prefix-tuning- Optimizing continuous prompts for generation</p>","autoDesc":true}');export{e as data};
