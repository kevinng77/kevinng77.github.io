const t=JSON.parse('{"key":"v-f612a608","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0llama_note.html","title":"LLaMa 零散笔记","lang":"zh-CN","frontmatter":{"title":"LLaMa 零散笔记","date":"2023-08-05T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC"],"description":"llama note LLaMa 系列模型 baichuan，qwen 等与 llama 架构类似。不同点在于中文的这几个模型对此表进行了扩充，预训练方式不同，instruction tuning prompt template 不同，baichuan，qwen 分别采用 w_proj 和 c_proj 来代替 hf llama 官方的 k,q,v_proj。因此除了 lora 训练时需要映射一下位置，GPTQ 也需要做一下调整。 lora 有些人直接给设置成对 q_proj, k_proj, v_proj, W_proj 等等一系列不同模型采用的权重名称，似乎也不是不行。","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0llama_note.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"LLaMa 零散笔记"}],["meta",{"property":"og:description","content":"llama note LLaMa 系列模型 baichuan，qwen 等与 llama 架构类似。不同点在于中文的这几个模型对此表进行了扩充，预训练方式不同，instruction tuning prompt template 不同，baichuan，qwen 分别采用 w_proj 和 c_proj 来代替 hf llama 官方的 k,q,v_proj。因此除了 lora 训练时需要映射一下位置，GPTQ 也需要做一下调整。 lora 有些人直接给设置成对 q_proj, k_proj, v_proj, W_proj 等等一系列不同模型采用的权重名称，似乎也不是不行。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-08-17T09:00:07.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:published_time","content":"2023-08-05T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-08-17T09:00:07.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLaMa 零散笔记\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-08-05T00:00:00.000Z\\",\\"dateModified\\":\\"2023-08-17T09:00:07.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"llama note","slug":"llama-note","link":"#llama-note","children":[{"level":3,"title":"LLaMa 系列模型","slug":"llama-系列模型","link":"#llama-系列模型","children":[]},{"level":3,"title":"SwiGLU","slug":"swiglu","link":"#swiglu","children":[]},{"level":3,"title":"训练","slug":"训练","link":"#训练","children":[]}]}],"git":{"createdTime":1692262807000,"updatedTime":1692262807000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":1}]},"readingTime":{"minutes":1.73,"words":518},"filePathRelative":"posts/notes/articles/笔记llama_note.md","localizedDate":"2023年8月5日","excerpt":"<h2> llama note</h2>\\n<h3> LLaMa 系列模型</h3>\\n<p>baichuan，qwen 等与 llama 架构类似。不同点在于中文的这几个模型对此表进行了扩充，预训练方式不同，instruction tuning prompt template 不同，baichuan，qwen 分别采用 <code>w_proj</code> 和 <code>c_proj</code> 来代替 hf llama 官方的 <code>k,q,v_proj</code>。因此除了 lora 训练时需要映射一下位置，GPTQ 也需要做一下调整。</p>\\n<p>lora 有些人直接给设置成对 <code>q_proj, k_proj, v_proj, W_proj</code> 等等一系列不同模型采用的权重名称，似乎也不是不行。</p>","autoDesc":true}');export{t as data};
