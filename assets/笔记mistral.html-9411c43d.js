import{_ as p}from"./plugin-vue_export-helper-c27b6911.js";import{r as o,o as t,c as i,a as s,b as n,d as l,f as e}from"./app-a26e9423.js";const r="/assets/img/mistral/image-20240805103929202.jpg",c="/assets/img/mistral/image-20240805103944726.jpg",d="/assets/img/mistral/image-20240805103956240.jpg",y="/assets/img/mistral/image-20240805104006798.jpg",m="/assets/img/mistral/image-20240805104016338.jpg",u="/assets/img/mistral/image-20240723203836298.jpg",g="/assets/img/mistral/image-20240805104032194.jpg",h="/assets/img/mistral/image-20240728094958591.jpg",_="/assets/img/mistral/image-20240727152529176.jpg",E="/assets/img/mistral/image-20240727152702974.jpg",v="/assets/img/mistral/image-20240727154936831.jpg",f="/assets/img/mistral/image-20240805104145922.jpg",b="/assets/img/mistral/image-20240727201347583.jpg",C="/assets/img/mistral/image-20240805104200493.jpg",A="/assets/img/mistral/image-20240805104212135.jpg",x={},D=s("p",null,"在本文中，我们梳理了 24 年 7 月前 Mistral 系列模型的关键信息，包括它们的主要特点、亮点以及相关资源链接。涉及模型 Mistral 7B， Mixtral 8x7B，Mixtral 8x22B，Mistral Nemo, Mistral Large 2",-1),k=s("h2",{id:"mistral-7b",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#mistral-7b","aria-hidden":"true"},"#"),n(" mistral 7B")],-1),M={href:"https://mistral.ai/news/announcing-mistral-7b/",target:"_blank",rel:"noopener noreferrer"},w={href:"https://arxiv.org/abs/2310.06825",target:"_blank",rel:"noopener noreferrer"},z=s("p",null,"Mistral 7B 模型的亮点包括：",-1),F=s("ul",null,[s("li",null,[s("strong",null,"Sliding Window Attention")])],-1),B=s("strong",null,"理论上",-1),T={href:"https://huggingface.co/mistralai/mathstral-7B-v0.1/blob/main/config.json",target:"_blank",rel:"noopener noreferrer"},N=s("code",null,"max_position_embeddings",-1),L={href:"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/config.json",target:"_blank",rel:"noopener noreferrer"},G=e('<figure><img src="'+r+`" alt="image-20240805103929202" tabindex="0" loading="lazy"><figcaption>image-20240805103929202</figcaption></figure><p>由于代用了固定的 attention 窗口大小，因此我们只需要一个大小为 <code>W=window size</code> 的 cache ，在计算第 i 个 token 的 cache 的时候，只需要覆盖 cache 中 <code>i mod M</code> 位置上的 hidden state 即可。</p><p>参考 huggingface 的 mistral 实现，Sliding window attention 通过 attention_mask 来控制：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># huggignface mistral attn mask 实现</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">_update_causal_mask</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">        self,</span></span>
<span class="line"><span style="color:#24292E;">        attention_mask: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        input_tensor: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        cache_position: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        past_key_values: Cache,</span></span>
<span class="line"><span style="color:#24292E;">    ):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># ... 省略部分无关代码</span></span>
<span class="line"><span style="color:#24292E;">        past_seen_tokens </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> cache_position[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> past_key_values </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">else</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span></span>
<span class="line"><span style="color:#24292E;">        using_static_cache </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(past_key_values, StaticCache)</span></span>
<span class="line"><span style="color:#24292E;">        using_sliding_window_cache </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(past_key_values, SlidingWindowCache)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        dtype, device </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> input_tensor.dtype, input_tensor.device</span></span>
<span class="line"><span style="color:#24292E;">        min_dtype </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.finfo(dtype).min</span></span>
<span class="line"><span style="color:#24292E;">        sequence_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> input_tensor.shape[</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># SlidingWindowCache</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> using_sliding_window_cache:</span></span>
<span class="line"><span style="color:#24292E;">            target_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">max</span><span style="color:#24292E;">(sequence_length, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># StaticCache</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">elif</span><span style="color:#24292E;"> using_static_cache:</span></span>
<span class="line"><span style="color:#24292E;">            target_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> past_key_values.get_max_length()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># DynamicCache or no cache</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            target_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> (</span></span>
<span class="line"><span style="color:#24292E;">                attention_mask.shape[</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(attention_mask, torch.Tensor)</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">else</span><span style="color:#24292E;"> past_seen_tokens </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> sequence_length </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span></span>
<span class="line"><span style="color:#24292E;">            )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">and</span><span style="color:#24292E;"> attention_mask.dim() </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># in this case we assume that the mask comes already in inverted form and requires no inversion or slicing</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask.max() </span><span style="color:#D73A49;">!=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">raise</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">ValueError</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;Custom 4D attention mask should be passed in inverted form with max==0\`&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> attention_mask</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.full(</span></span>
<span class="line"><span style="color:#24292E;">                (sequence_length, target_length), </span><span style="color:#E36209;">fill_value</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">min_dtype, </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">dtype, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">device</span></span>
<span class="line"><span style="color:#24292E;">            )</span></span>
<span class="line"><span style="color:#24292E;">            exclude_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.arange(target_length, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">device) </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> cache_position.reshape(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> using_sliding_window_cache </span><span style="color:#D73A49;">or</span><span style="color:#24292E;"> sequence_length </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window:</span></span>
<span class="line"><span style="color:#24292E;">                    exclude_mask.bitwise_or_(</span></span>
<span class="line"><span style="color:#24292E;">                        torch.arange(target_length, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">device)</span></span>
<span class="line"><span style="color:#24292E;">                        </span><span style="color:#D73A49;">&lt;=</span><span style="color:#24292E;"> (cache_position.reshape(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">) </span><span style="color:#D73A49;">-</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window)</span></span>
<span class="line"><span style="color:#24292E;">                    )</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">*=</span><span style="color:#24292E;"> exclude_mask</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask[</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, :, :].expand(input_tensor.shape[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask.clone()  </span><span style="color:#6A737D;"># copy to contiguous memory for in-place edit</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask.dim() </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                    mask_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> attention_mask.shape[</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">                    padding_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask[:, :, :, :mask_length] </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> attention_mask[:, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, :]</span></span>
<span class="line"><span style="color:#24292E;">                    padding_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> padding_mask </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span></span>
<span class="line"><span style="color:#24292E;">                    causal_mask[:, :, :, :mask_length] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask[:, :, :, :mask_length].masked_fill(</span></span>
<span class="line"><span style="color:#24292E;">                        padding_mask, min_dtype</span></span>
<span class="line"><span style="color:#24292E;">                    )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> causal_mask</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>GQA (Grouped Query Attention)</strong></li></ul>`,5),S={href:"https://arxiv.org/abs/2305.13245",target:"_blank",rel:"noopener noreferrer"},I=s("figure",null,[s("img",{src:c,alt:"image-20240805103944726",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20240805103944726")],-1),j={href:"https://arxiv.org/pdf/1911.02150.pdf",target:"_blank",rel:"noopener noreferrer"},q=s("p",null,"以下为 GQA 文中的实验结果，值得注意的是论文中使用原 MHA checkpoint 转换为 GQA 权重后，还进行了额外的预训练：",-1),Q=s("figure",null,[s("img",{src:d,alt:"image-20240805103956240",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20240805103956240")],-1),P=s("p",null,"此外 Mistral，Llama2 的部分模型使用 GQA 时，采用的 kv head 数量似乎都是 8。",-1),V={href:"https://zhuanlan.zhihu.com/p/647130255",target:"_blank",rel:"noopener noreferrer"},K=s("h2",{id:"mixtral-8-7b",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#mixtral-8-7b","aria-hidden":"true"},"#"),n(" Mixtral 8*7B")],-1),O={href:"https://arxiv.org/abs/2401.04088",target:"_blank",rel:"noopener noreferrer"},W={href:"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1",target:"_blank",rel:"noopener noreferrer"},H={href:"https://mistral.ai/news/mixtral-of-experts/",target:"_blank",rel:"noopener noreferrer"},R={href:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mixtral/modeling_mixtral.py",target:"_blank",rel:"noopener noreferrer"},X={href:"https://huggingface.co/blog/zh/moe",target:"_blank",rel:"noopener noreferrer"},U=e('<ul><li><p>发布时间：23 年 12 月</p></li><li><p>模型大小：8 个 expert MLP 层，一共 45B 大小。</p></li><li><p>训练：除了预训练外，Mixtral MOE 后续还开源了一个经过 SFT + DPO 微调的版本。</p></li><li><p>模型效果：</p></li></ul><figure><img src="'+y+'" alt="image-20240805104006798" tabindex="0" loading="lazy"><figcaption>image-20240805104006798</figcaption></figure><ul><li>架构：Mixtral 的 MOE 架构类似于，在 MoE 模型中，只有 FFN 层被视为独立的专家，而模型的其他参数是共享的。大致参数为：</li></ul><figure><img src="'+m+'" alt="image-20240805104016338" tabindex="0" loading="lazy"><figcaption>image-20240805104016338</figcaption></figure>',4),$={href:"https://huggingface.co/blog/zh/moe",target:"_blank",rel:"noopener noreferrer"},J=s("p",null,[n("参考 huggingface 中的 mixtral 和 mistral 实现对比，差异在于 mixtral 中将传统 transformer decoder layer 中的 FFN 替换为了 "),s("code",null,"block_sparse_moe"),n("。")],-1),Y=s("figure",null,[s("img",{src:u,alt:"https://github.com/open-compass/MixtralKit",tabindex:"0",loading:"lazy"}),s("figcaption",null,"https://github.com/open-compass/MixtralKit")],-1),Z=s("p",null,"主要逻辑为：",-1),ss=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"G"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mtext",null,"Softmax"),s("mo",{stretchy:"false"},"("),s("mi",null,"T"),s("mi",null,"o"),s("mi",null,"p"),s("mi",null,"K"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{separator:"true"},"⋅"),s("msub",null,[s("mi",null,"W"),s("mrow",null,[s("mi",null,"g"),s("mi",null,"a"),s("mi",null,"t"),s("mi",null,"e")])]),s("mo",{stretchy:"false"},")"),s("mo",{stretchy:"false"},")"),s("mspace",{linebreak:"newline"}),s("mtext",null,"final hidden states"),s("mo",null,"="),s("munderover",null,[s("mo",null,"∑"),s("mrow",null,[s("mi",null,"i"),s("mo",null,"="),s("mn",null,"0")]),s("mrow",null,[s("mi",null,"n"),s("mo",null,"−"),s("mn",null,"1")])]),s("mi",null,"G"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("msub",null,[s("mo",{stretchy:"false"},")"),s("mi",null,"i")]),s("mo",{separator:"true"},"⋅"),s("msub",null,[s("mi",null,"E"),s("mi",null,"i")]),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"}," G(x) = \\text{Softmax}(TopK(x · W_{gate}))\\\\ \\text{final hidden states} = \\sum^{n-1}_{i=0} G(x)_i·E_i(x) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"G"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0361em","vertical-align":"-0.2861em"}}),s("span",{class:"mord text"},[s("span",{class:"mord"},"Softmax")]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"T"),s("span",{class:"mord mathnormal"},"o"),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mpunct"},"⋅"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"g"),s("span",{class:"mord mathnormal mtight"},"a"),s("span",{class:"mord mathnormal mtight"},"t"),s("span",{class:"mord mathnormal mtight"},"e")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mclose"},"))")]),s("span",{class:"mspace newline"}),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord text"},[s("span",{class:"mord"},"final hidden states")]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3.0788em","vertical-align":"-1.2777em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.8011em"}},[s("span",{style:{top:"-1.8723em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"0")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])]),s("span",{style:{top:"-4.3em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"n"),s("span",{class:"mbin mtight"},"−"),s("span",{class:"mord mtight"},"1")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.2777em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"G"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},[s("span",{class:"mclose"},")"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},"⋅"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"E"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},")")])])])])],-1),ns=s("p",null,[n("其中 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"E"),s("mi",null,"i")]),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"E_i(x)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"E"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},")")])])]),n(" 为专家对应的网络，具体展示为下面 huggingface 实现中的 "),s("code",null,"MixtralBlockSparseTop2MLP"),n("。mixtral 中采用了 8 个 expert，每次推理使用选取 top 2 的 expert 进行推理。比如输入一句话 "),s("code",null,"你好，今天"),n("，那么我们每个 token 都会选出 top 2 的 expert 来负责这个 token 的预测，因此在推理 "),s("code",null,"你好，今天"),n(" 时， "),s("strong",null,"有概率所有 expert 都会参与到计算当中"),n(" ，具体可以参考 "),s("code",null,"MixtralSparseMoeBlock"),n(" 的实现。")],-1),as=e('<figure><img src="'+g+`" alt="image-20240805104032194" tabindex="0" loading="lazy"><figcaption>image-20240805104032194</figcaption></figure><p>mixtral 论文中提到专家分配在不同主题（如 ArXiv 论文、生物学和哲学文档）中没有明显的模式，只有在 DM 数学中显示出边际上的差异，这可能是由于其数据集的合成性质和有限的自然语言覆盖范围所致。router 在某些句法结构上表现出一定的结构化行为（比如 python 的 self 等），同时连续标记通常被分配给相同的专家。</p><ul><li>huggingface 中的 mixtral 核心代码：</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">MixtralDecoderLayer</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, config: MixtralConfig, layer_idx: </span><span style="color:#005CC5;">int</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_size </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.hidden_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.self_attn </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">MIXTRAL_ATTENTION_CLASSES</span><span style="color:#24292E;">[config._attn_implementation](config, layer_idx)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.block_sparse_moe </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MixtralSparseMoeBlock(config)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.input_layernorm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MixtralRMSNorm(config.hidden_size, </span><span style="color:#E36209;">eps</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">config.rms_norm_eps)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.post_attention_layernorm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MixtralRMSNorm(config.hidden_size, </span><span style="color:#E36209;">eps</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">config.rms_norm_eps)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        attention_mask: Optional[torch.Tensor] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 此处省略参数 ..</span></span>
<span class="line"><span style="color:#24292E;">    ) -&gt; Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        residual </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.input_layernorm(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states, self_attn_weights, present_key_value </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.self_attn(</span></span>
<span class="line"><span style="color:#24292E;">        	</span><span style="color:#6A737D;"># 此处省略参数 </span></span>
<span class="line"><span style="color:#24292E;">        )</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> residual </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        residual </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.post_attention_layernorm(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># Mixtral 将原本的 hidden_states = self.FFN(hidden_states) 替换为了：</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states, router_logits </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.block_sparse_moe(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> residual </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        outputs </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> (hidden_states,)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> outputs</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>huggingface 中 <code>block_sparse_moe</code> 的实现（省略部分次要代码）：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">MixtralSparseMoeBlock</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, config):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.hidden_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.intermediate_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.num_local_experts</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.top_k </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.num_experts_per_tok</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.gate </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.experts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.ModuleList([MixtralBlockSparseTop2MLP(config) </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> _ </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts)])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.jitter_noise </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.router_jitter_noise</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:</span></span>
<span class="line"><span style="color:#24292E;">        batch_size, sequence_length, hidden_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states.shape</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states.view(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">        router_logits </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.gate(hidden_states)  </span><span style="color:#6A737D;"># (batch * sequence_length, n_experts)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        routing_weights </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> F.softmax(router_logits, </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">torch.float)</span></span>
<span class="line"><span style="color:#24292E;">        routing_weights, selected_experts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.topk(routing_weights, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.top_k, </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        routing_weights </span><span style="color:#D73A49;">/=</span><span style="color:#24292E;"> routing_weights.sum(</span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#E36209;">keepdim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># we cast back to the input dtype</span></span>
<span class="line"><span style="color:#24292E;">        routing_weights </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> routing_weights.to(hidden_states.dtype)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        final_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.zeros(</span></span>
<span class="line"><span style="color:#24292E;">            (batch_size </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> sequence_length, hidden_dim), </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">hidden_states.dtype, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">hidden_states.device</span></span>
<span class="line"><span style="color:#24292E;">        )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># One hot encode the selected experts to create an expert mask</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># this will be used to easily index which expert is going to be sollicitated</span></span>
<span class="line"><span style="color:#24292E;">        expert_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.nn.functional.one_hot(selected_experts, </span><span style="color:#E36209;">num_classes</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts).permute(</span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># Loop over all available experts in the model and perform the computation on each expert</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> expert_idx </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts):</span></span>
<span class="line"><span style="color:#24292E;">            expert_layer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.experts[expert_idx]</span></span>
<span class="line"><span style="color:#24292E;">            idx, top_x </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.where(expert_mask[expert_idx])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># Index the correct hidden states and compute the expert hidden state for</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># the current expert. We need to make sure to multiply the output hidden</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># states by \`routing_weights\` on the corresponding tokens (top-1 and top-2)</span></span>
<span class="line"><span style="color:#24292E;">            </span></span>
<span class="line"><span style="color:#24292E;">            current_state </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states[</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, top_x].reshape(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># current_state: shape (n_i, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 所有 current_state 的长度 n 总和为 batch * sequence_length</span></span>
<span class="line"><span style="color:#24292E;">            current_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> expert_layer(current_state) </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> routing_weights[top_x, idx, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># However \`index_add_\` only support torch tensors for indexing so we&#39;ll use</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># the \`top_x\` tensor here.</span></span>
<span class="line"><span style="color:#24292E;">            final_hidden_states.index_add_(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, top_x, current_hidden_states.to(hidden_states.dtype))</span></span>
<span class="line"><span style="color:#24292E;">        final_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> final_hidden_states, router_logits</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中： <code>MixtralBlockSparseTop2MLP</code> 长这样：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">MixtralBlockSparseTop2MLP</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, config: MixtralConfig):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.intermediate_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.hidden_size</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w1 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w3 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.act_fn </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">ACT2FN</span><span style="color:#24292E;">[config.hidden_act]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, hidden_states):</span></span>
<span class="line"><span style="color:#24292E;">        current_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.act_fn(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w1(hidden_states)) </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w3(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        current_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w2(current_hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> current_hidden_states</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="推理" tabindex="-1"><a class="header-anchor" href="#推理" aria-hidden="true">#</a> 推理</h4>`,9),ls={href:"https://www.reddit.com/r/LocalLLaMA/comments/18jslmf/tokens_per_second_mistral_8x7b_performance/",target:"_blank",rel:"noopener noreferrer"},es=e(`<table><thead><tr><th>推理精度</th><th>设备</th><th>速度 tokens/s</th></tr></thead><tbody><tr><td>Q4_K_M</td><td>单卡 4090 + <strong>7950X3D</strong></td><td>20</td></tr><tr><td>Q4_K_M</td><td>2 x 3090</td><td>48.26</td></tr></tbody></table><p>如果有 100+GB 以上显存，可以用 vllm 快速搭建测试 api：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">docker</span><span style="color:#24292E;"> </span><span style="color:#032F62;">run</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--gpus</span><span style="color:#24292E;"> </span><span style="color:#032F62;">all</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">-e</span><span style="color:#24292E;"> </span><span style="color:#032F62;">HF_TOKEN=</span><span style="color:#24292E;">$HF_TOKEN </span><span style="color:#005CC5;">-p</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">8000</span><span style="color:#032F62;">:8000</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">ghcr.io/mistralai/mistral-src/vllm:latest</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--host</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0.0</span><span style="color:#032F62;">.0.0</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--model</span><span style="color:#24292E;"> </span><span style="color:#032F62;">mistralai/Mixtral-8x7B-Instruct-v0.1</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--tensor-parallel-size</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;"> </span><span style="color:#6A737D;"># 100+GB 显存 \\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6F42C1;">--load-format</span><span style="color:#24292E;"> </span><span style="color:#032F62;">pt</span><span style="color:#24292E;"> </span><span style="color:#6A737D;"># needed since both \`pt\` and \`safetensors\` are available</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,3),ps={href:"https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm/?ncid=so-twit-928467/",target:"_blank",rel:"noopener noreferrer"},os=s("figure",null,[s("img",{src:h,alt:"image-20240728094958591",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20240728094958591")],-1),ts=s("p",null,"文中没有给出当 sequence lengths 最大时候的吞吐量，但根据上图数据，可以猜测 2 个 H100 部署 8*7B 正常服务用户时，平均吞吐量应该可以大于 7500Tokens/秒，根据 H100 的功耗计算电费成本的话，生成 1M token 需要耗约为 0.02 度电。",-1),is=s("h2",{id:"mixtral-8-22b",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#mixtral-8-22b","aria-hidden":"true"},"#"),n(" Mixtral 8*22B")],-1),rs={href:"https://mistral.ai/news/mixtral-8x22b/",target:"_blank",rel:"noopener noreferrer"},cs={href:"https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1",target:"_blank",rel:"noopener noreferrer"},ds=e('<ul><li>架构：架构与 mixtral 8*7B 架构一样，在 huggingface 中使用的都是<code>MixtralForCausalLM</code> ，但 22B 的各方面参数大一点，比较特别的是 context window 从 32k 升级到了 65k， <code>vocab_size</code> 也更大一些。</li><li>支持 function calling，不过好像没有透露具体的 function calling 训练细节。</li><li>数学和 coding 能力明显超越 llama2 70B</li><li>似乎对中文的支持不是很好。</li></ul><figure><img src="'+_+'" alt="image-20240727152529176" tabindex="0" loading="lazy"><figcaption>image-20240727152529176</figcaption></figure><p>Mistral 团队开源的模型，都比较注重 coding 和 math 的能力，Mixtral 系列的模型在这方便表现也是比较好：</p><figure><img src="'+E+'" alt="image-20240727152702974" tabindex="0" loading="lazy"><figcaption>image-20240727152702974</figcaption></figure><h2 id="mistral-nemo" tabindex="-1"><a class="header-anchor" href="#mistral-nemo" aria-hidden="true">#</a> Mistral Nemo</h2>',5),ys={href:"https://mistral.ai/news/mistral-nemo/",target:"_blank",rel:"noopener noreferrer"},ms={href:"https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407",target:"_blank",rel:"noopener noreferrer"},us=s("p",null,[n("Mistral Nemo 使用的也是 "),s("code",null,"MistralForCausalLM"),n(" 架构，与 mistral 7B 的差别为：Mistral Nemo 的 "),s("code",null,"hidden_size"),n(" 从 4096 变为 5120；"),s("code",null,"max_position_embeddings"),n(" 变为 1024000，"),s("code",null,"num_hidden_layers"),n(" 增加到 40， vocab_size 增加到 131072，不用 sliding window。")],-1),gs=s("ul",null,[s("li",null,"支持 function calling！"),s("li",null,"采用了 Tekken 作为 tokenizer，比 SentencePiece 更高效（压缩率更高，官方描述是~30% more efficient at compressing，不确定是哪个方面的 efficient）")],-1),hs={href:"https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/",target:"_blank",rel:"noopener noreferrer"},_s={href:"https://github.com/NVIDIA/Megatron-LM",target:"_blank",rel:"noopener noreferrer"},Es=s("p",null,"但光采用 FP16 加载整个 Mistral Nemo 就需要花 23 GB 显存，要是要跑满整个 context window size，除了量化外，还是得需要采用 offload 或者其他方法来推理",-1),vs=s("p",null,"不过 mistral 官方把 12 B 的模型和其他 8B 的模型对比，感觉好像不太公平：",-1),fs=s("figure",null,[s("img",{src:v,alt:"image-20240727154936831",tabindex:"0",loading:"lazy"}),s("figcaption",null,"image-20240727154936831")],-1),bs=s("h2",{id:"mistral-large-2",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#mistral-large-2","aria-hidden":"true"},"#"),n(),s("strong",null,"Mistral Large"),n(" 2")],-1),Cs={href:"https://mistral.ai/news/mistral-large-2407/",target:"_blank",rel:"noopener noreferrer"},As={href:"https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",target:"_blank",rel:"noopener noreferrer"},xs=e('<p>Mistral Large 2，参数量 123B，主打多语言以及 coding 能力。采用与 mistral 7B 一样的架构，huggingface 中同样使用 <code>MistralForCausalLM</code>；比较值得注意的是 context window size 为 131072，不用 sliding window。</p><p>Llama 3.1 刚出不久，就拿 <strong>Mistral Large</strong> 2 和别人来对比：</p><figure><img src="'+f+'" alt="image-20240805104145922" tabindex="0" loading="lazy"><figcaption>image-20240805104145922</figcaption></figure><p>在代码能力上，Mistral large 2 比 llama 3.1 平均效果更好。</p><figure><img src="'+b+'" alt="image-20240727201347583" tabindex="0" loading="lazy"><figcaption>image-20240727201347583</figcaption></figure><p>除了 coding 和数学外，在 MT Bench 的评分也比 llama 3.1 高，平均生成的回复长度比 llama 3.1 要短</p><figure><img src="'+C+'" alt="image-20240805104200493" tabindex="0" loading="lazy"><figcaption>image-20240805104200493</figcaption></figure><p>同时，中文能力相对上一代 mistral large 有大步幅提升：</p><figure><img src="'+A+'" alt="image-20240805104212135" tabindex="0" loading="lazy"><figcaption>image-20240805104212135</figcaption></figure>',9);function Ds(ks,Ms){const a=o("ExternalLinkIcon");return t(),i("div",null,[D,k,s("p",null,[s("a",M,[n("官方博客"),l(a)]),n(" ，"),s("a",w,[n("mistral 7B 论文"),l(a)])]),z,F,s("p",null,[n("Mistral 采用的 window size 为 4096，而后一共有 32 层 layer，那么采用 SWA 之后，理论上在进行 attention 的时候， "),B,n(" 可以收集到约 131K tokens 的信息。(虽然论文里提到的 window size 是 4096，但 官方提供的 "),s("a",T,[n("huggingface 上的权重"),l(a)]),n(" 中 "),N,n(" 为 32768，且在新一点的版本中，比如 "),s("a",L,[n("mistral-7b-instruct-v0.2"),l(a)]),n("，都不采用 sliding window 了)")]),G,s("p",null,[s("a",S,[n("GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"),l(a)])]),I,s("p",null,[n("grouped-query attention 指出，"),s("a",j,[n("Multi-Query Attention"),l(a)]),n(" 提高了推理速度的同时，却可能极大地降低回复质量。因此根据上图，GQA 在推理速度和质量之间作了权衡。")]),q,Q,P,s("blockquote",null,[s("p",null,[s("a",V,[n("为什么现在大家都在用 MQA 和 GQA？"),l(a)]),n(" 文中提到 MQA 和 GQA 能获得巨大加速的一个点在于：GPU 内存强的限制。由于 MQA 和 GQA 都降低了内存中数据的读取量，减少了计算单元的等待时间，因此推理速度的提高比想象中的要快更多。")])]),K,s("p",null,[s("a",O,[n("论文"),l(a)]),n("，"),s("a",W,[n("huggingface 模型权重"),l(a)]),n("， "),s("a",H,[n("官方博客"),l(a)]),n("，"),s("a",R,[n("huggingface 模型代码"),l(a)]),n("，"),s("a",X,[n("混合专家模型基础（推荐）"),l(a)]),n("，官方给出的评分来看，mixtral 8*7 和 GPT3.5 有的一比。")]),U,s("p",null,[n("对 moe 架构不太了解的朋友，可以参考这篇博客 "),s("a",$,[n("混合专家模型基础（推荐）"),l(a)]),n("。")]),J,Y,Z,ss,ns,as,s("p",null,[n("根据模型参数量 45B 来推理的话，如果用 fp16 的话推理的话，得需要至少 90GB 以上的显存，如果用 4 bit 的话，30GB 显存就够了。量化的生成速度，可以参考"),s("a",ls,[n("这个 redis"),l(a)]),n(" 中的评论，大致为 ：")]),es,s("p",null,[n("NVIDIA 的 "),s("a",ps,[n("TensorRT-LLM 博客"),l(a)]),n("中发出了对 Mixtral 8*7B 的吞吐量 benchmark （using input and output sequence lengths of 128）：")]),os,ts,is,s("p",null,[s("a",rs,[n("官方博客"),l(a)]),n("，"),s("a",cs,[n("huggingface 开源模型"),l(a)]),n("，")]),ds,s("p",null,[s("a",ys,[n("官方博客"),l(a)]),n("，"),s("a",ms,[n("huggingface 模型权重"),l(a)])]),us,gs,s("p",null,[n("NVIDIA 在"),s("a",hs,[n("这个博客"),l(a)]),n("中提到：Mistral Nemo 采用这样的设计，是为了能够适配单个 NVIDIA L40S、NVIDIA GeForce RTX 4090 或 NVIDIA RTX 4500 GPU。模型采用 "),s("a",_s,[n(" Megatron-LM"),l(a)]),n(" 训练，用了 3,072 个 H100 80GB 。")]),Es,vs,fs,bs,s("p",null,[s("a",Cs,[n("官方博客"),l(a)]),n("， "),s("a",As,[n("huggingface 模型权重"),l(a)])]),xs])}const Fs=p(x,[["render",Ds],["__file","笔记mistral.html.vue"]]);export{Fs as default};
