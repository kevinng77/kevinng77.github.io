const e=JSON.parse('{"key":"v-0d61d6ea","path":"/posts/notes/articles/%E7%AC%94%E8%AE%B0vllm_tgi.html","title":"vllm vs TGI 踩坑笔记","lang":"zh-CN","frontmatter":{"title":"vllm vs TGI 踩坑笔记","date":"2023-07-27T00:00:00.000Z","author":"Kevin 吴嘉文","category":["知识笔记"],"tag":["NLP","AIGC"],"description":"LLM 高并发部署是个难题，具备高吞吐量的服务，能够让用户有更好的体验（比如模型生成文字速度提升，用户排队时间缩短）。本文对 vllm 和 TGI 两个开源方案进行了实践测试，并整理了一些部署的坑。 测试环境：单卡 4090 + i9-13900K。限制于设备条件，本文仅对单卡部署 llama v2 7B 模型进行了测试。 小结： TGI (0.9.3) 优于 vllm (v0.1.2)。最新版本的 TGI 在加入了 PagedAttention 之后，吞吐量和 vllm 差不多。 vllm","head":[["meta",{"property":"og:url","content":"http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0vllm_tgi.html"}],["meta",{"property":"og:site_name","content":"记忆笔书"}],["meta",{"property":"og:title","content":"vllm vs TGI 踩坑笔记"}],["meta",{"property":"og:description","content":"LLM 高并发部署是个难题，具备高吞吐量的服务，能够让用户有更好的体验（比如模型生成文字速度提升，用户排队时间缩短）。本文对 vllm 和 TGI 两个开源方案进行了实践测试，并整理了一些部署的坑。 测试环境：单卡 4090 + i9-13900K。限制于设备条件，本文仅对单卡部署 llama v2 7B 模型进行了测试。 小结： TGI (0.9.3) 优于 vllm (v0.1.2)。最新版本的 TGI 在加入了 PagedAttention 之后，吞吐量和 vllm 差不多。 vllm"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-08-01T16:48:40.000Z"}],["meta",{"property":"article:author","content":"Kevin 吴嘉文"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"AIGC"}],["meta",{"property":"article:published_time","content":"2023-07-27T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-08-01T16:48:40.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"vllm vs TGI 踩坑笔记\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-07-27T00:00:00.000Z\\",\\"dateModified\\":\\"2023-08-01T16:48:40.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kevin 吴嘉文\\"}]}"]]},"headers":[{"level":2,"title":"vllm","slug":"vllm","link":"#vllm","children":[{"level":3,"title":"安装","slug":"安装","link":"#安装","children":[]},{"level":3,"title":"快速开始","slug":"快速开始","link":"#快速开始","children":[]},{"level":3,"title":"一些碎碎念","slug":"一些碎碎念","link":"#一些碎碎念","children":[]}]},{"level":2,"title":"Text Generation Inference","slug":"text-generation-inference","link":"#text-generation-inference","children":[{"level":3,"title":"安装","slug":"安装-1","link":"#安装-1","children":[]},{"level":3,"title":"快速开始","slug":"快速开始-1","link":"#快速开始-1","children":[]}]},{"level":2,"title":"Serving 测试","slug":"serving-测试","link":"#serving-测试","children":[{"level":3,"title":"vllm   benchmark   测试","slug":"vllm-benchmark-测试","link":"#vllm-benchmark-测试","children":[]},{"level":3,"title":"JMeter 模拟","slug":"jmeter-模拟","link":"#jmeter-模拟","children":[]}]},{"level":2,"title":"其他","slug":"其他","link":"#其他","children":[{"level":3,"title":"运行 gptq 测试","slug":"运行-gptq-测试","link":"#运行-gptq-测试","children":[]}]}],"git":{"createdTime":1690471680000,"updatedTime":1690908520000,"contributors":[{"name":"kevinng77","email":"417333277@qq.com","commits":2}]},"readingTime":{"minutes":10.39,"words":3117},"filePathRelative":"posts/notes/articles/笔记vllm_tgi.md","localizedDate":"2023年7月27日","excerpt":"<p>LLM 高并发部署是个难题，具备高吞吐量的服务，能够让用户有更好的体验（比如模型生成文字速度提升，用户排队时间缩短）。本文对 vllm 和 TGI 两个开源方案进行了实践测试，并整理了一些部署的坑。</p>\\n<p>测试环境：单卡 4090 + i9-13900K。限制于设备条件，本文仅对单卡部署 llama v2 7B 模型进行了测试。</p>\\n<p><strong>小结：</strong> TGI (0.9.3) 优于 vllm (v0.1.2)。最新版本的 TGI 在加入了 PagedAttention 之后，吞吐量和 vllm 差不多。</p>\\n<h2> <strong>vllm</strong></h2>","autoDesc":true}');export{e as data};
