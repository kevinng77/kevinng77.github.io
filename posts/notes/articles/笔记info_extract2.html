<!DOCTYPE html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.61" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="http://wujiawen.xyz/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html"><meta property="og:site_name" content="记忆笔书"><meta property="og:title" content="信息抽取论文小述（二）|实体抽取、关系抽取"><meta property="og:description" content="本文对 PURE, TPLINKER, GPLINKER, UIE 四篇文章做部分论文笔记与整理。 信息抽取论文小述|实体抽取、关系抽取 PURE A Frustratingly Easy Approach for Joint Entity and Relation Extraction 论文链接"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2023-03-26T07:48:04.000Z"><meta property="article:author" content="Kevin 吴嘉文"><meta property="article:tag" content="NLP"><meta property="article:published_time" content="2022-09-06T00:00:00.000Z"><meta property="article:modified_time" content="2023-03-26T07:48:04.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"信息抽取论文小述（二）|实体抽取、关系抽取","image":[""],"datePublished":"2022-09-06T00:00:00.000Z","dateModified":"2023-03-26T07:48:04.000Z","author":[{"@type":"Person","name":"Kevin 吴嘉文"}]}</script><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;700&display=swap" rel="stylesheet"><link rel="icon" href="/logo.svg"><title>信息抽取论文小述（二）|实体抽取、关系抽取 | 记忆笔书</title><meta name="description" content="本文对 PURE, TPLINKER, GPLINKER, UIE 四篇文章做部分论文笔记与整理。 信息抽取论文小述|实体抽取、关系抽取 PURE A Frustratingly Easy Approach for Joint Entity and Relation Extraction 论文链接">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-974b4d3c.css" as="style"><link rel="stylesheet" href="/assets/style-974b4d3c.css">
    <link rel="modulepreload" href="/assets/app-8b900482.js"><link rel="modulepreload" href="/assets/framework-6cee4965.js"><link rel="modulepreload" href="/assets/笔记info_extract2.html-3e5fb2f8.js"><link rel="modulepreload" href="/assets/笔记info_extract2.html-ffc35641.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container has-toc"><!--[--><header class="navbar" id="navbar"><div class="navbar-start"><button type="button" class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><a href="/" class="brand"><img class="logo" src="/logo.svg" alt="记忆笔书"><!----><span class="site-name hide-in-pad">记忆笔书</span></a><!--[--><!----><!--]--></div><div class="navbar-center"><!--[--><!----><!--]--><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/" class="nav-link" aria-label="主页"><span class="font-icon icon iconfont icon-home" style=""></span>主页<!----></a></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="博文"><span class="title"><span class="font-icon icon iconfont icon-blog" style=""></span>博文</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a href="/posts/notes/" class="nav-link active" aria-label="知识笔记"><span class="font-icon icon iconfont icon-note" style=""></span>知识笔记<!----></a></li><li class="dropdown-item"><a href="/posts/hometown/" class="nav-link" aria-label="泉州忆往昔"><span class="font-icon icon iconfont icon-like" style=""></span>泉州忆往昔<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><a href="/timeline/" class="nav-link" aria-label="归档"><span class="font-icon icon iconfont icon-categoryselected" style=""></span>归档<!----></a></div></nav><!--[--><!----><!--]--></div><div class="navbar-end"><!--[--><!----><!--]--><!----><div class="nav-item"><a class="repo-link" href="https://github.com/kevinng77" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!----><!--[--><button type="button" class="search-pro-button" role="search" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="placeholder">搜索</div><div class="key-hints"><kbd class="key">Ctrl</kbd><kbd class="key">K</kbd></div></button><!--]--><!--[--><!----><!--]--><button type="button" class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside class="sidebar" id="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"><li><!--[--><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#信息抽取论文小述-实体抽取、关系抽取" class="router-link-active router-link-exact-active nav-link sidebar-link sidebar-heading" aria-label="信息抽取论文小述|实体抽取、关系抽取"><!---->信息抽取论文小述|实体抽取、关系抽取<!----></a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#pure" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="PURE"><!---->PURE<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#tplinker" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="TPLinker"><!---->TPLinker<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#gplinker" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="GPLINKER"><!---->GPLINKER<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#uie" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="UIE"><!---->UIE<!----></a><ul class="sidebar-sub-headers"></ul></li></ul><!--]--></li></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->信息抽取论文小述（二）|实体抽取、关系抽取</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Kevin 吴嘉文</span></span><span property="author" content="Kevin 吴嘉文"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2022-09-06T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 9 分钟</span><meta property="timeRequired" content="PT9M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><span class="page-category-item category7 clickable" role="navigation">知识笔记</span><meta property="articleSection" content="知识笔记"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><span class="page-tag-item tag8 clickable" role="navigation">NLP</span><meta property="keywords" content="NLP"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#信息抽取论文小述-实体抽取、关系抽取" class="router-link-active router-link-exact-active toc-link level2">信息抽取论文小述|实体抽取、关系抽取</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#pure" class="router-link-active router-link-exact-active toc-link level3">PURE</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#tplinker" class="router-link-active router-link-exact-active toc-link level3">TPLinker</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#gplinker" class="router-link-active router-link-exact-active toc-link level3">GPLINKER</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/posts/notes/articles/%E7%AC%94%E8%AE%B0info_extract2.html#uie" class="router-link-active router-link-exact-active toc-link level3">UIE</a></li><!----><!--]--></ul><!--]--></ul></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><p>本文对 PURE, TPLINKER, GPLINKER, UIE 四篇文章做部分论文笔记与整理。</p><h2 id="信息抽取论文小述-实体抽取、关系抽取" tabindex="-1"><a class="header-anchor" href="#信息抽取论文小述-实体抽取、关系抽取" aria-hidden="true">#</a> 信息抽取论文小述|实体抽取、关系抽取</h2><h3 id="pure" tabindex="-1"><a class="header-anchor" href="#pure" aria-hidden="true">#</a> PURE</h3><p>A Frustratingly Easy Approach for Joint Entity and Relation Extraction <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2010.12812.pdf" target="_blank" rel="noopener noreferrer">论文链接<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><figure><img src="/assets/img/info_extract2/image-20220909095553098.png" alt="image-20220909095553098" tabindex="0" loading="lazy"><figcaption>image-20220909095553098</figcaption></figure><p>陈丹琦组提出的 pipeline 方案，在当时击败了其余的 joint 模型，该方案的特点在于：</p><ul><li>文章同样尝试了共享编码器，与不共享编码器的区别。发现 NER 与关系抽取分别采用两个不同的编码器效果会好一点点。</li><li><strong>实验说明了在关系抽取阶段，加入实体的类别信息很重要。</strong></li><li>跨句信息能提高成绩。</li><li>Mitigating Error （信息抽取中的误差传播问题）理论上还是存在，但是文中提出的一些办法都没能很好的解决。</li></ul><p><strong>PURE 模块一：Span-level NER</strong></p><p>在 NER 阶段，使用了传统的 span-level NER，使用以下公式代表一个 span 的 logits：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>h</mi><mi>e</mi></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">[</mo><msub><mi>X</mi><mrow><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub><mo separator="true">;</mo><msub><mi>X</mi><mrow><mi>e</mi><mi>n</mi><mi>d</mi></mrow></msub><mo separator="true">;</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex"> h_e(s_i)=[X_{start};X_{end};\phi (s_i)] </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)]</span></span></span></span></span></p><p>其中 X 为 span 开头结尾对应的 logits，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi(s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span> 为一个长度编码器。及输入一个长度数字，返回一个 embedding。</p><p>这种方法需要遍历所有的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo stretchy="false">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n+1)/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/2</span></span></span></span> 中头尾排列组合。因此文中提出了我们要限制 span 的长度，来减少计算复杂度。</p><p>限制 span 的方法也很简单，代码中直接对实体进行了截取。即超过一定长度的实体，头的位置不变，尾 span 的位置调整。</p><p><strong>PURE 模块二：关系抽取</strong></p><p>文中的关系抽取，提出了两种方案：</p><p><strong>方案一：</strong> 首先将实体抽取阶段的结果加入到输入句子中。方式是在实体首尾加入 <code>&lt;S:Md&gt;</code> 或者 <code>&lt;O:Md&gt;</code> 标识符。<code>S</code> 表示 subject，<code>Md</code> 表示实体类别。 <strong>文中指出，实体类别能够很大地提高关系抽取的结果。</strong> 预测时使用标识符位置对应的 logits 进行预测。</p><figure><img src="/assets/img/info_extract2/image-20220909102200300.png" alt="image-20220909102200300" tabindex="0" loading="lazy"><figcaption>image-20220909102200300</figcaption></figure><blockquote><p>中文版引用于 <a href="https://zhuanlan.zhihu.com/p/274938894" target="_blank" rel="noopener noreferrer">JayJay 知乎<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，论文对表格也有详细的解释。</p><ul><li><strong>TEXT</strong> ：直接提取原始文本中，实体 span 所对应的编码表示。</li><li><strong>TEXTETYPE</strong> ：在 <strong>TEXT</strong> 的基础上，concatenate 实体类别向量。</li><li><strong>MARKERS</strong> ：将标识符 <strong>S、/S、O、/O</strong> 插入到原始文本中，但是标识符没有实体类别信息。</li><li><strong>MARKERSETYPE</strong> ：在 <strong>MARKERS</strong> 的基础上，concatenate 实体类别向量，这是一种隐式的融入实体类别的方法。</li><li><strong>MARKERSELOSS</strong> ：在关系模型中，构建判别实体类别的辅助 loss。</li><li><strong>TYPEDMARKERS</strong> ：就是本文所采取的方法，实体类别“显式”地插入到文本 input 中，如&lt;S:Md&gt; 和&lt;/S:Md&gt;、&lt;O:Md&gt;和&lt;/O:Md&gt;。</li></ul></blockquote><p>从图中可以看出，显示的添加实体的类别(TYPEDMARKERS) 比其他操作效果更好。</p><p>这种方案的一次只能预测一个实体对之间的关系。因此文章提出了一种 <strong>加速方案</strong> ：</p><p><strong>方案二：加速方案</strong></p><p>将所有实体标识符放在句子最后（参考上文中的图），标识符于其代表的实体共享位置向量，上图中的颜色就表示位置向量。</p><p>此外，文中的内容 token 只去 attend 文本 token，而标识符可以 attend 所有原文 token。在预测时使用 subject 和 object 头 span 的标识符位置对应的 logits 预测。这种方案加速效果明显，指标仅仅下降了不到 1%。</p><h3 id="tplinker" tabindex="-1"><a class="header-anchor" href="#tplinker" aria-hidden="true">#</a> TPLinker</h3><p>《TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking》<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2010.13415" target="_blank" rel="noopener noreferrer">文章链接<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <a href="https://github.com/131250208/TPlinker-joint-extraction" target="_blank" rel="noopener noreferrer">代码链接<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>TPlinker 解决了暴露偏差问题，同时也能针对实体重叠，关系重叠的情况。</p><p><strong>编码方案</strong></p><p>TPlinker 数据标注形式如下</p><figure><img src="/assets/img/info_extract2/image-20220909153455544.png" alt="相关图片" height="300" tabindex="0" loading="lazy"><figcaption>相关图片</figcaption></figure><p>文中采用三种 0/1 矩阵矩阵来实现了标注，</p><ul><li>Entity Head to Entity Tail（紫色）</li><li>Subject Head to Object head；用于判断关系，每种类型的关系使用一个矩阵实现。(红色)</li><li>subject tail to object tail；用于判断关系，每种类型的关系使用一个矩阵实现。（蓝色）</li></ul><p>因此，标注数据一共是 2*R + 1 个矩阵。同时，为了缓解稀疏矩阵计算，对于标注矩阵左下三角的数据，会被转置到右上部分，并且对应的 1 改为 2。</p><p><strong>解码方案</strong></p><figure><img src="/assets/img/info_extract2/image-20220909154419258.png" alt="相关图片" height="300" tabindex="0" loading="lazy"><figcaption>相关图片</figcaption></figure><p>首先能够通过 EH to ET 的到所有实体，而后遍历每种关系类型的 SH to OH 和 ST to OT 得到三元组结果。如 <code>mayor</code> 关系：对应的三元组结果就是 (New York City, mayor, De Blasio)</p><p><strong>其他</strong></p><p>假设 <code>len_seq=5</code>在经过 transformer encoder 编码成 <code>[batch_size, 5, hidden_size]</code> 后，采用一个 <code>HandshakingKernel</code> 对所有 token pair 的排列组合进行编码 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>W</mi><mi>h</mi></msub><mo separator="true">⋅</mo><mo stretchy="false">[</mo><msub><mi>h</mi><mi>i</mi></msub><mo separator="true">;</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">]</mo><mo>+</mo><msub><mi>b</mi><mi>h</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>j</mi><mo>≥</mo><mi>i</mi></mrow><annotation encoding="application/x-tex">h_{i,j} = \tanh(W_h·[h_i;h_j] + b_h), j\ge i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>，<code>hidden states</code> 形状变成 <code>[batch_size, 5+4+3+2+1, hidden_size]</code>，最后经过一层 fc，得到如上图（解码方案部分）所示的结果，可以注意到解码方案中，New 占据 7 个格子，而 York 占据 6 个格子，这就是 <code>HandshakingKernel</code> 处理出来的格式。</p><h3 id="gplinker" tabindex="-1"><a class="header-anchor" href="#gplinker" aria-hidden="true">#</a> GPLINKER</h3><p>参考自：</p><p><a href="https://spaces.ac.cn/archives/8373" target="_blank" rel="noopener noreferrer">GlobalPointer：用统一的方式处理嵌套和非嵌套 NER<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><a href="https://spaces.ac.cn/archives/8888" target="_blank" rel="noopener noreferrer">GPLinker：基于 GlobalPointer 的实体关系联合抽取<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>GlobalPointer</strong> 大致流程为：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    qw<span class="token punctuation">,</span> kw <span class="token operator">=</span> self<span class="token punctuation">.</span>dense_q<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dense_k<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    <span class="token comment"># RoPE 相对位置编码</span>
    qw<span class="token punctuation">,</span> kw <span class="token operator">=</span> self<span class="token punctuation">.</span>add_RoPE<span class="token punctuation">(</span>qw<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>add_RoPE<span class="token punctuation">(</span>kw<span class="token punctuation">)</span>
	<span class="token comment"># 计算注意力权重</span>
    logits <span class="token operator">=</span> paddle<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">&quot;bmd,bnd-&gt;bmn&quot;</span><span class="token punctuation">,</span> qw<span class="token punctuation">,</span> kw<span class="token punctuation">)</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>head_size<span class="token operator">**</span><span class="token number">0.5</span>
    <span class="token comment"># 计算每一种实体类别对应 q, k 的 bias</span>
    bias <span class="token operator">=</span> paddle<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dense2<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>
    logits <span class="token operator">=</span> logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">+</span> bias<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">+</span> bias<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
    <span class="token comment"># ... （去除 padding 以及下三角部分的数值）</span>
    <span class="token keyword">return</span> logits
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>因此 GlobalPointer 可以看做一个加上了 RoPE 相对位置信息的多头注意力机制。笔记特别的是，该论文作者提出采用多标签分类损失函数（参考： <a href="https://spaces.ac.cn/archives/7359" target="_blank" rel="noopener noreferrer">《将“softmax+交叉熵”推广到多标签分类问题》<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ），代替 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo stretchy="false">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n+1)/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/2</span></span></span></span> 次二分类，这样解码在并行的情况下能够达到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span> 的时间复杂度。并且 GlobalPointer 计算 F1 指标时候，会相对容易很多。</p><p><strong>GPLinker</strong></p><p>与 TPLinker 的一个较大结构区别是， <strong>GPLinker 采用乘性注意力，而 TPLinker 采用加性注意力。</strong> 与 TPLinker 相同的，我们可以采用一个 GlobalPointer 来负责实体的预测（但 GPLinker 将头实体与尾实体分开预测），一个负责 SH to OH 的计算，另一个负责 ST to OT 的计算。</p><p>在训练阶段使用 GlobalPointer 中提出的多标签分类交叉熵，这也不同于 TPLinker 采用的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo stretchy="false">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n+1)/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/2</span></span></span></span> 次分类交叉熵。</p><h3 id="uie" tabindex="-1"><a class="header-anchor" href="#uie" aria-hidden="true">#</a> UIE</h3><p>提出了 text-to-structure 生成任务，将各种信息抽取任务统一进行编码、学习与训练。</p><p><strong>统一输出</strong> SEL</p><p>采用结构化的方式来构建并且区分不同关系抽取中的任务，如：</p><figure><img src="/assets/img/info_extract2/image-20221126230558158.png" alt="相关图片" height="300" tabindex="0" loading="lazy"><figcaption>相关图片</figcaption></figure><p><strong>统一输入</strong> SSI</p><figure><img src="/assets/img/info_extract2/image-20221126202207374.png" alt="相关图片" tabindex="0" loading="lazy"><figcaption>相关图片</figcaption></figure><p>输入统一构建成以下格式：</p><p><code>[spot] person [spot] company [asso] work for [text] content</code></p><p>其中 <code>[spot] + Entity_type</code> 用来表示想收取的实体类型，<code>[asso] + relation_type</code> 表示想抽取的关系名称。</p><p>不同任务的输入构建方式：</p><ul><li>关系抽取：<code>[spot] person [spot] company [asso] work for [text] content</code></li><li>事件抽取：<code>[spot] 事件类别 [asso] 论元类别 [text]</code> 观点抽取：<code>[spot] 评价维度 [asso] 观点类别 [text]</code></li></ul><p>对于预训练时候 UIE 采用了什么 prompt 进行输入构建，可以参考论文附件。</p><p><strong>进一步的预训练</strong></p><p>UIE 英文版基于 T5 进行了进一步的预训练。</p><p>预训练的优化目标包括：</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>p</mi><mi>a</mi><mi>i</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{pair}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">ai</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>：通过 Wikipedia 构建统一的结构化输出 SEL 与输入 SSI。采用生成任务进行预测与训练。训练过程中在 SSI prompt 部分加入无关的实体类别，作为负例噪声训练</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{record}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">recor</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：用来学习信息抽取统一输出的结构。采用生成任务对输出结构 SEL 进行预测与训练。</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{text}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：Span corruption based MLM.</li></ul><p><strong>微调方式</strong></p><ul><li>微调过程中，在 prompt 处加入部分不存在的实体类型，作为噪声进行训练。</li></ul><p><strong>关于 UIE 代码</strong></p><p>PaddleNLP 中的 UIE 采用的是 <code>ErnieModel</code> ，为 Encoder 架构，使用 Span 标注，通过 MRC 方式统一解决实体抽取，关系抽取，情感分析等任务。此外，在 <code>taskflow/information-extraction</code> 找到了 <code>uie-data-distill-gp</code> 模型，其中采用的是 GPLINKER 架构。</p><p>以下为部分 paddlenlp/taskflow(ErnieModel</p><p>) 使用笔记：</p><p>对于实体抽取，用户 query 格式为列表：<code>[实体类别，实体类别，...]</code>，采用 <code>[cls] + prompt + [sep] + text</code> 的方法构建输入;</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>schema <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;姓名&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;省份&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;城市&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;县区&quot;</span><span class="token punctuation">]</span>
ie <span class="token operator">=</span> Taskflow<span class="token punctuation">(</span><span class="token string">&quot;information_extraction&quot;</span><span class="token punctuation">,</span> schema<span class="token operator">=</span>schema<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&quot;&quot;&quot;
中间过程变量
short inputs [{&#39;text&#39;: &#39;北京市海淀区上地十街 10 号 18888888888 张三&#39;, &#39;prompt&#39;: &#39;姓名&#39;}]
result_list [[{&#39;text&#39;: &#39;张三&#39;, &#39;start&#39;: 24, &#39;end&#39;: 26, &#39;probability&#39;: 0.9659837822589807}]]
&quot;&quot;&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>对于实体抽取，用户 query 格式为字典：<code>{头实体：[尾实体 1， 尾实体 2，...]}</code>。刚方案默认一个头实体与一个尾实体仅存在一种关系。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>schema <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;歌曲名称&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;歌手&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;所属专辑&#39;</span><span class="token punctuation">]</span><span class="token punctuation">}</span>  
ie<span class="token punctuation">.</span>set_schema<span class="token punctuation">(</span>schema<span class="token punctuation">)</span>
ie<span class="token punctuation">(</span><span class="token string">&#39;《告别了》是孙耀威在专辑爱的故事里面的歌曲&#39;</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&quot;&quot;&quot;
过程变量
short inputs [{&#39;text&#39;: &#39;《告别了》是孙耀威在专辑爱的故事里面的歌曲&#39;, &#39;prompt&#39;: &#39;歌曲名称&#39;}]
result_list [[{&#39;text&#39;: &#39;告别了&#39;, &#39;start&#39;: 1, &#39;end&#39;: 4, &#39;probability&#39;: 0.6296134126625006}, {&#39;text&#39;: &#39;爱的故事&#39;, &#39;start&#39;: 12, &#39;end&#39;: 16, &#39;probability&#39;: 0.28168733127927226}]]
# 这里识别错了爱的故事，因此下面就会出现由于 pipeline 流程导致的
short inputs [{&#39;text&#39;: &#39;《告别了》是孙耀威在专辑爱的故事里面的歌曲&#39;, &#39;prompt&#39;: &#39;告别了的歌手&#39;}, {&#39;text&#39;: &#39;《告别了》是孙耀威在专辑爱的故事里面的歌曲&#39;, &#39;prompt&#39;: &#39;爱的故事的歌手&#39;}]

result_list [[{&#39;text&#39;: &#39;孙耀威&#39;, &#39;start&#39;: 6, &#39;end&#39;: 9, &#39;probability&#39;: 0.9988381005599081}], [{&#39;text&#39;: &#39;孙耀威&#39;, &#39;start&#39;: 6, &#39;end&#39;: 9, &#39;probability&#39;: 0.9951415104192272}]]

examples: [{&#39;text&#39;: &#39;《告别了》是孙耀威在专辑爱的故事里面的歌曲&#39;, &#39;prompt&#39;: &#39;告别了的所属专辑&#39;}, {&#39;text&#39;: &#39;《告别了》是孙耀威在专辑爱的故事里面的歌曲&#39;, &#39;prompt&#39;: &#39;爱的故事的所属专辑&#39;}]
short inputs [{&#39;text&#39;: &#39;《告别了》是孙耀威在专辑爱的故事里面的歌曲&#39;, &#39;prompt&#39;: &#39;告别了的所属专辑&#39;}, {&#39;text&#39;: &#39;《告别了》是孙耀威在专辑爱的故事里面的歌曲&#39;, &#39;prompt&#39;: &#39;爱的故事的所属专辑&#39;}]
...
&quot;&quot;&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>该方式与训练描述的输入输出格式不同。但 paddlenlp 中模型支持中文，在 T5 没有中文权重的情况下，使用 ERNIE 中文权重应该会更合适。</p><p>论文官方的源码为： <a href="https://github.com/universal-ie/UIE/blob/main/run_uie_pretrain.py" target="_blank" rel="noopener noreferrer">universal-ie/UIE<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。输入输出处理方式与论文相同。</p></div><!----><footer class="page-meta"><!----><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 417333277@qq.com">kevinng77</span><!--]--><!--]--></div></div></footer><!----><!----><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer"><a href="https://beian.miit.gov.cn/" target="_blank">闽ICP备2020021116号</a>&nbsp;<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=" target="_blank"><img src="/assets/gongan.png">闽公网安备 35058102000231号 </a></div><div class="copyright">Copyright © 2023 Kevin 吴嘉文</div></footer></div><!--]--><!--]--><!----><!----><!--]--></div>
    <script type="module" src="/assets/app-8b900482.js" defer></script>
  </body>
</html>
