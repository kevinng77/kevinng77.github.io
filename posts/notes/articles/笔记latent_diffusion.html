<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0latent_diffusion.html"><meta property="og:site_name" content="记忆笔书"><meta property="og:title" content="DIFFUSION 系列笔记| Latent Diffusion Model"><meta property="og:description" content="相对于 DDIM, DDPM 以及 SDE，High-Resolution Image Synthesis with Latent Diffusion Models 一文重点在于 latent Space 和 Conditioning Cross Attention，而非 diffusion pipeline 流程。 以此不同于前几份笔记，本文主要参考 huggingface/diffusers 中 Latent Diffusion Model 及 Stable Diffusion 的实现，对 LDM 架构及其中的 Conditioning Cross Attention 做梳理。"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2023-10-20T06:56:10.000Z"><meta property="article:author" content="Kevin 吴嘉文"><meta property="article:tag" content="CV"><meta property="article:tag" content="Diffusion"><meta property="article:tag" content="AIGC"><meta property="article:published_time" content="2023-08-29T00:00:00.000Z"><meta property="article:modified_time" content="2023-10-20T06:56:10.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"DIFFUSION 系列笔记| Latent Diffusion Model","image":[""],"datePublished":"2023-08-29T00:00:00.000Z","dateModified":"2023-10-20T06:56:10.000Z","author":[{"@type":"Person","name":"Kevin 吴嘉文"}]}</script><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;700&display=swap" rel="stylesheet"><link rel="icon" href="/logo.svg"><title>DIFFUSION 系列笔记| Latent Diffusion Model | 记忆笔书</title><meta name="description" content="相对于 DDIM, DDPM 以及 SDE，High-Resolution Image Synthesis with Latent Diffusion Models 一文重点在于 latent Space 和 Conditioning Cross Attention，而非 diffusion pipeline 流程。 以此不同于前几份笔记，本文主要参考 huggingface/diffusers 中 Latent Diffusion Model 及 Stable Diffusion 的实现，对 LDM 架构及其中的 Conditioning Cross Attention 做梳理。">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-99575b2e.css" as="style"><link rel="stylesheet" href="/assets/style-99575b2e.css">
    <link rel="modulepreload" href="/assets/app-c62a9332.js"><link rel="modulepreload" href="/assets/笔记latent_diffusion.html-5a8b4305.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/assets/笔记latent_diffusion.html-02702749.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><!----><!----><span class="vp-site-name">记忆笔书</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="主页" class="vp-link nav-link nav-link" href="/"><span class="font-icon icon iconfont icon-home" style=""></span>主页<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="归档" class="vp-link nav-link nav-link" href="/timeline/"><span class="font-icon icon iconfont icon-categoryselected" style=""></span>归档<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/kevinng77" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!----><!--[--><button type="button" class="search-pro-button" role="search" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->DIFFUSION 系列笔记| Latent Diffusion Model</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Kevin 吴嘉文</span></span><span property="author" content="Kevin 吴嘉文"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2023-08-29T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 9 分钟</span><meta property="timeRequired" content="PT9M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category7 clickable" role="navigation">知识笔记</span><!--]--><meta property="articleSection" content="知识笔记"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag1 clickable" role="navigation">CV</span><span class="page-tag-item tag5 clickable" role="navigation">Diffusion</span><span class="page-tag-item tag4 clickable" role="navigation">AIGC</span><!--]--><meta property="keywords" content="CV,Diffusion,AIGC"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<!----></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#latent-diffusion-model">Latent Diffusion Model</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#ldm-主要思想">LDM 主要思想</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#ldm-使用示例">LDM 使用示例</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#ldm-pipeline">LDM Pipeline</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#ldm-super-resolution-pipeline">LDM Super Resolution Pipeline</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#stable-diffusion">Stable diffusion</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#sd-v1-架构">SD v1 架构</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#sd-v1-1-v1-5">SD v1.1 - v1.5</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#sd-v2">SD v2</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#lora">Lora</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><p>相对于 DDIM, DDPM 以及 SDE，High-Resolution Image Synthesis with Latent Diffusion Models 一文重点在于 latent Space 和 Conditioning Cross Attention，而非 diffusion pipeline 流程。</p><p>以此不同于前几份笔记，本文主要参考 <a href="https://zhuanlan.zhihu.com/p/659212489/huggingface/diffusers" target="_blank" rel="noopener noreferrer">huggingface/diffusers<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 中 Latent Diffusion Model 及 Stable Diffusion 的实现，对 LDM 架构及其中的 Conditioning Cross Attention 做梳理。</p><p>系列笔记</p><ul><li><a href="https://zhuanlan.zhihu.com/p/650495280" target="_blank" rel="noopener noreferrer">Kevin 吴嘉文：Diffusion|DDPM 理解、数学、代码<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://zhuanlan.zhihu.com/p/650614674" target="_blank" rel="noopener noreferrer">Kevin 吴嘉文：DIFFUSION 系列笔记|DDIM 数学、思考与 ppdiffuser 代码探索<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://zhuanlan.zhihu.com/p/655679978" target="_blank" rel="noopener noreferrer">Kevin 吴嘉文：DIFFUSION 系列笔记| SDE（上）<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li></ul><h2 id="latent-diffusion-model" tabindex="-1"><a class="header-anchor" href="#latent-diffusion-model" aria-hidden="true">#</a> Latent Diffusion Model</h2><p>论文：High-Resolution Image Synthesis with Latent Diffusion Models</p><figure><img src="https://pic3.zhimg.com/80/v2-da826549375793c4f8472a54a14c1616_1440w.webp" alt="LDM 架构图" tabindex="0" loading="lazy"><figcaption>LDM 架构图</figcaption></figure><h3 id="ldm-主要思想" tabindex="-1"><a class="header-anchor" href="#ldm-主要思想" aria-hidden="true">#</a> LDM 主要思想</h3><p>扩散模型（DMs）直接在像素领域工作，优化和推断都很费时。为了在有限的计算资源上训练它们，LDM 先使用一个预训练好的 AutoEncoder，将图片像素转换到了维度较小的 latent space 上，而后再进行传统的扩散模型推理与优化。这种训练方式使得 LDM 在算力和性能之间得到了平衡。</p><p>此外，通过引入交叉注意力，使得 DMs 能够在条件生成上有不错的效果，包括如文字生成图片，inpainting 等。</p><h3 id="ldm-使用示例" tabindex="-1"><a class="header-anchor" href="#ldm-使用示例" aria-hidden="true">#</a> <strong>LDM 使用示例</strong></h3><p>huggingface Diffusers 将各种 Diffusion Model Pipeline 都包装好了，使用 Diffusion model 就和使用 Transformers 一样地方便：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> diffusers </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> DiffusionPipeline</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># load model and scheduler</span></span>
<span class="line"><span style="color:#24292E;">ldm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> DiffusionPipeline.from_pretrained(</span><span style="color:#032F62;">&quot;CompVis/ldm-text2im-large-256&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;"> </span><span style="color:#6A737D;"># run pipeline in inference (sample random noise and denoise)</span></span>
<span class="line"><span style="color:#24292E;">prompt </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;A painting of a squirrel eating a burger&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">images </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> ldm([prompt], </span><span style="color:#E36209;">num_inference_steps</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">50</span><span style="color:#24292E;">, </span><span style="color:#E36209;">eta</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0.3</span><span style="color:#24292E;">, </span><span style="color:#E36209;">guidance_scale</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">6</span><span style="color:#24292E;">).images</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># save images</span></span>
<span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> idx, image </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">enumerate</span><span style="color:#24292E;">(images):</span></span>
<span class="line"><span style="color:#24292E;">    image.save(</span><span style="color:#D73A49;">f</span><span style="color:#032F62;">&quot;squirrel-</span><span style="color:#005CC5;">{</span><span style="color:#24292E;">idx</span><span style="color:#005CC5;">}</span><span style="color:#032F62;">.png&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="ldm-pipeline" tabindex="-1"><a class="header-anchor" href="#ldm-pipeline" aria-hidden="true">#</a> LDM Pipeline</h3><p>LDM 的 pipeline 可以简化表示为：<code>Pipeline(prompt, num_inference_steps, latents)</code>。我们暂时考虑没有 negative prompt 和 初始 latent 的输入，那么整个采样过程大致可以表示为：</p><ol><li>首先采用了 BERT 架构模型对 prompt 进行处理，生成 <code>text_hidden_state</code>；同时生成随机噪声 <code>latents</code>。</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">text_hidden_state </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> LDMBERT(prompt) </span><span style="color:#6A737D;"># shape=[bs, len_seq, d_model] = [1, 77, 1280] </span></span>
<span class="line"><span style="color:#24292E;">latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> randn_tensor(latents_shape) </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>对于 <code>&quot;CompVis/ldm-text2im-large-256&quot;</code>，其中使用了 <code>LDMBert</code>， 参考 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py" target="_blank" rel="noopener noreferrer">huggignface 的 LDMBert<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 实现，<code>LDMBert</code> 与传统 BERT 架构相似，规模不同，LDMBert 采用 32 层， hidden_size 为 1280，属实比 bert-base 大上不少。同时文本被 padding 到了固定的 77 长度，以此来保证文字的 hidden state 格式为 <code>[batch_size, 77, 1280]</code>。</p><ol start="2"><li>之后进行传统的扩散模型 backward process：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> t </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.progress_bar(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.timesteps):</span></span>
<span class="line"><span style="color:#24292E;">    noise_pred </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.unet(latents_input, t, </span><span style="color:#E36209;">encoder_hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">context).sample</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># compute the previous noisy sample x_t -&gt; x_t-1</span></span>
<span class="line"><span style="color:#24292E;">    latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.step(noise_pred, t, latents, </span><span style="color:#D73A49;">**</span><span style="color:#24292E;">extra_kwargs).prev_sample</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中 UNET 为 <code>UNet2DConditionModel</code>，与传统 Unet 不同在于其应用了 Cross Attention 对文字以及图片信息进行综合处理，下文会对改模块做梳理。scheduler 可以选 DDIM 或者其他算法。</p><ol start="3"><li>最后对 latent hidden state 进行 decode，生成图片：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.vqvae.config.scaling_factor </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> latents</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.vqvae.decode(latents).sample</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="ldm-中的-unet" tabindex="-1"><a class="header-anchor" href="#ldm-中的-unet" aria-hidden="true">#</a> LDM 中的 UNET</h4><p>backward process 中的 <code>self.unet(...)</code>，即 <code>UNET2DCondition(sample, timestep, encoder_hidden_state)</code> 前向推导可以看成五部分，（以下以 <code>CompVis/ldm-text2im-large-256</code> 为例介绍）：</p><ul><li><strong>准备 time steps</strong> ：Timesteps 编码信息是 diffusion 中 predict noise residual 模型的标配：</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 经过两次映射得到 timesteps 对应的 embedding</span></span>
<span class="line"><span style="color:#24292E;">t_emb </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.time_proj(timesteps)</span></span>
<span class="line"><span style="color:#24292E;">emb </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.time_embedding(t_emb, timestep_cond)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>pre-process：</strong> LDM 只用了一个 2D 卷积对输入的 hidden state 进行处理</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Conv2d(</span></span>
<span class="line"><span style="color:#24292E;">            in_channels, block_out_channels[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">], </span><span style="color:#E36209;">kernel_size</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">conv_in_kernel, </span><span style="color:#E36209;">padding</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">conv_in_padding</span></span>
<span class="line"><span style="color:#24292E;">        )(sample)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>down sampling</strong> ：down sampling 包括了 3 个 <code>CrossAttnDownBlock2D</code>, 和 1 个 <code>DownBlock2D</code>。</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># down sampling 大致前向推导</span></span>
<span class="line"><span style="color:#24292E;">down_block_res_samples </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> (sample,)</span></span>
<span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> downsample_block </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.down_blocks:</span></span>
<span class="line"><span style="color:#24292E;">    sample, res_samples </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> downsample_block(</span><span style="color:#E36209;">hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">sample, </span><span style="color:#E36209;">temb</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">emb, </span><span style="color:#E36209;">scale</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">lora_scale)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 用于 UNET 的残差链接</span></span>
<span class="line"><span style="color:#24292E;">    down_block_res_samples </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> res_samples</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中每个 <code>CrossAttnDownBlock2D</code> 大概前向过程为：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># CrossAttnDownBlock2D</span></span>
<span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, hidden_states, temb, encoder_hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">	output_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> ()</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> resnet, attn </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">zip</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.resnets, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.attentions):</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> resnet(hidden_states, temb)</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> attn(</span></span>
<span class="line"><span style="color:#24292E;">            hidden_states,</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#E36209;">encoder_hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">encoder_hidden_states,</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#E36209;">cross_attention_kwargs</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">cross_attention_kwargs,</span></span>
<span class="line"><span style="color:#24292E;">        ).sample</span></span>
<span class="line"><span style="color:#24292E;">        output_states </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> (hidden_states,)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># downsampler = Conv2D </span></span>
<span class="line"><span style="color:#24292E;">    hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> downsampler(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">    output_states </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> (hidden_states,)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> hidden_states, output_states</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在 <code>CompVis/ldm-text2im-large-256</code> 中，每个 <code>CrossAttnDownBlock2D</code> 包含了 2 个 <code>attn</code>（<code>Transformer2DModel</code>）以及 2 个 <code>resnet</code> （<code>ResnetBlock2D</code>）。</p><p>文字与图像的交互就发生在 <code>Transformer2DModel</code> 当中。每个 <a href="https://github.com/huggingface/diffusers/blob/16b9a57d29b6dbce4f97dbf439af1663d2c54588/src/diffusers/models/transformer_2d.py#L44C6-L44C6" target="_blank" rel="noopener noreferrer">Transformer2DModel<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 先对输入的图像数据进行预处理，将图片格式从如 <code>(batch_size, channel, width, height)</code> 或 <code>(batch_size, num_image_vectors)</code> 转换为 <code>(batch_size, len_seq, hidden_size)</code>，而后将 <code>hidden_states</code> 传入 1 层传统 Transformer layer（非 bert 或 GPT 类型），先对图像 <code>hidden_states</code> 进行 self-attention，而后结合 <code>encoder_hidden_states</code> 进行 cross attention 处理。</p><ul><li><strong>mid processing:</strong></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MidBlock2DCrossAttn()(sample, </span></span>
<span class="line"><span style="color:#24292E;">                              emb,</span></span>
<span class="line"><span style="color:#24292E;">                           encoder_hidden_states)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在 <code>CompVis/ldm-text2im-large-256</code> 中，upsampling 和 down sampling 之间采用 <a href="https://github.com/huggingface/diffusers/blob/16b9a57d29b6dbce4f97dbf439af1663d2c54588/src/diffusers/models/unet_2d_blocks.py#L572" target="_blank" rel="noopener noreferrer">MidBlock2DCrossAttn<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 连接，<code>MidBlock2DCrossAttn</code> 包括了 1 个 1 层的 <code>Transformer2DModel</code> 以及 1 个 <code>resnet</code> <code>ResnetBlock2D</code>。</p><ul><li><strong>upsampling</strong> ：upsampling 采用的模块 UpBlocks 包括了 <code> (&quot;UpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;)</code>，各个模块的架构与 down sampling 中的模块相似。</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># upsample_block</span></span>
<span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> i, upsample_block </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">enumerate</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.up_blocks):</span></span>
<span class="line"><span style="color:#24292E;">    sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> upsample_block(</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">hidden_states</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">sample,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">temb</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">emb,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">res_hidden_states_tuple</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">res_samples,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">upsample_size</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">upsample_size,</span></span>
<span class="line"><span style="color:#24292E;">                    </span><span style="color:#E36209;">scale</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">lora_scale,</span></span>
<span class="line"><span style="color:#24292E;">                )</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>post-process</strong></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># GroupNorm</span></span>
<span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.conv_norm_out(sample)</span></span>
<span class="line"><span style="color:#6A737D;"># Silu</span></span>
<span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.conv_act(sample)</span></span>
<span class="line"><span style="color:#6A737D;"># Conv2d(320, 4, kernel=(3,3), s=(1,1), padding=(1,1))</span></span>
<span class="line"><span style="color:#24292E;">sample </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.conv_out(sample)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>总结起来，down sampling，midprocess，upsampling 三个步骤中都涉及到了 <code>Transformer2DModel</code> ，实现多模态的信息交互。</p><h3 id="ldm-super-resolution-pipeline" tabindex="-1"><a class="header-anchor" href="#ldm-super-resolution-pipeline" aria-hidden="true">#</a> LDM Super Resolution Pipeline</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">low_res_img </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> Image.open(BytesIO(response.content)).convert(</span><span style="color:#032F62;">&quot;RGB&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">low_res_img </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> low_res_img.resize((</span><span style="color:#005CC5;">128</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">128</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E36209;">upscaled_image</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> pipeline(low_res_img, </span><span style="color:#E36209;">num_inference_steps</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">100</span><span style="color:#24292E;">, </span><span style="color:#E36209;">eta</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">).images[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">upscaled_image.save(</span><span style="color:#032F62;">&quot;ldm_generated_image.png&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>大致前项推导流程可以概括为：</p><ol><li>根据 输入图片大小，生成对应的 latent 噪音以及 time step embedding：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> randn_tensor(latents_shape, </span><span style="color:#E36209;">generator</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">generator, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.device, </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">latents_dtype)  </span><span style="color:#6A737D;"># shape 与输入图片相同</span></span>
<span class="line"><span style="color:#24292E;">latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> latents </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.init_noise_sigma</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>将 latent 与原始图片拼接，然后进行 diffusion 反向推导：</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> t </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.progress_bar(timesteps_tensor):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># concat latents and low resolution image in the channel dimension.</span></span>
<span class="line"><span style="color:#24292E;">    latents_input </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.cat([latents, image], </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">    latents_input </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.scale_model_input(latents_input, t)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># predict the noise residual</span></span>
<span class="line"><span style="color:#24292E;">    noise_pred </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.unet(latents_input, t).sample</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># compute the previous noisy sample x_t -&gt; x_t-1</span></span>
<span class="line"><span style="color:#24292E;">    latents </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scheduler.step(noise_pred, t, latents, </span><span style="color:#D73A49;">**</span><span style="color:#24292E;">extra_kwargs).prev_sample</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="3"><li>使用 vqvae 对 latent 进行解码，得到最终图片</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># decode the image latents with the VQVAE</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.vqvae.decode(latents).sample</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.clamp(image, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1.0</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1.0</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> image </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0.5</span></span>
<span class="line"><span style="color:#24292E;">image </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> image.cpu().permute(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">).numpy()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="stable-diffusion" tabindex="-1"><a class="header-anchor" href="#stable-diffusion" aria-hidden="true">#</a> Stable diffusion</h2><h3 id="sd-v1-架构" tabindex="-1"><a class="header-anchor" href="#sd-v1-架构" aria-hidden="true">#</a> SD v1 架构</h3><p>参考 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py" target="_blank" rel="noopener noreferrer">hugging face diffuser 的 SD pipeline 实现<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。以 <code>stable-diffusion-v1-5</code> 为例。</p><ol><li><strong>Text Encoder</strong></li></ol><p>采用 <code>CLIPTextModel</code>，来自于 <a href="https://arxiv.org/pdf/2103.00020.pdf" target="_blank" rel="noopener noreferrer">CLIP<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 的 Text Encoder 部分。相比于其他传统的 Transformer 语言模型，CLIP 在预训练时，在 text-image pair 数据集上进行了对比学习预训练。prompt_embeds, negative_prompt_embeds 在经过编码后，shape 都为 <code>[batch_size, 77, 768]</code></p><ol start="2"><li><strong>Diffusion 反向采样过程</strong></li></ol><p>SD v1.5 采样过程与 LDM 相似，其中的 latents 大小为 <code>[bs, 4, 64, 64]</code>。对于 txt2img，latents 通过随机生成，对于 img2img，latents 通过 VAE 模型进行 encode。</p><p>Unet 配置与 LDM 相似：</p><ul><li><p>down sampling 采用 3 个 <code>CrossAttnDownBlock2D</code>, 和 1 个 <code>DownBlock2D</code>。</p></li><li><p>mid block 采用 1 个 <code>MidBlock2DCrossAttn</code>。hidden size = 1280</p></li><li><p>Up sampling 采用 1 个 <code>UpBlock2D</code> + 3 个 <code>CrossAttnUpBlock2D</code></p></li></ul><p>每个 CrossAttn 的 transformer 中， text embedding 大小为 768，但 Transformer 模块的 <code>hidden size</code> 随着 Unet 深入而增加。如 down sampling 采用的维度为 320, 640, 1280, 1280。那么 3 个 Transformer 模块中的 hidden size 就分别是 320, 640, 1280。</p><p>以 down sampling 为例，在进行 cross attention 时候，图像的 hidden state （latent）大小分别被映射到了 <code>[4096, 320]</code>，<code>[2014, 640]</code>，<code>[256, 1280]</code> ，而后与文字的 hidden state <code>[77, 768]</code> 进行 cross attention 计算。（以上张量维度省略了 batch size）</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># hidden size 为 320 时候的 cross attention 单元示例</span></span>
<span class="line"><span style="color:#24292E;">Attention(</span></span>
<span class="line"><span style="color:#24292E;">(to_q): LoRACompatibleLinear(</span><span style="color:#E36209;">in_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">320</span><span style="color:#24292E;">, </span><span style="color:#E36209;">out_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">320</span><span style="color:#24292E;">, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">(to_k): LoRACompatibleLinear(</span><span style="color:#E36209;">in_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">768</span><span style="color:#24292E;">, </span><span style="color:#E36209;">out_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">320</span><span style="color:#24292E;">, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">(to_v): LoRACompatibleLinear(</span><span style="color:#E36209;">in_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">768</span><span style="color:#24292E;">, </span><span style="color:#E36209;">out_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">320</span><span style="color:#24292E;">, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这也是 SD Unet 中 Transformer2DBlock 与传统 Transformer 主要的不同，SD Unet 中的 Transformer2DBlock 输入与输出维度是不一样的。</p><ol start="3"><li><strong>super resolution</strong></li></ol><p>生成后 latent 大小为 64 * 64， 通过 VQModel 解码为 512*512</p><h3 id="sd-v1-1-v1-5" tabindex="-1"><a class="header-anchor" href="#sd-v1-1-v1-5" aria-hidden="true">#</a> SD v1.1 - v1.5</h3><p>stable diffusion 1.1-1.5 的模型架构相同，以下搬运 <a href="https://github.com/runwayml/stable-diffusion#weights" target="_blank" rel="noopener noreferrer">runwayml<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 的 stable diffusion weights 总结：</p><ul><li><p><a href="https://huggingface.co/compvis" target="_blank" rel="noopener noreferrer"><code>sd-v1-1.ckpt</code><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>: 237k steps at resolution <code>256x256</code> on <a href="https://huggingface.co/datasets/laion/laion2B-en" target="_blank" rel="noopener noreferrer">laion2B-en<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>. 194k steps at resolution <code>512x512</code> on <a href="https://huggingface.co/datasets/laion/laion-high-resolution" target="_blank" rel="noopener noreferrer">laion-high-resolution<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> (170M examples from LAION-5B with resolution <code>&gt;= 1024x1024</code>).</p></li><li><p><a href="https://huggingface.co/compvis" target="_blank" rel="noopener noreferrer"><code>sd-v1-2.ckpt</code><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>: Resumed from <code>sd-v1-1.ckpt</code>. 515k steps at resolution <code>512x512</code> on <a href="https://laion.ai/blog/laion-aesthetics/" target="_blank" rel="noopener noreferrer">laion-aesthetics v2 5+<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> (a subset of laion2B-en with estimated aesthetics score <code>&gt; 5.0</code>, and additionally filtered to images with an original size <code>&gt;= 512x512</code>, and an estimated watermark probability <code>&lt; 0.5</code>. The watermark estimate is from the <a href="https://laion.ai/blog/laion-5b/" target="_blank" rel="noopener noreferrer">LAION-5B<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> metadata, the aesthetics score is estimated using the <a href="https://github.com/christophschuhmann/improved-aesthetic-predictor" target="_blank" rel="noopener noreferrer">LAION-Aesthetics Predictor V2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>).</p></li><li><p><a href="https://huggingface.co/compvis" target="_blank" rel="noopener noreferrer"><code>sd-v1-3.ckpt</code><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>: Resumed from <code>sd-v1-2.ckpt</code>. 195k steps at resolution <code>512x512</code> on &quot;laion-aesthetics v2 5+&quot; and 10% dropping of the text-conditioning to improve <a href="https://arxiv.org/abs/2207.12598" target="_blank" rel="noopener noreferrer">classifier-free guidance sampling<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p></li><li><p><a href="https://huggingface.co/compvis" target="_blank" rel="noopener noreferrer"><code>sd-v1-4.ckpt</code><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>: Resumed from <code>sd-v1-2.ckpt</code>. 225k steps at resolution <code>512x512</code> on &quot;laion-aesthetics v2 5+&quot; and 10% dropping of the text-conditioning to improve <a href="https://arxiv.org/abs/2207.12598" target="_blank" rel="noopener noreferrer">classifier-free guidance sampling<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p></li><li><p><a href="https://huggingface.co/runwayml/stable-diffusion-v1-5" target="_blank" rel="noopener noreferrer"><code>sd-v1-5.ckpt</code><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>: Resumed from <code>sd-v1-2.ckpt</code>. 595k steps at resolution <code>512x512</code> on &quot;laion-aesthetics v2 5+&quot; and 10% dropping of the text-conditioning to improve <a href="https://arxiv.org/abs/2207.12598" target="_blank" rel="noopener noreferrer">classifier-free guidance sampling<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p></li><li><p><a href="https://huggingface.co/runwayml/stable-diffusion-inpainting" target="_blank" rel="noopener noreferrer"><code>sd-v1-5-inpainting.ckpt</code><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>: Resumed from <code>sd-v1-5.ckpt</code>. 440k steps of inpainting training at resolution <code>512x512</code> on &quot;laion-aesthetics v2 5+&quot; and 10% dropping of the text-conditioning to improve <a href="https://arxiv.org/abs/2207.12598" target="_blank" rel="noopener noreferrer">classifier-free guidance sampling<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.</p></li></ul><h3 id="sd-v2" tabindex="-1"><a class="header-anchor" href="#sd-v2" aria-hidden="true">#</a> SD v2</h3><p>参考 <a href="https://github.com/Stability-AI/stablediffusion" target="_blank" rel="noopener noreferrer">stability-AI 仓库<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，SD v2 相对 v1 系列改动较大：</p><p>架构方面 SD v2 系列：</p><ul><li>采用了 <a href="https://github.com/mlfoundations/open_clip" target="_blank" rel="noopener noreferrer">OpenCLIP-ViT/H<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 作为 text encoder。</li><li>Unet 架构改变：其中 Transformer 模块中的 <code>attention_head_dim</code> 变为了 <code>5,10,20,20</code>，SD v1 中为 <code>8,8,8,8</code>。<code>cross_attention_dim</code> 从 768 变为 1280。同时在 latent hidden state 进入 cross attention 之前，额外采用了 <code>linear_projection</code> 进行 latent hidden state 的处理，SD v1 中为卷积层处理。</li></ul><p>训练方面 SD v2 系列，（以下拷贝了 huggingface 中 SD 模型 model card 的介绍） ：</p><ul><li><a href="https://huggingface.co/stabilityai/stable-diffusion-2-base" target="_blank" rel="noopener noreferrer">SD 2.0-base<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>：The model is trained from scratch 550k steps at resolution <code>256x256</code> on a subset of <a href="https://laion.ai/blog/laion-5b/" target="_blank" rel="noopener noreferrer">LAION-5B<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> filtered for explicit pornographic material, using the <a href="https://github.com/LAION-AI/CLIP-based-NSFW-Detector" target="_blank" rel="noopener noreferrer">LAION-NSFW classifier<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> with <code>punsafe=0.1</code> and an <a href="https://github.com/christophschuhmann/improved-aesthetic-predictor" target="_blank" rel="noopener noreferrer">aesthetic score<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> &gt;= <code>4.5</code>. Then it is further trained for 850k steps at resolution <code>512x512</code> on the same dataset on images with resolution <code>&gt;= 512x512</code>.</li><li><a href="https://huggingface.co/stabilityai/stable-diffusion-2" target="_blank" rel="noopener noreferrer">SD v2.0<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>：This <code>stable-diffusion-2</code> model is resumed from <a href="https://huggingface.co/stabilityai/stable-diffusion-2-base" target="_blank" rel="noopener noreferrer">stable-diffusion-2-base<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> (<code>512-base-ema.ckpt</code>) and trained for 150k steps using a <a href="https://arxiv.org/abs/2202.00512" target="_blank" rel="noopener noreferrer">v-objective<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> on the same dataset. Resumed for another 140k steps on <code>768x768</code> images.</li><li><a href="https://huggingface.co/stabilityai/stable-diffusion-2-1" target="_blank" rel="noopener noreferrer">SD v2.1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>：This <code>stable-diffusion-2-1</code> model is fine-tuned from <a href="https://huggingface.co/stabilityai/stable-diffusion-2" target="_blank" rel="noopener noreferrer">stable-diffusion-2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> (<code>768-v-ema.ckpt</code>) with an additional 55k steps on the same dataset (with <code>punsafe=0.1</code>), and then fine-tuned for another 155k extra steps with <code>punsafe=0.98</code>.</li></ul><h2 id="lora" tabindex="-1"><a class="header-anchor" href="#lora" aria-hidden="true">#</a> Lora</h2><p>huggingface diffuser 中 Lora 的实现与 huggingface/PEFT 实现方法相似，添加 Lora 只需要通过撰写规则，锁定需要改动的 layer，并替换为 LoRACompatibleLayer 实现，huggingface 也提供好了 <a href="https://link.zhihu.com/?target=https%3A//huggingface.co/docs/diffusers/training/lora" target="_blank" rel="noopener noreferrer">lora 训练代码<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，和 SD lora 推理方法。</p><p>Diffusers 中，SD 采用 Lora 的部分位于 Unet 当中，大部分的 Lora 在 Transformer 模块当中，SD 的 lora 与 NLP Lora 实现方式基本相同， <strong>一个较大的区别在于，SD 中的 Lora 除了对线性层进行 Lora 叠加外，也对卷积层进行了 Lora 改造</strong> 。</p></div><!----><footer class="page-meta"><!----><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 417333277@qq.com">kevinng77</span><!--]--><!--]--></div></div></footer><!----><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer"><a href="https://beian.miit.gov.cn/" target="_blank">沪ICP备2023027904号-1</a>&nbsp;<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=" target="_blank"><img src="/assets/gongan.png">公安备案号 </a></div><div class="vp-copyright">Copyright © 2024 Kevin 吴嘉文</div></footer></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-c62a9332.js" defer></script>
  </body>
</html>
