<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0spark.html"><meta property="og:site_name" content="记忆笔书"><meta property="og:title" content="Spark - 框架理解与使用"><meta property="og:description" content="内容包括：Spark 搭建本地模式，集群模式（StandAlon, YARN）搭建；认识 Spark 框架、运行逻辑；认识 PySpark 下 SparkCore、SparkSQL 操作。 python 上 Spark 不断个更新，实时关注 官方文档"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2023-12-26T15:42:10.000Z"><meta property="article:author" content="Kevin 吴嘉文"><meta property="article:tag" content="大数据"><meta property="article:published_time" content="2022-03-07T00:00:00.000Z"><meta property="article:modified_time" content="2023-12-26T15:42:10.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Spark - 框架理解与使用","image":[""],"datePublished":"2022-03-07T00:00:00.000Z","dateModified":"2023-12-26T15:42:10.000Z","author":[{"@type":"Person","name":"Kevin 吴嘉文"}]}</script><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;700&display=swap" rel="stylesheet"><link rel="icon" href="/logo.svg"><title>Spark - 框架理解与使用 | 记忆笔书</title><meta name="description" content="内容包括：Spark 搭建本地模式，集群模式（StandAlon, YARN）搭建；认识 Spark 框架、运行逻辑；认识 PySpark 下 SparkCore、SparkSQL 操作。 python 上 Spark 不断个更新，实时关注 官方文档">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-99575b2e.css" as="style"><link rel="stylesheet" href="/assets/style-99575b2e.css">
    <link rel="modulepreload" href="/assets/app-86c9497f.js"><link rel="modulepreload" href="/assets/笔记spark.html-d7c1d7c1.js"><link rel="modulepreload" href="/assets/笔记spark.html-00f4893a.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><!----><!----><span class="vp-site-name">记忆笔书</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="主页" class="vp-link nav-link nav-link" href="/"><span class="font-icon icon iconfont icon-home" style=""></span>主页<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="归档" class="vp-link nav-link nav-link" href="/timeline/"><span class="font-icon icon iconfont icon-categoryselected" style=""></span>归档<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/kevinng77" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!----><!--[--><button type="button" class="search-pro-button" role="search" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Spark - 框架理解与使用</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Kevin 吴嘉文</span></span><span property="author" content="Kevin 吴嘉文"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2022-03-07T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 23 分钟</span><meta property="timeRequired" content="PT23M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category7 clickable" role="navigation">知识笔记</span><!--]--><meta property="articleSection" content="知识笔记"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag8 clickable" role="navigation">大数据</span><!--]--><meta property="keywords" content="大数据"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<!----></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#spark-与-hadoop">Spark 与 hadoop</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#spark-框架模型">spark 框架模型</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#spark-架构角色">Spark 架构角色</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#本地-local-模式配置">本地 local 模式配置</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#spark-操作介绍">Spark 操作介绍</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#spark-端口">Spark 端口</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#pyspark">PySpark</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#初体验-wordcount-实例">初体验：WordCount 实例</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#spark-core-api">Spark Core API</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#创建-rdd">创建 RDD</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#rdd-算子">RDD 算子</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#rdd-持久化">RDD 持久化</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#共享变量">共享变量</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#内核调度">内核调度</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#总结-spark-层级">总结 Spark 层级</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#spark-shuffle">Spark Shuffle</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#sparksql">SparkSQL</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#快速体验">快速体验</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#dataframe">DataFrame</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#sparksql-定义-udf">SparkSQL 定义 UDF</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#窗口函数">窗口函数</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#sql-流程">SQL 流程</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#spark-on-hive">Spark on Hive</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#分布式-sql-执行引擎">分布式 SQL 执行引擎</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#standalone-架构">Standalone 架构</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#实例集群规划">实例集群规划</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#安装">安装</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#配置配置文件">配置配置文件</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#将-spark-安装文件夹-分发到其它的节点上">将 Spark 安装文件夹 分发到其它的节点上</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#验证环境">验证环境</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#spark-standalone-ha">Spark StandAlone HA</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#步骤">步骤</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#master-主备切换">master 主备切换</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#spark-on-yarn-环境搭建">Spark On YARN 环境搭建</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><blockquote><p>内容包括：Spark 搭建本地模式，集群模式（StandAlon, YARN）搭建；认识 Spark 框架、运行逻辑；认识 PySpark 下 SparkCore、SparkSQL 操作。</p><p>python 上 Spark 不断个更新，实时关注 <a href="https://spark.apache.org/docs/3.1.2/api/python/user_guide/index.html" target="_blank" rel="noopener noreferrer">官方文档<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><!--more--><h1 id="概述" tabindex="-1"><a class="header-anchor" href="#概述" aria-hidden="true">#</a> 概述</h1><p>Spark 用于大规模数据处理的统一分析引擎。其特点就是对任意类型的数据进行自定义计算。 RDD 是一种分布式内存抽象，使程序员能够在大规模集群中做内存运算，并且有一定的容错方式。这也是 Spark 的核心数据结构。<a href="https://zhuanlan.zhihu.com/p/34436165" target="_blank" rel="noopener noreferrer">知乎解答 - spark 概述<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h3 id="spark-与-hadoop" tabindex="-1"><a class="header-anchor" href="#spark-与-hadoop" aria-hidden="true">#</a> <strong>Spark 与 hadoop</strong></h3><p>Spark 仅做计算，而 Hadoop 生态圈不仅有计算（MR）也有存储（HDFS）和资源管理调度（YARN），HDFS 和 YARN 仍是许多大数据体系的核心架构。在计算层面，Spark 相比较 MR（MapReduce）有巨大的性能优势，但至今仍有许多计算工具基于 MR 构架，比如非常成熟的 Hive</p><h3 id="spark-框架模型" tabindex="-1"><a class="header-anchor" href="#spark-框架模型" aria-hidden="true">#</a> spark 框架模型</h3><p><strong>Spark Core：</strong> Spark 的核心，Spark 核心功能均由 Spark Core 模块提供，是 Spark 运行的基础。Spark Core 以 RDD 为数据抽象，提供 Python、Java、Scala、R 语言的 API，可以编程进行海量离线数据批处理计算。</p><p><strong>SparkSQL：</strong> 基于 SparkCore 之上，提供结构化数据的处理模块。SparkSQL 支持以 SQL 语言对数据进行处理，SparkSQL 本身针对离线计算场景。同时基于 SparkSQL，Spark 提供了 StructuredStreaming 模块，可以以 SparkSQL 为基础，进行数据的流式计算。</p><p><strong>SparkStreaming：</strong> 以 SparkCore 为基础，提供数据的流式计算功能。</p><p><strong>MLlib：</strong> 以 SparkCore 为基础，进行机器学习计算，内置了大量的机器学习库和 API 算法等。方便用户以分布式计算的模式进行机器学习计算。</p><p><strong>GraphX：</strong> 以 SparkCore 为基础，进行图计算，提供了大量的图计算 API，方便用于以分布式计算模式进行图计算。</p><h3 id="spark-架构角色" tabindex="-1"><a class="header-anchor" href="#spark-架构角色" aria-hidden="true">#</a> Spark 架构角色</h3><p><strong>资源管理层面：</strong> 管理集群资源：Master 管理单个服务器资源：worker</p><p><strong>任务执行层面：</strong> 管理单个 spark 任务在运行的时候的工作：Driver 单个任务运行时的工作者： Executor</p><h1 id="快速开始" tabindex="-1"><a class="header-anchor" href="#快速开始" aria-hidden="true">#</a> 快速开始</h1><p>根据 <a href="https://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="noopener noreferrer">官方指南<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>， spark 提供了多种运行模式：包括本地，集群（StandAlone, YARN, K8S），云模式。</p><h2 id="本地-local-模式配置" tabindex="-1"><a class="header-anchor" href="#本地-local-模式配置" aria-hidden="true">#</a> 本地 local 模式配置</h2><p>Local 模式搭建方便，是学习 Spark 入门操作应用的首选模式。Local 模式的本质是启动一个 JVM Process 进程(一个进程里面有多个线程)，执行任务 Task。Local 模式可以使用<code>Local[N]</code> 或 <code>Local[*]</code> 限制模拟 Spark 集群环境的线程数量。N 为线程数，<code>*</code> 为 CPU 最大核心数。 在 local 模式下，全部四种 spark 角色都为 jvm 进程本身，且只能运行一个 Spark 程序。Local 模式可以做到开箱即用：解压从官网下载的 Spark 安装包：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">tar</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-zxvf</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-3.2.0-bin-hadoop3.2.tgz</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-C</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/export/server/</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>配置 Spark 需要调整以下 5 个环境变量：<code>SPARK_HOME</code>,<code>PYSPARK_PYTHON</code>,<code>JAVA_HOME</code>,<code>HADOOP_CONF_DIR</code>,<code>HADOOP_HOME</code></p><h3 id="spark-操作介绍" tabindex="-1"><a class="header-anchor" href="#spark-操作介绍" aria-hidden="true">#</a> Spark 操作介绍</h3><p>配置好后测试环境，分别运行：</p><p><code>bin/pyspark</code> 可以提供一个 <code>交互式</code>的 Python 解释器环境, 在这里面可以写普通 python 代码。添加 <code>--master local[*]</code> 参数控制使用线程数量。</p><p><code>bin/spark-shell</code> 提供交互式解析器环境，运行 scala 程序代码。</p><p><code>bin/spark-submit</code> 提交指定的 Spark 代码到 Spark 环境中运行。如 <code>bin/spark-submit /export/server/spark/examples/src/main/python/pi.py 10</code></p><h3 id="spark-端口" tabindex="-1"><a class="header-anchor" href="#spark-端口" aria-hidden="true">#</a> Spark 端口</h3><p><strong>4040:</strong> 一个运行的 Application 在运行的过程中临时绑定的端口，用以查看当前任务的状态，当前程序运行完成后,，4040 就会被注销。4040 被占用会顺延到 4041.4042 等</p><p><strong>8080</strong> : 默认是 StandAlone 下, Master 角色(进程)的 WEB 端口,用以查看当前 Master(集群)的状态</p><p><strong>18080/9870</strong> : 默认是历史服务器的端口, 回看某个程序的运行状态就可以通过历史服务器查看,历史服务器长期稳定运行,可供随时查看被记录的程序的运行过程。</p><h2 id="pyspark" tabindex="-1"><a class="header-anchor" href="#pyspark" aria-hidden="true">#</a> PySpark</h2><p><a href="https://spark.apache.org/docs/3.1.2/rdd-programming-guide.html" target="_blank" rel="noopener noreferrer">官方文档<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>对于大规模数据集处理，安装直接使用 <code>pip install pyspark</code> 即可。</p><h3 id="初体验-wordcount-实例" tabindex="-1"><a class="header-anchor" href="#初体验-wordcount-实例" aria-hidden="true">#</a> 初体验：WordCount 实例</h3><p><strong>初始化</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">conf </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> SparkConf().setAppName(appName).setMaster(master)</span></span>
<span class="line"><span style="color:#24292E;">sc </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> SparkContext(</span><span style="color:#E36209;">conf</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">conf)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>local 模式的话 <code>master=&#39;local[n]&#39;</code></p><p><strong>读取数据到 RDD 对象：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">wordsRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.textFile(</span><span style="color:#032F62;">&quot;hdfs://node1:8020/input/words.txt&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#6A737D;"># 本地文件读取：file:///home/data/word.txt</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>RDD 对象处理</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">flatMapRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> wordsRDD.flatMap(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> line: line.split(</span><span style="color:#032F62;">&quot; &quot;</span><span style="color:#24292E;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">mapRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> flatMapRDD.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: (x, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 以上步骤将在不同的集群节点上执行，参考下图。因此可以并行执行，加快效率。</span></span>
<span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> mapRDD.reduceByKey(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> a, b: a </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> b)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">res_rdd_col2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> resultRDD.collect()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="/assets/img/spark/image-20220401122356005.png" alt="image-20220401122356005" tabindex="0" loading="lazy"><figcaption>image-20220401122356005</figcaption></figure><p>将结果写入文件并通过 spark 储存于 HDFS：<code>resultRDD.saveAsTextFile(&quot;hdfs://node1:8020/words_new.txt&quot;)</code></p><p>提交代码到集群运行，需要修改初始化方式：<code>conf = SparkConf().setAppName(appName)</code></p><p>而后通过 <code>bin/spark-submit</code> 提交到集群运行：通过 <code>--py-files</code> 提交依赖文件，可以单个文件 <code>.py</code> 或者多个文件 <code>.zip</code>。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">/export/server/spark/bin/spark-submit</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">yarn</span><span style="color:#24292E;"> </span><span style="color:#032F62;">./test.py</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>该 python 代码底层由 java 实现（通过 py4j 交互)</p><h2 id="spark-core-api" tabindex="-1"><a class="header-anchor" href="#spark-core-api" aria-hidden="true">#</a> Spark Core API</h2><p>分布式框架中，需要由同意的数据结构对象来实现分布式计算所需功能，这个对象就是 <strong>RDD（Resilient Distributed Dataset）</strong> 。在初体验中 <code>wordsRDD = sc.textFile(&quot;hdfs://node1:8020/input/words.txt&quot;)</code> 读取的就是 RDD 对象。RDD 是通过 java 实现的抽象类、泛型类型。特征包括：</p><ul><li><strong>有分区（RDD is a list of partitions）</strong></li></ul><p>RDD 的分区是 RDD 数据存储的最小单位，如：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">7</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">8</span><span style="color:#24292E;">], </span><span style="color:#E36209;">numSlices</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">3</span><span style="color:#24292E;">).glom().collect()</span></span>
<span class="line"><span style="color:#6A737D;"># [[1, 2], [3, 4], [5, 6, 7, 8]] </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>分区为 3 时，RDD 内数据分为 3 份。</p><ul><li><strong>可并行计算，计算方法会作用到每一个分区上</strong></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">7</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">8</span><span style="color:#24292E;">], </span><span style="color:#E36209;">numSlices</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">3</span><span style="color:#24292E;">).map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: x</span><span style="color:#D73A49;">*</span><span style="color:#005CC5;">10</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">   </span><span style="color:#005CC5;">print</span><span style="color:#24292E;">(resultRDD.glom().collect())</span></span>
<span class="line"><span style="color:#6A737D;"># [[10, 20], [30, 40], [50, 60, 70, 80]]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>RDD 之间相互依赖、迭代的；</strong></li></ul><p>针对快速探索中的 wordcount 案例，每个操作步骤都是一次对 RDD 的迭代。一个新的 RDD 由上一步骤的 RDD 计算得来。</p><ul><li><strong>KV 型 RDD 可以由分区器；</strong></li></ul><p>对 key-value 型的 RDD，默认采用 Hash 分区，可以手动使用 <code>rdd.partitionBy</code> 设置分区。</p><ul><li><strong>分区的计算会尽量选择靠近数据所在地</strong></li></ul><p>目的是最大化性能，毕竟本地读取效率是大于网络读取的。</p><h3 id="创建-rdd" tabindex="-1"><a class="header-anchor" href="#创建-rdd" aria-hidden="true">#</a> 创建 RDD</h3><p>可以从文件加载：<code>sc.textFile(&quot;hdfs://node1:8020/input/words.txt&quot;)</code>，或者从本地的集合对象（如 list）创建：<code>sc.parallelize(set(), numSlices=3)</code>，从文件夹加载：<code>sc.wholeTextFiles(&quot;hdfs://node1:8020/input&quot;)</code></p><h3 id="rdd-算子" tabindex="-1"><a class="header-anchor" href="#rdd-算子" aria-hidden="true">#</a> RDD 算子</h3><p>详细算子 <a href="https://spark.apache.org/docs/3.1.2/rdd-programming-guide.html#" target="_blank" rel="noopener noreferrer">官网<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 可查看，以下总结一些常用的。</p><h4 id="transformation-算子" tabindex="-1"><a class="header-anchor" href="#transformation-算子" aria-hidden="true">#</a> <strong>Transformation 算子</strong></h4><p>返回一个 RDD，如果没有 action 算子，那么这类算子是不工作的。</p><p><code>flatMap()</code>：对 rdd 进行 map 后接触嵌套。如：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#032F62;">&quot;hello hello&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;nihao nihao &quot;</span><span style="color:#24292E;">])</span></span>
<span class="line"><span style="color:#24292E;">resultRDD.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:x.split()).collect()</span></span>
<span class="line"><span style="color:#6A737D;"># [[&#39;hello&#39;, &#39;hello&#39;], [&#39;nihao&#39;, &#39;nihao&#39;]]</span></span>
<span class="line"><span style="color:#24292E;">mappedRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> resultRDD.flatMap(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:x.split())</span></span>
<span class="line"><span style="color:#6A737D;"># [&#39;hello&#39;, &#39;hello&#39;, &#39;nihao&#39;, &#39;nihao&#39;]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>reduceByKey()</code>：真能对 KV 型 RDD，对组内数据根据 key 分类并聚合。如：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 接 flatmap() 例子</span></span>
<span class="line"><span style="color:#24292E;">mappedRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> mappedRDD.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: (x, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">))</span></span>
<span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> mappedRDD.reduceByKey(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> a, b: a </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> b)</span></span>
<span class="line"><span style="color:#6A737D;"># [(&#39;nihao&#39;, 2), (&#39;hello&#39;, 2)]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>groupBy()</code>：将元素分组，分组后每个组是一个 KV，key 为分组 <code>func</code> 的返回值，value 为该组元素构成的迭代器。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">dataRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">7</span><span style="color:#24292E;">])</span></span>
<span class="line"><span style="color:#24292E;">mappedRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dataRDD.groupBy(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: </span><span style="color:#032F62;">&quot;even&quot;</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> x </span><span style="color:#D73A49;">%</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">else</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;odd&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> mappedRDD.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:(x[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">list</span><span style="color:#24292E;">(x[</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">])))</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(resultRDD.collect())</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>Filter(func)</code>：过滤元素，传入的函数方法需要返回布尔值。</p><p><code>distinct()</code>：数据去重，一般不需要传入参数。</p><p><code>union()</code>：合并两个 RDD 并返回，支持不同各类型。<code>union_rdd = rdd1.union(rdd2)</code></p><p><code>join()</code>：同 sql 的 join。支持左右拼接，<code>leftOuterJoin()</code>, <code>rightOuterJoin()</code></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">dataRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([(</span><span style="color:#032F62;">&quot;hello&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">), (</span><span style="color:#032F62;">&quot;good&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">)])</span></span>
<span class="line"><span style="color:#24292E;">dataRDD2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([(</span><span style="color:#032F62;">&quot;hello&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">10</span><span style="color:#24292E;">), (</span><span style="color:#032F62;">&quot;good&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">20</span><span style="color:#24292E;">), (</span><span style="color:#032F62;">&quot;world&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">30</span><span style="color:#24292E;">)])</span></span>
<span class="line"><span style="color:#24292E;">resultRDD </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dataRDD.join(dataRDD2)</span></span>
<span class="line"><span style="color:#6A737D;"># [(&#39;good&#39;, (2, 20)), (&#39;hello&#39;, (1, 10))]</span></span>
<span class="line"><span style="color:#6A737D;"># rightOuterJoin 后：[(&#39;good&#39;, (2, 20)), (&#39;hello&#39;, (1, 10)), (&#39;world&#39;, (None, 30))]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>intersection()</code>：返回交集<code>rdd1.intersection(rdd2)</code></p><p><code>glom()</code> ：将 RDD 数据根据分区进行嵌套。</p><p><code>groupByKey()</code>：针对 KV 型 RDD，根据 key 分组，但不聚合，类似 <code>grouBy()</code>。使用<code>groupByKey()</code> + 聚合的性能是远差与直接使用 <code>reduceByKey()</code> 的。</p><p><code>sortByKey()</code> 或 <code>sortBy(func, ascending=False, numPartitions=1)</code>：排序 <code>func</code> 为排序依据。 <strong>如果要全局有序，排序分区数需要设置为 1。</strong></p><p><code>mapPartition(func)</code>：<code>func</code> 输入可迭代对象，输出可迭代对象。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">func</span><span style="color:#24292E;">(iter_tool):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> x </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> iter_tool:</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">yield</span><span style="color:#24292E;"> x</span><span style="color:#D73A49;">+</span><span style="color:#005CC5;">1</span></span>
<span class="line"><span style="color:#24292E;">rdd1 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(rdd1.mapPartitions(func).glom().collect())</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>partitionBy(func)</code>：<code>func</code> 参数应该为元素 hash 值到分区编号的映射。只能根据 hash 分区，因此非 KV 型 RDD 需要先进行转换。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">rdd1 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">7</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">8</span><span style="color:#24292E;">]).map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:(x,x))</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(rdd1.partitionBy(</span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: x </span><span style="color:#D73A49;">%</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">).glom().collect())</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><code>repartition(N)</code>：将 RDD 重新分为 N 个分区，一般除了要全局排序外，不会进行充分区。</p><p><code>coalesce()</code>：对分区数量进行加减，比<code>repartition()</code>常用，<code>rdd.coalesce(4,shuffle=True)</code> 如果 <code>shuffle</code> 为 <code>Flase</code>，那么将忽略分区增加操作，仅支持分区减少。 <strong>尽量不要增加分区，可能破坏内存迭代的计算管道。</strong></p><p><code>mapValues(func)</code>：仅针对 KV 型 RDD，功能等价于 <code>.map(lambda x:(x[0],func(x[1])))</code></p><h4 id="action-算子" tabindex="-1"><a class="header-anchor" href="#action-算子" aria-hidden="true">#</a> <strong>Action 算子</strong></h4><p><code>countByKey()</code>：返回一个 <code>collections.defaultdict</code></p><p><code>collect()</code>：统一各分区的数据，形成一个<code>List</code>。 <strong>谨慎使用：结果数据集太大的话，Driver 内存会爆炸。</strong></p><p><code>reduce(func)</code>：迭代地减小数据维度：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">]).reduce(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> a, b: a </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> b)</span></span>
<span class="line"><span style="color:#6A737D;"># 返回 15</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><code>fold()</code>：分别对各分区进行 reduce，然后聚合。reduce 是有初始值的：如：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">rdd1 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">5</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">6</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">3</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(rdd1.glom().collect())</span></span>
<span class="line"><span style="color:#6A737D;"># [[1, 2], [3, 4], [5, 6]]</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(rdd1.fold(</span><span style="color:#005CC5;">10</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> a, b: a </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> b))  </span></span>
<span class="line"><span style="color:#6A737D;"># (1+2+10) + (3+4+10)+(5+6+10) = 61</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>first()</code>：返回第一个元素。</p><p><code>take(N)</code>：返回前 N 个元素，<code>List</code> 形式储存。</p><p><code>top(N)</code>：返回降序排序后的前 N 个值，<code>List</code> 储存。</p><p><code>count()</code>：返回 RDD 数据数量，<code>int</code>。</p><p><code>takeSample(withReplacement=True, num=10, seed=7))</code>：随机抽取 n 个样本。</p><p><code>takeOrdered(num, key=func)</code>：返回 <code>List</code> 为根据 <code>func()</code> 排序后的前 <code>num</code> 个元素</p><p><code>foreach(func)</code>：迭代对所有元素进行处理。该执行不经过 Driver。</p><p><code>saveAsTextFile()</code>：将 RDD 写入文本文件中。由 Executor 直接执行，执行结果不会发送到 Driver。</p><p><code>foreachPartition()</code>：与 <code>foreach()</code> 类似，但调用一次函数处理一个分区的数据。</p><h3 id="rdd-持久化" tabindex="-1"><a class="header-anchor" href="#rdd-持久化" aria-hidden="true">#</a> RDD 持久化</h3><p>spark 提供了 RDD 缓存 API 以减少重复计算。可以使用 <code>persist()</code> 或 <code>cache()</code> 来缓存，一般用的较多的是：<code>persist(StorageLevel.MEMORY_AND_DISK)</code> 。对于 python，都是用 Pickle 进行序列化缓存，更多缓存等级参考：<a href="https://spark.apache.org/docs/3.1.2/rdd-programming-guide.html#rdd-persistence" target="_blank" rel="noopener noreferrer">官方<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。主动清理缓存的 API：<code>rdd.unpersist()</code>。 <strong>缓存并不是安全的，内存不足/断电/硬盘损坏等都可能造成数据出错。</strong></p><p>RDD 的 CheckPoint 被设计认为是安全的（不排除物理因素破坏数据），仅支持硬盘储存。checkpoint 可以选择数据备份地址，因此可以存储在 HDFS 中；性能比缓存更好；不管分区多少，风险是一样的（不同于缓存）。</p><p>配置 checkpoint 目标地址：<code>sc.setCheckpointDir(&quot;hdfs://node1:8020/output/bj52kp&quot;)</code>；保存时直接调用：<code>rdd.checkpoint()</code></p><p><strong>缓存与 checkpoint 都不是 action 操作，所以后面要加上 Action。</strong></p><h3 id="共享变量" tabindex="-1"><a class="header-anchor" href="#共享变量" aria-hidden="true">#</a> 共享变量</h3><p>每个 excutor 进程可以处理多个分区。当 excutor 处理 RDD 时需要某些 python 变量（RDD 外的 python 变量都由 Driver 处理与储存。），Driver 需要为每个分区发送一份数据。因此每个 excutor 中就有多个相同数据，造成额外网络 IO 开销与内存浪费。</p><p><strong>广播变量：</strong> 将 python 变量数据 <code>python_val</code> 标记为广播变量：<code>broadcase = sc.broadcast(python_val)</code>。使用时：<code>broadcast.value</code> 提取数据。</p><p><strong>累加器：</strong> 需求：当不同分区运行时，他们需要对同一个全局变量进行操作。python 的 <code>global</code> 无法满足需求，由于机器不同，无法通过指针等实现。需要使用 spark 提供的累加器。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">acmlt </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.accumulator(</span><span style="color:#005CC5;">66</span><span style="color:#24292E;">)  </span><span style="color:#6A737D;"># 初始化值为 acmlt=66</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">map_func</span><span style="color:#24292E;">(data):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">global</span><span style="color:#24292E;"> acmlt</span></span>
<span class="line"><span style="color:#24292E;">    acmlt </span><span style="color:#D73A49;">+=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>累加器需注意：</strong> 可能需要加缓存来解决以下问题。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">rdd </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.parallelize([</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">2</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">3</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">4</span><span style="color:#24292E;">],</span><span style="color:#005CC5;">2</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">acmlt </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.accumulator(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">rdd2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> rdd.map(map_func)</span></span>
<span class="line"><span style="color:#24292E;">rdd2.collect()  </span><span style="color:#6A737D;"># rdd2 action 后不保存数据。此时 acmlt 为 4</span></span>
<span class="line"><span style="color:#24292E;">rdd3 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> rdd2.map(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x:x)  </span><span style="color:#6A737D;"># rdd3 构造需要再次构造 rdd2，此时 acmlt 为 8.</span></span>
<span class="line"><span style="color:#24292E;">rdd3.collect() </span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(accmlt)  </span><span style="color:#6A737D;"># 8</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="内核调度" tabindex="-1"><a class="header-anchor" href="#内核调度" aria-hidden="true">#</a> 内核调度</h3><p>根据 RDD 的迭代特性，程序的整个计算路程可以通过 <strong>DAG</strong> 有向无环图表示。每个 Action 操作的执行都会触发该一个 JOB 来计算 Action 之前的子图。<code>1 个 action=1 个有向无环图=1 个 JOB</code>。如果有 3 个 action，那么代码就产生 3 个 JOB，这些 JOB，Spark 称为 Application。</p><p>DAG 中 RDD 节点之间的关系分为： <strong>窄依赖</strong> ：父 RDD 的一个分区，全部将数据发给子 RDD 的一个分区。 <strong>宽依赖/shuffle</strong> ：父 RDD 的一个分区，将数据发给子 RDD 的多个分区。</p><p><strong>stage 根据宽依赖进行划分</strong> ，因此每个 stage 内部都是窄依赖。</p><p>每个 task 是一个线程（DAG 上呈现为，一个 stage 中的一条来连通的计算流程线），线程内是纯内存计算，所有线程并行计算，并行程度受全局并行度 <code>spark.default.parallelism</code>、分区数量影响。 <strong>spark 一般不要再算子上设置并行度，除了部分排序算子，分区数量让系统自动设置即可</strong> 。</p><p>集群中，并行度可以设置为 CPU 总核心的 2 到 10 倍。<code>spark.default.parallelism=1000</code>。</p><p>基于以上分析，spark 程序的调度流程为：</p><ul><li>构建 Driver</li><li>构建 SparkContext</li><li>产生 DAG，基于 <strong>DAG Scheduler</strong> 构建逻辑 task 分配</li><li>基于 <strong>Task Scheduler</strong> ，将 Task 分配到 executor 上工作并监督他们，executor 工作时汇报进度。</li></ul><p>其中 DAG Scheduler 与 Task Scheduler 为 Driver 内部的两个组件。</p><h3 id="总结-spark-层级" tabindex="-1"><a class="header-anchor" href="#总结-spark-层级" aria-hidden="true">#</a> 总结 Spark 层级</h3><p>一个 Spark Application （如 bin/pyspark）中，包含多个 Job，每个 Job 由多个 Stage 组成，每个 Job 执行按照 DAG 图进行。每个 Stage 中包含多个 Task 任务，每个 Task 以线程 Thread 方式执行，需要 1Core CPU。</p><ul><li><strong>Job：</strong> 由多个 Task 的并行计算部分，一般 Spark 中的 action 操作（如 save、collect），会生成一个 Job。</li><li><strong>Stage：</strong> Job 的组成单位，一个 Job 会切分成多个 Stage，Stage 彼此之间相互依赖顺序执行，而每个 Stage 是多个 Task 的集合，类似 map 和 reduce stage。</li><li><strong>Task：</strong> 被分配到各个 Executor 的单位工作内容，它是 Spark 中的最小执行单位，一般来说有多少个 Paritition（物理层面的概念，即分支可以理解为将数据划分成不同部分并行处理），就会有多少个 Task，每个 Task 只会处理单一分支上的数据。</li></ul><p>在 18080 中选择 刚刚提交的 pi 计算任务。选择 Executors 查看。从 页面结果可以看出，Spark Application 运行到集群上时，由两部分组成：Driver Program 和 Executors</p><h3 id="spark-shuffle" tabindex="-1"><a class="header-anchor" href="#spark-shuffle" aria-hidden="true">#</a> Spark Shuffle</h3><p>DAG 将一个 Job 划分为多个 Stage，若用 map 或 reduce 来标注每个 Stage，Spark Shuffle 的作用时将 map 的输出对应到 reduce 上。shuffle 分为 shuffle write（map 的最后一步） 与 shuffle read（reduce 的第一步）。因此数据流大致为 <code> stage1 - partition - stage2</code>。</p><h2 id="sparksql" tabindex="-1"><a class="header-anchor" href="#sparksql" aria-hidden="true">#</a> SparkSQL</h2><p>用于存储海量结构化数据。支持 SQL，HIVE 等。SparkSQL 与 HIVE 都为分布式 SQL 计算引擎，SparkSQL 具有更好的性能。SparkSQL 中共有 DataSet、DataFrame 对象。Python 仅支持 DataFrame 对象，即一助攻二维表结构数据。</p><h3 id="快速体验" tabindex="-1"><a class="header-anchor" href="#快速体验" aria-hidden="true">#</a> 快速体验</h3><p>Spark 2.0 后，推出了 SparkSession 统一编码入口对象，支持 RDD 编程与 SparkSQL 编程。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> pyspark.sql </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> SparkSession</span></span>
<span class="line"><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__name__</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&#39;__main__&#39;</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">    spark </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> SparkSession.builder. \</span></span>
<span class="line"><span style="color:#24292E;">        appName(</span><span style="color:#032F62;">&quot;local[*]&quot;</span><span style="color:#24292E;">). \</span></span>
<span class="line"><span style="color:#24292E;">        config(</span><span style="color:#032F62;">&quot;spark.sql.shuffle.partitions&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;4&quot;</span><span style="color:#24292E;">). \</span></span>
<span class="line"><span style="color:#24292E;">        getOrCreate()</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># appName 设置程序名称, config 设置一些常用属性</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 最后通过 getOrCreate()方法创建 SparkSession 对象</span></span>
<span class="line"><span style="color:#24292E;">    df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.read.csv(</span><span style="color:#032F62;">&#39;file:///home/data/sql/stu_score.txt&#39;</span><span style="color:#24292E;">, </span><span style="color:#E36209;">sep</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&#39;,&#39;</span><span style="color:#24292E;">, </span><span style="color:#E36209;">header</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">    df2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> df.toDF(</span><span style="color:#032F62;">&#39;id&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;name&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;score&#39;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">    df2.printSchema()</span></span>
<span class="line"><span style="color:#24292E;">    df2.show()</span></span>
<span class="line"><span style="color:#24292E;">    df2.createTempView(</span><span style="color:#032F62;">&quot;score&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    spark.sql(</span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">    SELECT * FROM score WHERE name=&#39;语文&#39; LIMIT 5</span></span>
<span class="line"><span style="color:#032F62;">    &quot;&quot;&quot;</span><span style="color:#24292E;">).show()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="dataframe" tabindex="-1"><a class="header-anchor" href="#dataframe" aria-hidden="true">#</a> DataFrame</h3><p>DataFrame 为二维表结构，其中储存四个对象：</p><p><strong>StructType</strong> ：整个表结构的信息 <strong>StructField</strong> ：描述列的信息 <strong>Row</strong> ：行数据 <strong>Column</strong> ：列数据以及列的信息</p><h4 id="创建-dataframe" tabindex="-1"><a class="header-anchor" href="#创建-dataframe" aria-hidden="true">#</a> 创建 DataFrame</h4><p>从 RDD 创建，数据类型根据 RDD 推断。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">sc </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.sparkContext</span></span>
<span class="line"><span style="color:#24292E;">rdd </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sc.textFile(</span><span style="color:#032F62;">&quot;../data/sql/people.txt&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#005CC5;">map</span><span style="color:#24292E;">(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: x.split(</span><span style="color:#032F62;">&#39;,&#39;</span><span style="color:#24292E;">)).\</span></span>
<span class="line"><span style="color:#005CC5;">map</span><span style="color:#24292E;">(</span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: [x[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">int</span><span style="color:#24292E;">(x[</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">])])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.createDataFrame(rdd, </span><span style="color:#E36209;">schema</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> [</span><span style="color:#032F62;">&#39;name&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;age&#39;</span><span style="color:#24292E;">])</span></span>
<span class="line"><span style="color:#6A737D;"># 或 df = rdd.toDF([&#39;name&#39;, &#39;age&#39;])</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中的 <code>schema</code> 参数可以通过 <code>StructType</code> 定义：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> pyspark.sql.types </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> StructType, StringType, IntegerType</span></span>
<span class="line"><span style="color:#24292E;">schema </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> StructType().\</span></span>
<span class="line"><span style="color:#24292E;">add(</span><span style="color:#032F62;">&quot;id&quot;</span><span style="color:#24292E;">, IntegerType(), </span><span style="color:#E36209;">nullable</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">add(</span><span style="color:#032F62;">&quot;name&quot;</span><span style="color:#24292E;">, StringType(), </span><span style="color:#E36209;">nullable</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">add(</span><span style="color:#032F62;">&quot;score&quot;</span><span style="color:#24292E;">, IntegerType(), </span><span style="color:#E36209;">nullable</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>从 <code>pd.DataFrame</code> 创建：直接使用 <code>spark_df = spark.createDataFrame(p_df)</code></p><p>从外部文件读取：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">schema </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> StructType().add(</span><span style="color:#032F62;">&quot;data&quot;</span><span style="color:#24292E;">, StringType(), </span><span style="color:#E36209;">nullable</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.read.format(</span><span style="color:#032F62;">&quot;text&quot;</span><span style="color:#24292E;">)\</span></span>
<span class="line"><span style="color:#24292E;">.schema(schema)\</span></span>
<span class="line"><span style="color:#24292E;">.load(</span><span style="color:#032F62;">&quot;../data/sql/people.txt&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>一般读取的时 <code>json</code>、<code>parquet</code> 类型的话，不需要提供<code>schema</code>。</p><p>对于 CSV 等，可能需要提供 <code>option</code> 参数。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.read.format(</span><span style="color:#032F62;">&quot;csv&quot;</span><span style="color:#24292E;">)\</span></span>
<span class="line"><span style="color:#24292E;">.option(</span><span style="color:#032F62;">&quot;sep&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;;&quot;</span><span style="color:#24292E;">)\</span></span>
<span class="line"><span style="color:#24292E;">.option(</span><span style="color:#032F62;">&quot;header&quot;</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)\</span></span>
<span class="line"><span style="color:#24292E;">.option(</span><span style="color:#032F62;">&quot;encoding&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;utf-8&quot;</span><span style="color:#24292E;">)\</span></span>
<span class="line"><span style="color:#24292E;">.schema(</span><span style="color:#032F62;">&quot;name STRING, age INT, job STRING&quot;</span><span style="color:#24292E;">)\</span></span>
<span class="line"><span style="color:#24292E;">.load(</span><span style="color:#032F62;">&quot;../data/sql/people.csv&quot;</span><span style="color:#24292E;">)</span><span style="color:#6A737D;">#</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="dataframe-操作" tabindex="-1"><a class="header-anchor" href="#dataframe-操作" aria-hidden="true">#</a> DataFrame 操作</h4><p>DataFrame 可以通过 <code>sparksession.sql()</code> 直接操作，使用 sql 命令需要先注册成临时表：<code>df.createTempView(&quot;temp_name&quot;)</code>，以下展示部分处 <code>sparksession.sql()</code> 外的常用操作，详细查看 <a href="https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame" target="_blank" rel="noopener noreferrer">官方 API<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><code>.show(n)</code>，<code>.printSchema()</code>：查看数据信息。</p><p><code>.select()</code>：选择指定的列，传入列名或 Column 对象。</p><p><code>.filter(condition)</code>，<code>.where()</code>：筛选行，条件如：布尔值的列数据<code>df[&#39;score&#39;]&lt;9</code> 或 SQL 风格<code>&#39;score&#39;&lt;9</code></p><p><code>.groupBy()</code>：按列分组，传入列名或列对象。返回 <code>GroupedData</code> 对象</p><p><code>from pyspark.sql import functions as F</code> 提供了基础表数据计算功能，如 <code>F.round()</code>, <code>F.avg()</code>, <code>F.min()</code>, <code>F.count</code> 等来直接对二维数据进行计算。</p><p><code>.dropDuplicates()</code>：默认对整体去重，传入需要去重的列。</p><p><code>.dropna(thresh, subset)</code>：针对 <code>subset</code> 给出的列，有效信息至少为 <code>thresh</code> 个才保留该行数据。</p><p><code>.fillna()</code>： 传入一个填充规则，表示每个列的填充值<code>{&#39;name&#39;:&quot;unk&quot;,&quot;job&quot;:0}</code></p><h4 id="读写数据" tabindex="-1"><a class="header-anchor" href="#读写数据" aria-hidden="true">#</a> 读写数据</h4><p>写出为文件：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">df.write.mode(</span><span style="color:#032F62;">&quot;overwrite&quot;</span><span style="color:#24292E;">).format(</span><span style="color:#032F62;">&quot;csv&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;sep&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;,&quot;</span><span style="color:#24292E;">).option(</span><span style="color:#032F62;">&quot;header&quot;</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">save(</span><span style="color:#032F62;">&quot;../mydata.csv&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>对于 <code>json</code>格式直接写出，不需要 <code>option</code>，默认的文件写出格式为 <code>parquet</code>。</p><p><strong>使用 JDBC 读写数据库</strong> ，需要驱动<code>mysql-connector-java-5.1.41-bin.jar</code> 不同 mysql 对应版本不同。jar 包存放地址：<code>py 解析器环境/lib/python3.8/site-packages/pyspark/jars</code></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">df.write.mode(</span><span style="color:#032F62;">&quot;overwrite&quot;</span><span style="color:#24292E;">).format(</span><span style="color:#032F62;">&quot;jdbc&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;url&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;jdbc:mysql://node1:3306/databese?useSSL=false&amp;useUnicode=true&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;dbtable&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;u_data&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;user&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;root&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;password&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;123456&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">save()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>jdbc</code> 连接字符串中，建议使用 <code>useSSL=false</code> 保证正常连接，<code>useUnicode=true</code> 保证传输无乱码。<code>dbtable</code> 为写出的表名。</p><p><strong>使用 JDBC 读取数据库：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">df </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> read.format(</span><span style="color:#032F62;">&quot;jdbc&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;url&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;jdbc:mysql://node1:3306/databese?useSSL=false&amp;useUnicode=true&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;dbtable&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;u_data&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;user&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;root&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">option(</span><span style="color:#032F62;">&quot;password&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;123456&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">load()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="sparksql-定义-udf" tabindex="-1"><a class="header-anchor" href="#sparksql-定义-udf" aria-hidden="true">#</a> SparkSQL 定义 UDF</h3><p>User-Define-Function，对 python 函数进行注册，返回可用于 DSL 的函数操作，注册的函数名称可用于 SQL 风格。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">udf </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> spark.udf.register(</span><span style="color:#032F62;">&quot;udf1&quot;</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">lambda</span><span style="color:#24292E;"> x: x </span><span style="color:#D73A49;">&lt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">10</span><span style="color:#24292E;">, BooleanType())</span></span>
<span class="line"><span style="color:#24292E;">df.filter(udf(df[</span><span style="color:#032F62;">&#39;age&#39;</span><span style="color:#24292E;">])).show()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>对于传递的返回类型参数，需要从 <code>pyspark.sql.types</code> 中选取，数组类型可用 <code>ArrayType(StringType())</code>（即 python 中的 <code>List(Str)</code>）；字典类型使用 <code>StructType()</code>，此处需要提前声明 <code>StructTrype()</code> 中的结构体信息。</p><h3 id="窗口函数" tabindex="-1"><a class="header-anchor" href="#窗口函数" aria-hidden="true">#</a> 窗口函数</h3><p>可以将聚合前与聚合后的数据显示在同一个表中。</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;"># 在 </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> 数据后追加一列窗口，代表学生的平均成绩</span></span>
<span class="line"><span style="color:#005CC5;">spark</span><span style="color:#24292E;">.</span><span style="color:#005CC5;">sql</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">          SELECT *, AVG(SCORE) OVER() AS avg_score FROM stu</span></span>
<span class="line"><span style="color:#032F62;">          &quot;&quot;&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">          </span></span>
<span class="line"><span style="color:#24292E;"># 聚合类型 SUM</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">MIN</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">MAX</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">AVG</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">COUNT</span></span>
<span class="line"><span style="color:#005CC5;">SUM</span><span style="color:#24292E;">() </span><span style="color:#D73A49;">OVER</span><span style="color:#24292E;">([PARTITION BY XX][ORDER BY XX])</span></span>
<span class="line"><span style="color:#24292E;"># 排序类型 RANK</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">ROW_NUMBER</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">DENSE_RANK</span></span>
<span class="line"><span style="color:#005CC5;">RANK</span><span style="color:#24292E;">() </span><span style="color:#D73A49;">OVER</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;"># 分区类型 NTILE</span></span>
<span class="line"><span style="color:#005CC5;">NTILE</span><span style="color:#24292E;">(</span><span style="color:#D73A49;">number</span><span style="color:#24292E;">) </span><span style="color:#D73A49;">OVER</span><span style="color:#24292E;">() </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="sql-流程" tabindex="-1"><a class="header-anchor" href="#sql-流程" aria-hidden="true">#</a> SQL 流程</h3><p>SparkSQL 可以自动优化（依赖于 Catalyst 优化器），提升代码运行效率。SparkSQL 接收到 SQL 语句后，通过 Catalyst 解析，并生成 RDD 执行计划，而后交给集群执行。</p><p><strong>Catalyst 优化：</strong></p><p>首先生成 AST 抽象语法树，在 AST 中加入元数据信息（以供优化）。优化方式包括：</p><ul><li><p><strong>Predicate Pushdown</strong> 断言下推：将 Filter 这种可以减小数据集的操作下推，放在 Scan 的位置，减少无用操作量。（如提前执行 where）</p></li><li><p><strong>Column Pruning</strong> 列值裁剪：断言下推后，对无用的列进行裁剪。（如提前规划 select 数量）</p></li><li><p><strong>等等许多优化方案</strong> ，具体可查看 <code>org.apache.spark.sql.catalyst.optimizer.Optimizer</code> 源码</p></li></ul><h3 id="spark-on-hive" tabindex="-1"><a class="header-anchor" href="#spark-on-hive" aria-hidden="true">#</a> Spark on Hive</h3><p>相当于将 HIVE SQL 解释器引擎换成了 SparkSQL 解释器引擎。</p><p>Spark 自身没有元数据管理功能，当 Spark 执行 SQL 风格语句如 <code>SELECT * FROM person</code> 时候，如果没有 person 储存位置，person 包含的字段、字段类型的话，则 SQL 语句将无法被解析并执行。</p><p>SparkSQL 中将这些元数据信息注册在了 DataFrame 中，而数据库的元数据信息，由 Hive 的 MetaStore 来提供管理，Spark 只是提供执行引擎。因此 Spark 能够链接上 Hive 的 MetaStore 就可以了，MetaStore 需要存在并开机，通过配置 <code>hive-site.xml</code> Spark 能知道 MetaStore 的端口号：</p><div class="language-xml line-numbers-mode" data-ext="xml"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">&lt;</span><span style="color:#22863A;">property</span><span style="color:#24292E;">&gt;</span></span>
<span class="line"><span style="color:#24292E;">    &lt;</span><span style="color:#22863A;">name</span><span style="color:#24292E;">&gt;hive.metastore.uris&lt;/</span><span style="color:#22863A;">name</span><span style="color:#24292E;">&gt;</span></span>
<span class="line"><span style="color:#24292E;">    &lt;</span><span style="color:#22863A;">value</span><span style="color:#24292E;">&gt;thrift://node1:9083&lt;/</span><span style="color:#22863A;">value</span><span style="color:#24292E;">&gt;</span></span>
<span class="line"><span style="color:#24292E;">&lt;/</span><span style="color:#22863A;">property</span><span style="color:#24292E;">&gt;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>代码中只需要增加 3 行代码以继承 Hive</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">spark </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> SparkSession.builder. \</span></span>
<span class="line"><span style="color:#24292E;">        appName(</span><span style="color:#032F62;">&quot;local[*]&quot;</span><span style="color:#24292E;">). \</span></span>
<span class="line"><span style="color:#24292E;">        config(</span><span style="color:#032F62;">&quot;spark.sql.shuffle.partitions&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;4&quot;</span><span style="color:#24292E;">). \</span></span>
<span class="line"><span style="color:#24292E;">        config(</span><span style="color:#032F62;">&quot;spark.sql.warehouse.dir&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;hdfs://node1:8020/user/hive/warehouse&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">        config(</span><span style="color:#032F62;">&quot;hive.metastore.uris&quot;</span><span style="color:#24292E;">,</span><span style="color:#032F62;">&quot;thrift://node1:9083&quot;</span><span style="color:#24292E;">).\</span></span>
<span class="line"><span style="color:#24292E;">        enableHiveSupport().\ </span></span>
<span class="line"><span style="color:#24292E;">        getOrCreate()</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="分布式-sql-执行引擎" tabindex="-1"><a class="header-anchor" href="#分布式-sql-执行引擎" aria-hidden="true">#</a> 分布式 SQL 执行引擎</h3><p>Spark 的 ThriftServer 服务可以在 10000 端口监听。通过该服务，用户会写 SQL 就可操作 spark。</p><p><strong>启动 ThriftServer 服务</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">$SPARK_HOME/sbin/start-thriftserver.sh \</span></span>
<span class="line"><span style="color:#6F42C1;">--hiveconf</span><span style="color:#24292E;"> </span><span style="color:#032F62;">hive.server2.thrift.port=</span><span style="color:#005CC5;">10000</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">--hiveconf </span><span style="color:#032F62;">hive.server.thrift.bind.host=node1</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">--master </span><span style="color:#032F62;">local[</span><span style="color:#005CC5;">2</span><span style="color:#032F62;">]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>常用的用来链接的客户端工具有 DBeaver, datagrip, heidisql。python 代码链接 Thrift 可使用 pyhive 库。</p><p><code>pip install pyhive pymysql sasl thrift thrift_sasl</code></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> pyhive </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> hive</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">conn </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hive.Connection(</span><span style="color:#E36209;">host</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;node1&quot;</span><span style="color:#24292E;">, </span><span style="color:#E36209;">port</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">10000</span><span style="color:#24292E;">, </span><span style="color:#E36209;">username</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;hadoop&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">cursor </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> conn.cursor()</span></span>
<span class="line"><span style="color:#24292E;">cursor.execute(</span><span style="color:#032F62;">&quot;SELECT * FROM test&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">result </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> cursor.fetchall()</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(result)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h1 id="其他" tabindex="-1"><a class="header-anchor" href="#其他" aria-hidden="true">#</a> 其他</h1><p>Koalas——基于 Apache Spark 的 pandas API 实现</p><p><a href="https://spark.apache.org/docs/latest/ml-guide.html" target="_blank" rel="noopener noreferrer">Spark mllib 文档<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> MLlib 提供基础的机械学习算法，代码风格类似 sklearn。</p><h1 id="其他运行模式搭建" tabindex="-1"><a class="header-anchor" href="#其他运行模式搭建" aria-hidden="true">#</a> 其他运行模式搭建</h1><h2 id="standalone-架构" tabindex="-1"><a class="header-anchor" href="#standalone-架构" aria-hidden="true">#</a> Standalone 架构</h2><p>Standalone 模式是 Spark 自带的一种集群模式，该模式中 master 与 worker 以独立进程的形式存在。 StandAlone 是完整的 Spark 运行环境，其中: Master 角色以 Master 进程存在, Worker 角色以 Worker 进程存在 Driver 和 Executor 运行于 Worker 进程内, 由 Worker 提供资源供给它们运行</p><p>StandAlone 集群在进程上主要有 3 类进程:</p><ul><li>主节点 Master 进程：Master 角色, 管理整个集群资源，并托管运行各个任务的 Driver</li><li>从节点 Workers：Worker 角色, 管理每个机器的资源，分配对应的资源来- 运行 Executor(Task)； 每个从节点分配资源信息给 Worker 管理，资源信息包含内存 Memory 和 CPU Cores 核数</li><li>历史服务器 HistoryServer(可选)：Spark Application 运行完成以后，保存事件日志数据至 HDFS，启动 HistoryServer 可以查看应用运行相关信息。</li></ul><h3 id="实例集群规划" tabindex="-1"><a class="header-anchor" href="#实例集群规划" aria-hidden="true">#</a> 实例集群规划</h3><p>尝试使用三台 Linux 虚拟机来组成集群环境进行体验，非别是:</p><p>node1 运行: Spark 的 Master 进程 和 1 个 Worker 进程 node2 运行: spark 的 1 个 worker 进程 node3 运行: spark 的 1 个 worker 进程</p><p>整个集群提供: 1 个 master 进程 和 3 个 worker 进程</p><h3 id="安装" tabindex="-1"><a class="header-anchor" href="#安装" aria-hidden="true">#</a> 安装</h3><p>在所有节点上安装 python anaconda ，同时不要忘记 都创建<code>pyspark</code>虚拟环境 以及安装虚拟环境所需要的包<code>pyspark jieba pyhive</code></p><p>为了让 spark 拥有 hdfs 最大权限，spark 安装也使用 hadoop 用户：<code>chown -R hadoop:hadoop spark*</code></p><h3 id="配置配置文件" tabindex="-1"><a class="header-anchor" href="#配置配置文件" aria-hidden="true">#</a> 配置配置文件</h3><p>进入到 spark 的配置文件目录中, <code>cd $SPARK_HOME/conf</code></p><p>配置 workers 文件</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 改名, 去掉后面的.template 后缀</span></span>
<span class="line"><span style="color:#6F42C1;">mv</span><span style="color:#24292E;"> </span><span style="color:#032F62;">workers.template</span><span style="color:#24292E;"> </span><span style="color:#032F62;">workers</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 编辑 worker 文件</span></span>
<span class="line"><span style="color:#6F42C1;">vim</span><span style="color:#24292E;"> </span><span style="color:#032F62;">workers</span></span>
<span class="line"><span style="color:#6A737D;"># 将里面的 localhost 删除, 追加</span></span>
<span class="line"><span style="color:#6F42C1;">node1</span></span>
<span class="line"><span style="color:#6F42C1;">node2</span></span>
<span class="line"><span style="color:#6F42C1;">node3</span></span>
<span class="line"><span style="color:#6F42C1;">到</span><span style="color:#24292E;"> </span><span style="color:#032F62;">workers</span><span style="color:#24292E;"> </span><span style="color:#032F62;">文件内</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 功能: 这个文件就是指示了  当前 SparkStandAlone 环境下, 有哪些 worker</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>配置 spark-env.sh 文件</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 1. 改名</span></span>
<span class="line"><span style="color:#6F42C1;">mv</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-env.sh.template</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-env.sh</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 2. 编辑 spark-env.sh, 在底部追加如下内容</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## 设置 JAVA 安装目录</span></span>
<span class="line"><span style="color:#24292E;">JAVA_HOME</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/jdk</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## HADOOP 软件配置文件目录，读取 HDFS 上文件和运行 YARN 集群</span></span>
<span class="line"><span style="color:#24292E;">HADOOP_CONF_DIR</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/hadoop/etc/hadoop</span></span>
<span class="line"><span style="color:#24292E;">YARN_CONF_DIR</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/hadoop/etc/hadoop</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## 指定 spark 老大 Master 的 IP 和提交任务的通信端口</span></span>
<span class="line"><span style="color:#6A737D;"># 告知 Spark 的 master 运行在哪个机器上</span></span>
<span class="line"><span style="color:#D73A49;">export</span><span style="color:#24292E;"> SPARK_MASTER_HOST</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">node1</span></span>
<span class="line"><span style="color:#6A737D;"># 告知 sparkmaster 的通讯端口</span></span>
<span class="line"><span style="color:#D73A49;">export</span><span style="color:#24292E;"> SPARK_MASTER_PORT</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">7077</span></span>
<span class="line"><span style="color:#6A737D;"># 告知 spark master 的 webui 端口</span></span>
<span class="line"><span style="color:#24292E;">SPARK_MASTER_WEBUI_PORT</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">8080</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># worker cpu 可用核数</span></span>
<span class="line"><span style="color:#24292E;">SPARK_WORKER_CORES</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span></span>
<span class="line"><span style="color:#6A737D;"># worker 可用内存</span></span>
<span class="line"><span style="color:#24292E;">SPARK_WORKER_MEMORY</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#032F62;">g</span></span>
<span class="line"><span style="color:#6A737D;"># worker 的工作通讯地址</span></span>
<span class="line"><span style="color:#24292E;">SPARK_WORKER_PORT</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">7078</span></span>
<span class="line"><span style="color:#6A737D;"># worker 的 webui 地址</span></span>
<span class="line"><span style="color:#24292E;">SPARK_WORKER_WEBUI_PORT</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">8081</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## 设置历史服务器</span></span>
<span class="line"><span style="color:#6A737D;"># 配置的意思是  将 spark 程序运行的历史日志 存到 hdfs 的/sparklog 文件夹中</span></span>
<span class="line"><span style="color:#24292E;">SPARK_HISTORY_OPTS</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>注意, 上面的配置的路径 要根据你自己机器实际的路径来写</p><p>在 HDFS 上创建程序运行历史记录存放的文件夹:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">hadoop</span><span style="color:#24292E;"> </span><span style="color:#032F62;">fs</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-mkdir</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/sparklog</span></span>
<span class="line"><span style="color:#6F42C1;">hadoop</span><span style="color:#24292E;"> </span><span style="color:#032F62;">fs</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-chmod</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">777</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/sparklog</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>配置 spark-defaults.conf 文件</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 1. 改名</span></span>
<span class="line"><span style="color:#6F42C1;">mv</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-defaults.conf.template</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-defaults.conf</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 2. 修改内容, 追加如下内容</span></span>
<span class="line"><span style="color:#6A737D;"># 开启 spark 的日期记录功能</span></span>
<span class="line"><span style="color:#6F42C1;">spark.eventLog.enabled</span><span style="color:#24292E;"> 	</span><span style="color:#005CC5;">true</span></span>
<span class="line"><span style="color:#6A737D;"># 设置 spark 日志记录的路径</span></span>
<span class="line"><span style="color:#6F42C1;">spark.eventLog.dir</span><span style="color:#24292E;">	 </span><span style="color:#032F62;">hdfs://node1:8020/sparklog/</span><span style="color:#24292E;"> </span></span>
<span class="line"><span style="color:#6A737D;"># 设置 spark 日志是否启动压缩</span></span>
<span class="line"><span style="color:#6F42C1;">spark.eventLog.compress</span><span style="color:#24292E;"> 	</span><span style="color:#005CC5;">true</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>配置 log4j.properties 文件 [可选配置]</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 1. 改名</span></span>
<span class="line"><span style="color:#6F42C1;">mv</span><span style="color:#24292E;"> </span><span style="color:#032F62;">log4j.properties.template</span><span style="color:#24292E;"> </span><span style="color:#032F62;">log4j.properties</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 2. 修改日志警报级别 因为 Spark 是个话痨</span></span>
<span class="line"><span style="color:#6F42C1;">log4j.rootCategory</span><span style="color:#24292E;">=WARN, </span><span style="color:#032F62;">console</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="将-spark-安装文件夹-分发到其它的节点上" tabindex="-1"><a class="header-anchor" href="#将-spark-安装文件夹-分发到其它的节点上" aria-hidden="true">#</a> 将 Spark 安装文件夹 分发到其它的节点上</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">scp</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-r</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-3.1.2-bin-hadoop3.2</span><span style="color:#24292E;"> </span><span style="color:#032F62;">node2:/export/server/</span></span>
<span class="line"><span style="color:#6F42C1;">scp</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">-r</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-3.1.2-bin-hadoop3.2</span><span style="color:#24292E;"> </span><span style="color:#032F62;">node3:/export/server/</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>不要忘记, 在 node2 和 node3 上 给 spark 安装目录增加软链接</p><p><code>ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark</code></p><h3 id="验证环境" tabindex="-1"><a class="header-anchor" href="#验证环境" aria-hidden="true">#</a> 验证环境</h3><p>先开启 zoopkeeper <code>zookeeper/bin/zkServer.sh start</code>, hadoop <code>s</code>等。 启动历史服务器 <code>sbin/start-history-server.sh</code> 启动全部 workers 和 master：<code>sbin/start-all.sh</code></p><p>分别测试 <code>pyspark</code>, <code>spark-shell</code>, <code>spark-submit</code> 使用情况：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">bin/pyspark</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark://node1:7077</span></span>
<span class="line"><span style="color:#6A737D;"># 提供 --master 连接到 StandAlone，不写默认是 local 模式</span></span>
<span class="line"><span style="color:#6F42C1;">bin/spark-shell</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark://node1:7077</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#6F42C1;">bin/spark-submit</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark://node1:7077</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/export/server/spark/examples/src/main/python/pi.py</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">100</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行后通过 web <code>node1:18080</code> 查看历史服务器信息；<code>node1:8080</code> 查看 master。</p><h2 id="spark-standalone-ha" tabindex="-1"><a class="header-anchor" href="#spark-standalone-ha" aria-hidden="true">#</a> Spark StandAlone HA</h2><p>Spark Standalone 集群是 Master-Slaves 架构的集群模式，和大部分的 Master-Slaves 结构集群一样，存在着 Master 单点故障（SPOF）的问题。 如何解决这个单点故障的问题，Spark 提供了两种方案： 1.基于文件系统的单点恢复(Single-Node Recovery with Local File System)--只能用于开发或测试环境。 2.基于 zookeeper 的 Standby Masters(Standby Masters with ZooKeeper)--可以用于生产环境。 ZooKeeper 提供了一个 Leader Election 机制，利用这个机制可以保证虽然集群存在多个 Master，但是只有一个是 Active 的，其他的都是 Standby。当 Active 的 Master 出现故障时，另外的一个 Standby Master 会被选举出来。由于集群的信息，包括 Worker，Driver 和 Application 的信息都已经持久化到文件系统，因此在切换的过程中只会影响新 Job 的提交，对于正在进行的 Job 没有任何的影响。加入 ZooKeeper 的集群整体架构如下图所示。 toadd <a href="https://spark.apache.org/docs/3.1.2/spark-standalone.html#standby-masters-with-zookeeper" target="_blank" rel="noopener noreferrer">基于 Zookeeper 实现 HA<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h3 id="步骤" tabindex="-1"><a class="header-anchor" href="#步骤" aria-hidden="true">#</a> 步骤</h3><p>前提: 确保 Zookeeper 和 HDFS 均已经启动 在<code>spark-env.sh</code>中, 删除: <code>SPARK_MASTER_HOST=node1</code> 在<code>spark-env.sh</code>中, 增加:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">SPARK_DAEMON_JAVA_OPTS</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span></span>
<span class="line"><span style="color:#6A737D;"># spark.deploy.recoveryMode 指定 HA 模式 基于 Zookeeper 实现</span></span>
<span class="line"><span style="color:#6A737D;"># 指定 Zookeeper 的连接地址</span></span>
<span class="line"><span style="color:#6A737D;"># 指定在 Zookeeper 中注册临时节点的路径</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><a href="http://xn--spark-env-kj5q.sh/" target="_blank" rel="noopener noreferrer">将 spark-env.sh<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 分发到每一台服务器上</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">scp</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-env.sh</span><span style="color:#24292E;"> </span><span style="color:#032F62;">node2:/export/server/spark/conf/</span></span>
<span class="line"><span style="color:#6F42C1;">scp</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark-env.sh</span><span style="color:#24292E;"> </span><span style="color:#032F62;">node3:/export/server/spark/conf/</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>停止当前 StandAlone 集群</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">sbin/stop-all.sh</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="master-主备切换" tabindex="-1"><a class="header-anchor" href="#master-主备切换" aria-hidden="true">#</a> master 主备切换</h3><p>提交一个 spark 任务到当前<code>alive</code>master 上:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">bin/spark-submit</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--master</span><span style="color:#24292E;"> </span><span style="color:#032F62;">spark://node1:7077</span><span style="color:#24292E;"> </span><span style="color:#032F62;">/export/server/spark/examples/src/main/python/pi.py</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>在提交成功后, 将 alivemaster 直接 kill 掉，不会影响程序运行:<br><img src="https://pybd.yuque.com/api/filetransfer/images?url=https%3A%2F%2Fimage-set.oss-cn-zhangjiakou.aliyuncs.com%2Fimg-out%2F2021%2F09%2F08%2F20210908162555.png&amp;sign=6c267116ad788645fdc2af413af7ac1c6e22ae0d655afe3dd4fda1117f6d5253#from=url&amp;id=AAdNb&amp;margin=[object Object]&amp;originHeight=314&amp;originWidth=1889&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none" alt="" loading="lazy"><br> 当新的 master 接收集群后, 程序继续运行, 正常得到结果。同时新 master 的 8080 界面显示状态从 STANDBY 变为 RECOVERING/ ACTIVE。</p><p>HA 模式下, 主备切换 不会影响到正在运行的程序.</p><h2 id="spark-on-yarn-环境搭建" tabindex="-1"><a class="header-anchor" href="#spark-on-yarn-环境搭建" aria-hidden="true">#</a> Spark On YARN 环境搭建</h2><p>许多企业不管做什么业务,都基本上会有 Hadoop 集群. 也就是会有 YARN 集群。因此在有 YARN 集群的前提下单独准备 Spark StandAlone 集群,对资源的利用就不高。 Spark on YARN 是最常见的应用框架。 对于 Spark On YARN, 无需部署 Spark 集群, 只要找一台服务器, 充当 Spark 的客户端, 即可提交任务到 YARN 集群中运行。</p><p><strong>本质</strong></p><p>Master 角色由 YARN 的 ResourceManager 担任 Worker 角色由 YARN 的 NodeManager 担任 Driver 角色运行在 YARN 容器内或提交任务的客户端进程中 真正干活的 Executor 运行在 YARN 提供的容器内</p><p><strong>部署</strong></p><p>配置 spark-env.sh 中的 HADOOP_CONF_DIR、 YARN_CONF_DIR 环境变量，指向 hadoop 与 yarn 的配置文件目录</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">HADOOP_CONF_DIR</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/hadoop/etc/hadoop/</span></span>
<span class="line"><span style="color:#24292E;">YARN_CONF_DIR</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">/export/server/hadoop/etc/hadoop/</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><a href="https://www.cnblogs.com/rmxd/p/12273395.html" target="_blank" rel="noopener noreferrer">参考链接 - 连接到 YARN 中<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></div><!----><footer class="page-meta"><!----><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 417333277@qq.com">kevinng77</span><!--]--><!--]--></div></div></footer><!----><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer"><a href="https://beian.miit.gov.cn/" target="_blank">沪ICP备2023027904号-1</a>&nbsp;<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=" target="_blank"><img src="/assets/gongan.png">公安备案号 </a></div><div class="vp-copyright">Copyright © 2024 Kevin 吴嘉文</div></footer></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-86c9497f.js" defer></script>
  </body>
</html>
