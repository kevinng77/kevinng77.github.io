<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0mistral.html"><meta property="og:site_name" content="记忆笔书"><meta property="og:title" content="Mistral 系列模型整理"><meta property="og:description" content="在本文中，我们梳理了 24 年 7 月前 Mistral 系列模型的关键信息，包括它们的主要特点、亮点以及相关资源链接。涉及模型 Mistral 7B， Mixtral 8x7B，Mixtral 8x22B，Mistral Nemo, Mistral Large 2 mistral 7B 官方博客 ，mistral 7B 论文"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-01-29T12:24:45.000Z"><meta property="article:author" content="Kevin 吴嘉文"><meta property="article:tag" content="AIGC"><meta property="article:tag" content="LLM"><meta property="article:published_time" content="2024-07-27T00:00:00.000Z"><meta property="article:modified_time" content="2025-01-29T12:24:45.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Mistral 系列模型整理","image":[""],"datePublished":"2024-07-27T00:00:00.000Z","dateModified":"2025-01-29T12:24:45.000Z","author":[{"@type":"Person","name":"Kevin 吴嘉文"}]}</script><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;700&display=swap" rel="stylesheet"><link rel="icon" href="/logo.svg"><title>Mistral 系列模型整理 | 记忆笔书</title><meta name="description" content="在本文中，我们梳理了 24 年 7 月前 Mistral 系列模型的关键信息，包括它们的主要特点、亮点以及相关资源链接。涉及模型 Mistral 7B， Mixtral 8x7B，Mixtral 8x22B，Mistral Nemo, Mistral Large 2 mistral 7B 官方博客 ，mistral 7B 论文">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-99575b2e.css" as="style"><link rel="stylesheet" href="/assets/style-99575b2e.css">
    <link rel="modulepreload" href="/assets/app-80ce1db6.js"><link rel="modulepreload" href="/assets/笔记mistral.html-62286caa.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/assets/笔记mistral.html-8e4d5b1c.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><!----><!----><span class="vp-site-name">记忆笔书</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="主页" class="vp-link nav-link nav-link" href="/"><span class="font-icon icon iconfont icon-home" style=""></span>主页<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="归档" class="vp-link nav-link nav-link" href="/timeline/"><span class="font-icon icon iconfont icon-categoryselected" style=""></span>归档<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/kevinng77" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!----><!--[--><button type="button" class="search-pro-button" role="search" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Mistral 系列模型整理</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Kevin 吴嘉文</span></span><span property="author" content="Kevin 吴嘉文"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-07-27T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 9 分钟</span><meta property="timeRequired" content="PT9M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category7 clickable" role="navigation">知识笔记</span><!--]--><meta property="articleSection" content="知识笔记"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag4 clickable" role="navigation">AIGC</span><span class="page-tag-item tag6 clickable" role="navigation">LLM</span><!--]--><meta property="keywords" content="AIGC,LLM"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<!----></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mistral-7b">mistral 7B</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mixtral-8-7b">Mixtral 8*7B</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mixtral-8-22b">Mixtral 8*22B</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mistral-nemo">Mistral Nemo</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mistral-large-2">Mistral Large  2</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><p>在本文中，我们梳理了 24 年 7 月前 Mistral 系列模型的关键信息，包括它们的主要特点、亮点以及相关资源链接。涉及模型 Mistral 7B， Mixtral 8x7B，Mixtral 8x22B，Mistral Nemo, Mistral Large 2</p><h2 id="mistral-7b" tabindex="-1"><a class="header-anchor" href="#mistral-7b" aria-hidden="true">#</a> mistral 7B</h2><p><a href="https://mistral.ai/news/announcing-mistral-7b/" target="_blank" rel="noopener noreferrer">官方博客<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ，<a href="https://arxiv.org/abs/2310.06825" target="_blank" rel="noopener noreferrer">mistral 7B 论文<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Mistral 7B 模型的亮点包括：</p><ul><li><strong>Sliding Window Attention</strong></li></ul><p>Mistral 采用的 window size 为 4096，而后一共有 32 层 layer，那么采用 SWA 之后，理论上在进行 attention 的时候， <strong>理论上</strong> 可以收集到约 131K tokens 的信息。(虽然论文里提到的 window size 是 4096，但 官方提供的 <a href="https://huggingface.co/mistralai/mathstral-7B-v0.1/blob/main/config.json" target="_blank" rel="noopener noreferrer">huggingface 上的权重<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 中 <code>max_position_embeddings</code> 为 32768，且在新一点的版本中，比如 <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/config.json" target="_blank" rel="noopener noreferrer">mistral-7b-instruct-v0.2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，都不采用 sliding window 了)</p><figure><img src="/assets/img/mistral/image-20240805103929202.jpg" alt="image-20240805103929202" tabindex="0" loading="lazy"><figcaption>image-20240805103929202</figcaption></figure><p>由于代用了固定的 attention 窗口大小，因此我们只需要一个大小为 <code>W=window size</code> 的 cache ，在计算第 i 个 token 的 cache 的时候，只需要覆盖 cache 中 <code>i mod M</code> 位置上的 hidden state 即可。</p><p>参考 huggingface 的 mistral 实现，Sliding window attention 通过 attention_mask 来控制：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># huggignface mistral attn mask 实现</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">_update_causal_mask</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">        self,</span></span>
<span class="line"><span style="color:#24292E;">        attention_mask: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        input_tensor: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        cache_position: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        past_key_values: Cache,</span></span>
<span class="line"><span style="color:#24292E;">    ):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># ... 省略部分无关代码</span></span>
<span class="line"><span style="color:#24292E;">        past_seen_tokens </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> cache_position[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> past_key_values </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">else</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span></span>
<span class="line"><span style="color:#24292E;">        using_static_cache </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(past_key_values, StaticCache)</span></span>
<span class="line"><span style="color:#24292E;">        using_sliding_window_cache </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(past_key_values, SlidingWindowCache)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        dtype, device </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> input_tensor.dtype, input_tensor.device</span></span>
<span class="line"><span style="color:#24292E;">        min_dtype </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.finfo(dtype).min</span></span>
<span class="line"><span style="color:#24292E;">        sequence_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> input_tensor.shape[</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># SlidingWindowCache</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> using_sliding_window_cache:</span></span>
<span class="line"><span style="color:#24292E;">            target_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">max</span><span style="color:#24292E;">(sequence_length, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># StaticCache</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">elif</span><span style="color:#24292E;"> using_static_cache:</span></span>
<span class="line"><span style="color:#24292E;">            target_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> past_key_values.get_max_length()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># DynamicCache or no cache</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            target_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> (</span></span>
<span class="line"><span style="color:#24292E;">                attention_mask.shape[</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(attention_mask, torch.Tensor)</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">else</span><span style="color:#24292E;"> past_seen_tokens </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> sequence_length </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span></span>
<span class="line"><span style="color:#24292E;">            )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">and</span><span style="color:#24292E;"> attention_mask.dim() </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># in this case we assume that the mask comes already in inverted form and requires no inversion or slicing</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask.max() </span><span style="color:#D73A49;">!=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">raise</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">ValueError</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;Custom 4D attention mask should be passed in inverted form with max==0`&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> attention_mask</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.full(</span></span>
<span class="line"><span style="color:#24292E;">                (sequence_length, target_length), </span><span style="color:#E36209;">fill_value</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">min_dtype, </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">dtype, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">device</span></span>
<span class="line"><span style="color:#24292E;">            )</span></span>
<span class="line"><span style="color:#24292E;">            exclude_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.arange(target_length, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">device) </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> cache_position.reshape(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> using_sliding_window_cache </span><span style="color:#D73A49;">or</span><span style="color:#24292E;"> sequence_length </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window:</span></span>
<span class="line"><span style="color:#24292E;">                    exclude_mask.bitwise_or_(</span></span>
<span class="line"><span style="color:#24292E;">                        torch.arange(target_length, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">device)</span></span>
<span class="line"><span style="color:#24292E;">                        </span><span style="color:#D73A49;">&lt;=</span><span style="color:#24292E;"> (cache_position.reshape(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">) </span><span style="color:#D73A49;">-</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window)</span></span>
<span class="line"><span style="color:#24292E;">                    )</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">*=</span><span style="color:#24292E;"> exclude_mask</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask[</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, :, :].expand(input_tensor.shape[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask.clone()  </span><span style="color:#6A737D;"># copy to contiguous memory for in-place edit</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask.dim() </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                    mask_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> attention_mask.shape[</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">                    padding_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask[:, :, :, :mask_length] </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> attention_mask[:, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, :]</span></span>
<span class="line"><span style="color:#24292E;">                    padding_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> padding_mask </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span></span>
<span class="line"><span style="color:#24292E;">                    causal_mask[:, :, :, :mask_length] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask[:, :, :, :mask_length].masked_fill(</span></span>
<span class="line"><span style="color:#24292E;">                        padding_mask, min_dtype</span></span>
<span class="line"><span style="color:#24292E;">                    )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> causal_mask</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>GQA (Grouped Query Attention)</strong></li></ul><p><a href="https://arxiv.org/abs/2305.13245" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><figure><img src="/assets/img/mistral/image-20240805103944726.jpg" alt="image-20240805103944726" tabindex="0" loading="lazy"><figcaption>image-20240805103944726</figcaption></figure><p>grouped-query attention 指出，<a href="https://arxiv.org/pdf/1911.02150.pdf" target="_blank" rel="noopener noreferrer">Multi-Query Attention<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 提高了推理速度的同时，却可能极大地降低回复质量。因此根据上图，GQA 在推理速度和质量之间作了权衡。</p><p>以下为 GQA 文中的实验结果，值得注意的是论文中使用原 MHA checkpoint 转换为 GQA 权重后，还进行了额外的预训练：</p><figure><img src="/assets/img/mistral/image-20240805103956240.jpg" alt="image-20240805103956240" tabindex="0" loading="lazy"><figcaption>image-20240805103956240</figcaption></figure><p>此外 Mistral，Llama2 的部分模型使用 GQA 时，采用的 kv head 数量似乎都是 8。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/647130255" target="_blank" rel="noopener noreferrer">为什么现在大家都在用 MQA 和 GQA？<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 文中提到 MQA 和 GQA 能获得巨大加速的一个点在于：GPU 内存强的限制。由于 MQA 和 GQA 都降低了内存中数据的读取量，减少了计算单元的等待时间，因此推理速度的提高比想象中的要快更多。</p></blockquote><h2 id="mixtral-8-7b" tabindex="-1"><a class="header-anchor" href="#mixtral-8-7b" aria-hidden="true">#</a> Mixtral 8*7B</h2><p><a href="https://arxiv.org/abs/2401.04088" target="_blank" rel="noopener noreferrer">论文<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，<a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1" target="_blank" rel="noopener noreferrer">huggingface 模型权重<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>， <a href="https://mistral.ai/news/mixtral-of-experts/" target="_blank" rel="noopener noreferrer">官方博客<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，<a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/mixtral/modeling_mixtral.py" target="_blank" rel="noopener noreferrer">huggingface 模型代码<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，<a href="https://huggingface.co/blog/zh/moe" target="_blank" rel="noopener noreferrer">混合专家模型基础（推荐）<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，官方给出的评分来看，mixtral 8*7 和 GPT3.5 有的一比。</p><ul><li><p>发布时间：23 年 12 月</p></li><li><p>模型大小：8 个 expert MLP 层，一共 45B 大小。</p></li><li><p>训练：除了预训练外，Mixtral MOE 后续还开源了一个经过 SFT + DPO 微调的版本。</p></li><li><p>模型效果：</p></li></ul><figure><img src="/assets/img/mistral/image-20240805104006798.jpg" alt="image-20240805104006798" tabindex="0" loading="lazy"><figcaption>image-20240805104006798</figcaption></figure><ul><li>架构：Mixtral 的 MOE 架构类似于，在 MoE 模型中，只有 FFN 层被视为独立的专家，而模型的其他参数是共享的。大致参数为：</li></ul><figure><img src="/assets/img/mistral/image-20240805104016338.jpg" alt="image-20240805104016338" tabindex="0" loading="lazy"><figcaption>image-20240805104016338</figcaption></figure><p>对 moe 架构不太了解的朋友，可以参考这篇博客 <a href="https://huggingface.co/blog/zh/moe" target="_blank" rel="noopener noreferrer">混合专家模型基础（推荐）<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。</p><p>参考 huggingface 中的 mixtral 和 mistral 实现对比，差异在于 mixtral 中将传统 transformer decoder layer 中的 FFN 替换为了 <code>block_sparse_moe</code>。</p><figure><img src="/assets/img/mistral/image-20240723203836298.jpg" alt="https://github.com/open-compass/MixtralKit" tabindex="0" loading="lazy"><figcaption>https://github.com/open-compass/MixtralKit</figcaption></figure><p>主要逻辑为：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><mi>T</mi><mi>o</mi><mi>p</mi><mi>K</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">⋅</mo><msub><mi>W</mi><mrow><mi>g</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mtext>final hidden states</mtext><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></munderover><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo separator="true">⋅</mo><msub><mi>E</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> G(x) = \text{Softmax}(TopK(x · W_{gate}))\\ \text{final hidden states} = \sum^{n-1}_{i=0} G(x)_i·E_i(x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal">o</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">final hidden states</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0788em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8011em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E_i(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 为专家对应的网络，具体展示为下面 huggingface 实现中的 <code>MixtralBlockSparseTop2MLP</code>。mixtral 中采用了 8 个 expert，每次推理使用选取 top 2 的 expert 进行推理。比如输入一句话 <code>你好，今天</code>，那么我们每个 token 都会选出 top 2 的 expert 来负责这个 token 的预测，因此在推理 <code>你好，今天</code> 时， <strong>有概率所有 expert 都会参与到计算当中</strong> ，具体可以参考 <code>MixtralSparseMoeBlock</code> 的实现。</p><figure><img src="/assets/img/mistral/image-20240805104032194.jpg" alt="image-20240805104032194" tabindex="0" loading="lazy"><figcaption>image-20240805104032194</figcaption></figure><p>mixtral 论文中提到专家分配在不同主题（如 ArXiv 论文、生物学和哲学文档）中没有明显的模式，只有在 DM 数学中显示出边际上的差异，这可能是由于其数据集的合成性质和有限的自然语言覆盖范围所致。router 在某些句法结构上表现出一定的结构化行为（比如 python 的 self 等），同时连续标记通常被分配给相同的专家。</p><ul><li>huggingface 中的 mixtral 核心代码：</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">MixtralDecoderLayer</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, config: MixtralConfig, layer_idx: </span><span style="color:#005CC5;">int</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_size </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.hidden_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.self_attn </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">MIXTRAL_ATTENTION_CLASSES</span><span style="color:#24292E;">[config._attn_implementation](config, layer_idx)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.block_sparse_moe </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MixtralSparseMoeBlock(config)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.input_layernorm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MixtralRMSNorm(config.hidden_size, </span><span style="color:#E36209;">eps</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">config.rms_norm_eps)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.post_attention_layernorm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MixtralRMSNorm(config.hidden_size, </span><span style="color:#E36209;">eps</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">config.rms_norm_eps)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        attention_mask: Optional[torch.Tensor] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 此处省略参数 ..</span></span>
<span class="line"><span style="color:#24292E;">    ) -&gt; Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        residual </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.input_layernorm(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states, self_attn_weights, present_key_value </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.self_attn(</span></span>
<span class="line"><span style="color:#24292E;">        	</span><span style="color:#6A737D;"># 此处省略参数 </span></span>
<span class="line"><span style="color:#24292E;">        )</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> residual </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        residual </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.post_attention_layernorm(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># Mixtral 将原本的 hidden_states = self.FFN(hidden_states) 替换为了：</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states, router_logits </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.block_sparse_moe(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> residual </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        outputs </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> (hidden_states,)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> outputs</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>huggingface 中 <code>block_sparse_moe</code> 的实现（省略部分次要代码）：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">MixtralSparseMoeBlock</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, config):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.hidden_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.intermediate_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.num_local_experts</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.top_k </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.num_experts_per_tok</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.gate </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.experts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.ModuleList([MixtralBlockSparseTop2MLP(config) </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> _ </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts)])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.jitter_noise </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.router_jitter_noise</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:</span></span>
<span class="line"><span style="color:#24292E;">        batch_size, sequence_length, hidden_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states.shape</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states.view(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">        router_logits </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.gate(hidden_states)  </span><span style="color:#6A737D;"># (batch * sequence_length, n_experts)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        routing_weights </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> F.softmax(router_logits, </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">torch.float)</span></span>
<span class="line"><span style="color:#24292E;">        routing_weights, selected_experts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.topk(routing_weights, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.top_k, </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        routing_weights </span><span style="color:#D73A49;">/=</span><span style="color:#24292E;"> routing_weights.sum(</span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#E36209;">keepdim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># we cast back to the input dtype</span></span>
<span class="line"><span style="color:#24292E;">        routing_weights </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> routing_weights.to(hidden_states.dtype)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        final_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.zeros(</span></span>
<span class="line"><span style="color:#24292E;">            (batch_size </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> sequence_length, hidden_dim), </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">hidden_states.dtype, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">hidden_states.device</span></span>
<span class="line"><span style="color:#24292E;">        )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># One hot encode the selected experts to create an expert mask</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># this will be used to easily index which expert is going to be sollicitated</span></span>
<span class="line"><span style="color:#24292E;">        expert_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.nn.functional.one_hot(selected_experts, </span><span style="color:#E36209;">num_classes</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts).permute(</span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># Loop over all available experts in the model and perform the computation on each expert</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> expert_idx </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts):</span></span>
<span class="line"><span style="color:#24292E;">            expert_layer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.experts[expert_idx]</span></span>
<span class="line"><span style="color:#24292E;">            idx, top_x </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.where(expert_mask[expert_idx])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># Index the correct hidden states and compute the expert hidden state for</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># the current expert. We need to make sure to multiply the output hidden</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># states by `routing_weights` on the corresponding tokens (top-1 and top-2)</span></span>
<span class="line"><span style="color:#24292E;">            </span></span>
<span class="line"><span style="color:#24292E;">            current_state </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states[</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, top_x].reshape(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># current_state: shape (n_i, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 所有 current_state 的长度 n 总和为 batch * sequence_length</span></span>
<span class="line"><span style="color:#24292E;">            current_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> expert_layer(current_state) </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> routing_weights[top_x, idx, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># However `index_add_` only support torch tensors for indexing so we&#39;ll use</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># the `top_x` tensor here.</span></span>
<span class="line"><span style="color:#24292E;">            final_hidden_states.index_add_(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, top_x, current_hidden_states.to(hidden_states.dtype))</span></span>
<span class="line"><span style="color:#24292E;">        final_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> final_hidden_states, router_logits</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中： <code>MixtralBlockSparseTop2MLP</code> 长这样：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">MixtralBlockSparseTop2MLP</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, config: MixtralConfig):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.intermediate_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.hidden_size</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w1 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w3 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.act_fn </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">ACT2FN</span><span style="color:#24292E;">[config.hidden_act]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, hidden_states):</span></span>
<span class="line"><span style="color:#24292E;">        current_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.act_fn(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w1(hidden_states)) </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w3(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        current_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w2(current_hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> current_hidden_states</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="推理" tabindex="-1"><a class="header-anchor" href="#推理" aria-hidden="true">#</a> 推理</h4><p>根据模型参数量 45B 来推理的话，如果用 fp16 的话推理的话，得需要至少 90GB 以上的显存，如果用 4 bit 的话，30GB 显存就够了。量化的生成速度，可以参考<a href="https://www.reddit.com/r/LocalLLaMA/comments/18jslmf/tokens_per_second_mistral_8x7b_performance/" target="_blank" rel="noopener noreferrer">这个 redis<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 中的评论，大致为 ：</p><table><thead><tr><th>推理精度</th><th>设备</th><th>速度 tokens/s</th></tr></thead><tbody><tr><td>Q4_K_M</td><td>单卡 4090 + <strong>7950X3D</strong></td><td>20</td></tr><tr><td>Q4_K_M</td><td>2 x 3090</td><td>48.26</td></tr></tbody></table><p>如果有 100+GB 以上显存，可以用 vllm 快速搭建测试 api：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">docker</span><span style="color:#24292E;"> </span><span style="color:#032F62;">run</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--gpus</span><span style="color:#24292E;"> </span><span style="color:#032F62;">all</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">-e</span><span style="color:#24292E;"> </span><span style="color:#032F62;">HF_TOKEN=</span><span style="color:#24292E;">$HF_TOKEN </span><span style="color:#005CC5;">-p</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">8000</span><span style="color:#032F62;">:8000</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">ghcr.io/mistralai/mistral-src/vllm:latest</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--host</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0.0</span><span style="color:#032F62;">.0.0</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--model</span><span style="color:#24292E;"> </span><span style="color:#032F62;">mistralai/Mixtral-8x7B-Instruct-v0.1</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--tensor-parallel-size</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;"> </span><span style="color:#6A737D;"># 100+GB 显存 \</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6F42C1;">--load-format</span><span style="color:#24292E;"> </span><span style="color:#032F62;">pt</span><span style="color:#24292E;"> </span><span style="color:#6A737D;"># needed since both `pt` and `safetensors` are available</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>NVIDIA 的 <a href="https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm/?ncid=so-twit-928467/" target="_blank" rel="noopener noreferrer">TensorRT-LLM 博客<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>中发出了对 Mixtral 8*7B 的吞吐量 benchmark （using input and output sequence lengths of 128）：</p><figure><img src="/assets/img/mistral/image-20240728094958591.jpg" alt="image-20240728094958591" tabindex="0" loading="lazy"><figcaption>image-20240728094958591</figcaption></figure><p>文中没有给出当 sequence lengths 最大时候的吞吐量，但根据上图数据，可以猜测 2 个 H100 部署 8*7B 正常服务用户时，平均吞吐量应该可以大于 7500Tokens/秒，根据 H100 的功耗计算电费成本的话，生成 1M token 需要耗约为 0.02 度电。</p><h2 id="mixtral-8-22b" tabindex="-1"><a class="header-anchor" href="#mixtral-8-22b" aria-hidden="true">#</a> Mixtral 8*22B</h2><p><a href="https://mistral.ai/news/mixtral-8x22b/" target="_blank" rel="noopener noreferrer">官方博客<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，<a href="https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1" target="_blank" rel="noopener noreferrer">huggingface 开源模型<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，</p><ul><li>架构：架构与 mixtral 8*7B 架构一样，在 huggingface 中使用的都是<code>MixtralForCausalLM</code> ，但 22B 的各方面参数大一点，比较特别的是 context window 从 32k 升级到了 65k， <code>vocab_size</code> 也更大一些。</li><li>支持 function calling，不过好像没有透露具体的 function calling 训练细节。</li><li>数学和 coding 能力明显超越 llama2 70B</li><li>似乎对中文的支持不是很好。</li></ul><figure><img src="/assets/img/mistral/image-20240727152529176.jpg" alt="image-20240727152529176" tabindex="0" loading="lazy"><figcaption>image-20240727152529176</figcaption></figure><p>Mistral 团队开源的模型，都比较注重 coding 和 math 的能力，Mixtral 系列的模型在这方便表现也是比较好：</p><figure><img src="/assets/img/mistral/image-20240727152702974.jpg" alt="image-20240727152702974" tabindex="0" loading="lazy"><figcaption>image-20240727152702974</figcaption></figure><h2 id="mistral-nemo" tabindex="-1"><a class="header-anchor" href="#mistral-nemo" aria-hidden="true">#</a> Mistral Nemo</h2><p><a href="https://mistral.ai/news/mistral-nemo/" target="_blank" rel="noopener noreferrer">官方博客<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，<a href="https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407" target="_blank" rel="noopener noreferrer">huggingface 模型权重<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Mistral Nemo 使用的也是 <code>MistralForCausalLM</code> 架构，与 mistral 7B 的差别为：Mistral Nemo 的 <code>hidden_size</code> 从 4096 变为 5120；<code>max_position_embeddings</code> 变为 1024000，<code>num_hidden_layers</code> 增加到 40， vocab_size 增加到 131072，不用 sliding window。</p><ul><li>支持 function calling！</li><li>采用了 Tekken 作为 tokenizer，比 SentencePiece 更高效（压缩率更高，官方描述是~30% more efficient at compressing，不确定是哪个方面的 efficient）</li></ul><p>NVIDIA 在<a href="https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/" target="_blank" rel="noopener noreferrer">这个博客<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>中提到：Mistral Nemo 采用这样的设计，是为了能够适配单个 NVIDIA L40S、NVIDIA GeForce RTX 4090 或 NVIDIA RTX 4500 GPU。模型采用 <a href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noopener noreferrer"> Megatron-LM<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 训练，用了 3,072 个 H100 80GB 。</p><p>但光采用 FP16 加载整个 Mistral Nemo 就需要花 23 GB 显存，要是要跑满整个 context window size，除了量化外，还是得需要采用 offload 或者其他方法来推理</p><p>不过 mistral 官方把 12 B 的模型和其他 8B 的模型对比，感觉好像不太公平：</p><figure><img src="/assets/img/mistral/image-20240727154936831.jpg" alt="image-20240727154936831" tabindex="0" loading="lazy"><figcaption>image-20240727154936831</figcaption></figure><h2 id="mistral-large-2" tabindex="-1"><a class="header-anchor" href="#mistral-large-2" aria-hidden="true">#</a> <strong>Mistral Large</strong> 2</h2><p><a href="https://mistral.ai/news/mistral-large-2407/" target="_blank" rel="noopener noreferrer">官方博客<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>， <a href="https://huggingface.co/mistralai/Mistral-Large-Instruct-2407" target="_blank" rel="noopener noreferrer">huggingface 模型权重<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Mistral Large 2，参数量 123B，主打多语言以及 coding 能力。采用与 mistral 7B 一样的架构，huggingface 中同样使用 <code>MistralForCausalLM</code>；比较值得注意的是 context window size 为 131072，不用 sliding window。</p><p>Llama 3.1 刚出不久，就拿 <strong>Mistral Large</strong> 2 和别人来对比：</p><figure><img src="/assets/img/mistral/image-20240805104145922.jpg" alt="image-20240805104145922" tabindex="0" loading="lazy"><figcaption>image-20240805104145922</figcaption></figure><p>在代码能力上，Mistral large 2 比 llama 3.1 平均效果更好。</p><figure><img src="/assets/img/mistral/image-20240727201347583.jpg" alt="image-20240727201347583" tabindex="0" loading="lazy"><figcaption>image-20240727201347583</figcaption></figure><p>除了 coding 和数学外，在 MT Bench 的评分也比 llama 3.1 高，平均生成的回复长度比 llama 3.1 要短</p><figure><img src="/assets/img/mistral/image-20240805104200493.jpg" alt="image-20240805104200493" tabindex="0" loading="lazy"><figcaption>image-20240805104200493</figcaption></figure><p>同时，中文能力相对上一代 mistral large 有大步幅提升：</p><figure><img src="/assets/img/mistral/image-20240805104212135.jpg" alt="image-20240805104212135" tabindex="0" loading="lazy"><figcaption>image-20240805104212135</figcaption></figure></div><!----><footer class="page-meta"><!----><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 417333277@qq.com">kevinng77</span><!--]--><!--]--></div></div></footer><!----><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer"><a href="https://beian.miit.gov.cn/" target="_blank">沪ICP备2023027904号-1</a>&nbsp;<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=" target="_blank"><img src="/assets/gongan.png">公安备案号 </a></div><div class="vp-copyright">Copyright © 2025 Kevin 吴嘉文</div></footer></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-80ce1db6.js" defer></script>
  </body>
</html>
