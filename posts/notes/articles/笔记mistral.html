<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0mistral.html"><meta property="og:site_name" content="è®°å¿†ç¬”ä¹¦"><meta property="og:title" content="Mistral ç³»åˆ—æ¨¡å‹æ•´ç†"><meta property="og:description" content="åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢³ç†äº† 24 å¹´ 7 æœˆå‰ Mistral ç³»åˆ—æ¨¡å‹çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„ä¸»è¦ç‰¹ç‚¹ã€äº®ç‚¹ä»¥åŠç›¸å…³èµ„æºé“¾æ¥ã€‚æ¶‰åŠæ¨¡å‹ Mistral 7Bï¼Œ Mixtral 8x7Bï¼ŒMixtral 8x22Bï¼ŒMistral Nemo, Mistral Large 2 mistral 7B å®˜æ–¹åšå®¢ ï¼Œmistral 7B è®ºæ–‡"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-01-29T12:24:45.000Z"><meta property="article:author" content="Kevin å´å˜‰æ–‡"><meta property="article:tag" content="AIGC"><meta property="article:tag" content="LLM"><meta property="article:published_time" content="2024-07-27T00:00:00.000Z"><meta property="article:modified_time" content="2025-01-29T12:24:45.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Mistral ç³»åˆ—æ¨¡å‹æ•´ç†","image":[""],"datePublished":"2024-07-27T00:00:00.000Z","dateModified":"2025-01-29T12:24:45.000Z","author":[{"@type":"Person","name":"Kevin å´å˜‰æ–‡"}]}</script><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;700&display=swap" rel="stylesheet"><link rel="icon" href="/logo.svg"><title>Mistral ç³»åˆ—æ¨¡å‹æ•´ç† | è®°å¿†ç¬”ä¹¦</title><meta name="description" content="åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢³ç†äº† 24 å¹´ 7 æœˆå‰ Mistral ç³»åˆ—æ¨¡å‹çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„ä¸»è¦ç‰¹ç‚¹ã€äº®ç‚¹ä»¥åŠç›¸å…³èµ„æºé“¾æ¥ã€‚æ¶‰åŠæ¨¡å‹ Mistral 7Bï¼Œ Mixtral 8x7Bï¼ŒMixtral 8x22Bï¼ŒMistral Nemo, Mistral Large 2 mistral 7B å®˜æ–¹åšå®¢ ï¼Œmistral 7B è®ºæ–‡">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-99575b2e.css" as="style"><link rel="stylesheet" href="/assets/style-99575b2e.css">
    <link rel="modulepreload" href="/assets/app-80ce1db6.js"><link rel="modulepreload" href="/assets/ç¬”è®°mistral.html-62286caa.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/assets/ç¬”è®°mistral.html-8e4d5b1c.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">è·³è‡³ä¸»è¦å…§å®¹</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><!----><!----><span class="vp-site-name">è®°å¿†ç¬”ä¹¦</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="ä¸»é¡µ" class="vp-link nav-link nav-link" href="/"><span class="font-icon icon iconfont icon-home" style=""></span>ä¸»é¡µ<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="å½’æ¡£" class="vp-link nav-link nav-link" href="/timeline/"><span class="font-icon icon iconfont icon-categoryselected" style=""></span>å½’æ¡£<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/kevinng77" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!----><!--[--><button type="button" class="search-pro-button" role="search" aria-label="æœç´¢"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">æœç´¢</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Mistral ç³»åˆ—æ¨¡å‹æ•´ç†</h1><div class="page-info"><span class="page-author-info" aria-label="ä½œè€…ğŸ–Š" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Kevin å´å˜‰æ–‡</span></span><span property="author" content="Kevin å´å˜‰æ–‡"></span></span><!----><span class="page-date-info" aria-label="å†™ä½œæ—¥æœŸğŸ“…" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-07-27T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="é˜…è¯»æ—¶é—´âŒ›" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>å¤§çº¦ 9 åˆ†é’Ÿ</span><meta property="timeRequired" content="PT9M"></span><span class="page-category-info" aria-label="åˆ†ç±»ğŸŒˆ" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category7 clickable" role="navigation">çŸ¥è¯†ç¬”è®°</span><!--]--><meta property="articleSection" content="çŸ¥è¯†ç¬”è®°"></span><span class="page-tag-info" aria-label="æ ‡ç­¾ğŸ·" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag4 clickable" role="navigation">AIGC</span><span class="page-tag-item tag6 clickable" role="navigation">LLM</span><!--]--><meta property="keywords" content="AIGC,LLM"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">æ­¤é¡µå†…å®¹<!----></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mistral-7b">mistral 7B</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mixtral-8-7b">Mixtral 8*7B</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mixtral-8-22b">Mixtral 8*22B</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mistral-nemo">Mistral Nemo</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mistral-large-2">Mistral Large  2</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢³ç†äº† 24 å¹´ 7 æœˆå‰ Mistral ç³»åˆ—æ¨¡å‹çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„ä¸»è¦ç‰¹ç‚¹ã€äº®ç‚¹ä»¥åŠç›¸å…³èµ„æºé“¾æ¥ã€‚æ¶‰åŠæ¨¡å‹ Mistral 7Bï¼Œ Mixtral 8x7Bï¼ŒMixtral 8x22Bï¼ŒMistral Nemo, Mistral Large 2</p><h2 id="mistral-7b" tabindex="-1"><a class="header-anchor" href="#mistral-7b" aria-hidden="true">#</a> mistral 7B</h2><p><a href="https://mistral.ai/news/announcing-mistral-7b/" target="_blank" rel="noopener noreferrer">å®˜æ–¹åšå®¢<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ï¼Œ<a href="https://arxiv.org/abs/2310.06825" target="_blank" rel="noopener noreferrer">mistral 7B è®ºæ–‡<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Mistral 7B æ¨¡å‹çš„äº®ç‚¹åŒ…æ‹¬ï¼š</p><ul><li><strong>Sliding Window Attention</strong></li></ul><p>Mistral é‡‡ç”¨çš„ window size ä¸º 4096ï¼Œè€Œåä¸€å…±æœ‰ 32 å±‚ layerï¼Œé‚£ä¹ˆé‡‡ç”¨ SWA ä¹‹åï¼Œç†è®ºä¸Šåœ¨è¿›è¡Œ attention çš„æ—¶å€™ï¼Œ <strong>ç†è®ºä¸Š</strong> å¯ä»¥æ”¶é›†åˆ°çº¦ 131K tokens çš„ä¿¡æ¯ã€‚(è™½ç„¶è®ºæ–‡é‡Œæåˆ°çš„ window size æ˜¯ 4096ï¼Œä½† å®˜æ–¹æä¾›çš„ <a href="https://huggingface.co/mistralai/mathstral-7B-v0.1/blob/main/config.json" target="_blank" rel="noopener noreferrer">huggingface ä¸Šçš„æƒé‡<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ä¸­ <code>max_position_embeddings</code> ä¸º 32768ï¼Œä¸”åœ¨æ–°ä¸€ç‚¹çš„ç‰ˆæœ¬ä¸­ï¼Œæ¯”å¦‚ <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/config.json" target="_blank" rel="noopener noreferrer">mistral-7b-instruct-v0.2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œéƒ½ä¸é‡‡ç”¨ sliding window äº†)</p><figure><img src="/assets/img/mistral/image-20240805103929202.jpg" alt="image-20240805103929202" tabindex="0" loading="lazy"><figcaption>image-20240805103929202</figcaption></figure><p>ç”±äºä»£ç”¨äº†å›ºå®šçš„ attention çª—å£å¤§å°ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªå¤§å°ä¸º <code>W=window size</code> çš„ cache ï¼Œåœ¨è®¡ç®—ç¬¬ i ä¸ª token çš„ cache çš„æ—¶å€™ï¼Œåªéœ€è¦è¦†ç›– cache ä¸­ <code>i mod M</code> ä½ç½®ä¸Šçš„ hidden state å³å¯ã€‚</p><p>å‚è€ƒ huggingface çš„ mistral å®ç°ï¼ŒSliding window attention é€šè¿‡ attention_mask æ¥æ§åˆ¶ï¼š</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># huggignface mistral attn mask å®ç°</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">_update_causal_mask</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">        self,</span></span>
<span class="line"><span style="color:#24292E;">        attention_mask: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        input_tensor: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        cache_position: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        past_key_values: Cache,</span></span>
<span class="line"><span style="color:#24292E;">    ):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># ... çœç•¥éƒ¨åˆ†æ— å…³ä»£ç </span></span>
<span class="line"><span style="color:#24292E;">        past_seen_tokens </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> cache_position[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> past_key_values </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">else</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span></span>
<span class="line"><span style="color:#24292E;">        using_static_cache </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(past_key_values, StaticCache)</span></span>
<span class="line"><span style="color:#24292E;">        using_sliding_window_cache </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(past_key_values, SlidingWindowCache)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        dtype, device </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> input_tensor.dtype, input_tensor.device</span></span>
<span class="line"><span style="color:#24292E;">        min_dtype </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.finfo(dtype).min</span></span>
<span class="line"><span style="color:#24292E;">        sequence_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> input_tensor.shape[</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># SlidingWindowCache</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> using_sliding_window_cache:</span></span>
<span class="line"><span style="color:#24292E;">            target_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">max</span><span style="color:#24292E;">(sequence_length, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># StaticCache</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">elif</span><span style="color:#24292E;"> using_static_cache:</span></span>
<span class="line"><span style="color:#24292E;">            target_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> past_key_values.get_max_length()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># DynamicCache or no cache</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            target_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> (</span></span>
<span class="line"><span style="color:#24292E;">                attention_mask.shape[</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(attention_mask, torch.Tensor)</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">else</span><span style="color:#24292E;"> past_seen_tokens </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> sequence_length </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span></span>
<span class="line"><span style="color:#24292E;">            )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">and</span><span style="color:#24292E;"> attention_mask.dim() </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">4</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># in this case we assume that the mask comes already in inverted form and requires no inversion or slicing</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask.max() </span><span style="color:#D73A49;">!=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">raise</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">ValueError</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;Custom 4D attention mask should be passed in inverted form with max==0`&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> attention_mask</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.full(</span></span>
<span class="line"><span style="color:#24292E;">                (sequence_length, target_length), </span><span style="color:#E36209;">fill_value</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">min_dtype, </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">dtype, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">device</span></span>
<span class="line"><span style="color:#24292E;">            )</span></span>
<span class="line"><span style="color:#24292E;">            exclude_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.arange(target_length, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">device) </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> cache_position.reshape(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> using_sliding_window_cache </span><span style="color:#D73A49;">or</span><span style="color:#24292E;"> sequence_length </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window:</span></span>
<span class="line"><span style="color:#24292E;">                    exclude_mask.bitwise_or_(</span></span>
<span class="line"><span style="color:#24292E;">                        torch.arange(target_length, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">device)</span></span>
<span class="line"><span style="color:#24292E;">                        </span><span style="color:#D73A49;">&lt;=</span><span style="color:#24292E;"> (cache_position.reshape(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">) </span><span style="color:#D73A49;">-</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.config.sliding_window)</span></span>
<span class="line"><span style="color:#24292E;">                    )</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">*=</span><span style="color:#24292E;"> exclude_mask</span></span>
<span class="line"><span style="color:#24292E;">            causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask[</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, :, :].expand(input_tensor.shape[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">], </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                causal_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask.clone()  </span><span style="color:#6A737D;"># copy to contiguous memory for in-place edit</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> attention_mask.dim() </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                    mask_length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> attention_mask.shape[</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">                    padding_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask[:, :, :, :mask_length] </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> attention_mask[:, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, :]</span></span>
<span class="line"><span style="color:#24292E;">                    padding_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> padding_mask </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span></span>
<span class="line"><span style="color:#24292E;">                    causal_mask[:, :, :, :mask_length] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> causal_mask[:, :, :, :mask_length].masked_fill(</span></span>
<span class="line"><span style="color:#24292E;">                        padding_mask, min_dtype</span></span>
<span class="line"><span style="color:#24292E;">                    )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> causal_mask</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>GQA (Grouped Query Attention)</strong></li></ul><p><a href="https://arxiv.org/abs/2305.13245" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><figure><img src="/assets/img/mistral/image-20240805103944726.jpg" alt="image-20240805103944726" tabindex="0" loading="lazy"><figcaption>image-20240805103944726</figcaption></figure><p>grouped-query attention æŒ‡å‡ºï¼Œ<a href="https://arxiv.org/pdf/1911.02150.pdf" target="_blank" rel="noopener noreferrer">Multi-Query Attention<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> æé«˜äº†æ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œå´å¯èƒ½æå¤§åœ°é™ä½å›å¤è´¨é‡ã€‚å› æ­¤æ ¹æ®ä¸Šå›¾ï¼ŒGQA åœ¨æ¨ç†é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´ä½œäº†æƒè¡¡ã€‚</p><p>ä»¥ä¸‹ä¸º GQA æ–‡ä¸­çš„å®éªŒç»“æœï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯è®ºæ–‡ä¸­ä½¿ç”¨åŸ MHA checkpoint è½¬æ¢ä¸º GQA æƒé‡åï¼Œè¿˜è¿›è¡Œäº†é¢å¤–çš„é¢„è®­ç»ƒï¼š</p><figure><img src="/assets/img/mistral/image-20240805103956240.jpg" alt="image-20240805103956240" tabindex="0" loading="lazy"><figcaption>image-20240805103956240</figcaption></figure><p>æ­¤å¤– Mistralï¼ŒLlama2 çš„éƒ¨åˆ†æ¨¡å‹ä½¿ç”¨ GQA æ—¶ï¼Œé‡‡ç”¨çš„ kv head æ•°é‡ä¼¼ä¹éƒ½æ˜¯ 8ã€‚</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/647130255" target="_blank" rel="noopener noreferrer">ä¸ºä»€ä¹ˆç°åœ¨å¤§å®¶éƒ½åœ¨ç”¨ MQA å’Œ GQAï¼Ÿ<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> æ–‡ä¸­æåˆ° MQA å’Œ GQA èƒ½è·å¾—å·¨å¤§åŠ é€Ÿçš„ä¸€ä¸ªç‚¹åœ¨äºï¼šGPU å†…å­˜å¼ºçš„é™åˆ¶ã€‚ç”±äº MQA å’Œ GQA éƒ½é™ä½äº†å†…å­˜ä¸­æ•°æ®çš„è¯»å–é‡ï¼Œå‡å°‘äº†è®¡ç®—å•å…ƒçš„ç­‰å¾…æ—¶é—´ï¼Œå› æ­¤æ¨ç†é€Ÿåº¦çš„æé«˜æ¯”æƒ³è±¡ä¸­çš„è¦å¿«æ›´å¤šã€‚</p></blockquote><h2 id="mixtral-8-7b" tabindex="-1"><a class="header-anchor" href="#mixtral-8-7b" aria-hidden="true">#</a> Mixtral 8*7B</h2><p><a href="https://arxiv.org/abs/2401.04088" target="_blank" rel="noopener noreferrer">è®ºæ–‡<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œ<a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1" target="_blank" rel="noopener noreferrer">huggingface æ¨¡å‹æƒé‡<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œ <a href="https://mistral.ai/news/mixtral-of-experts/" target="_blank" rel="noopener noreferrer">å®˜æ–¹åšå®¢<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œ<a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/mixtral/modeling_mixtral.py" target="_blank" rel="noopener noreferrer">huggingface æ¨¡å‹ä»£ç <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œ<a href="https://huggingface.co/blog/zh/moe" target="_blank" rel="noopener noreferrer">æ··åˆä¸“å®¶æ¨¡å‹åŸºç¡€ï¼ˆæ¨èï¼‰<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œå®˜æ–¹ç»™å‡ºçš„è¯„åˆ†æ¥çœ‹ï¼Œmixtral 8*7 å’Œ GPT3.5 æœ‰çš„ä¸€æ¯”ã€‚</p><ul><li><p>å‘å¸ƒæ—¶é—´ï¼š23 å¹´ 12 æœˆ</p></li><li><p>æ¨¡å‹å¤§å°ï¼š8 ä¸ª expert MLP å±‚ï¼Œä¸€å…± 45B å¤§å°ã€‚</p></li><li><p>è®­ç»ƒï¼šé™¤äº†é¢„è®­ç»ƒå¤–ï¼ŒMixtral MOE åç»­è¿˜å¼€æºäº†ä¸€ä¸ªç»è¿‡ SFT + DPO å¾®è°ƒçš„ç‰ˆæœ¬ã€‚</p></li><li><p>æ¨¡å‹æ•ˆæœï¼š</p></li></ul><figure><img src="/assets/img/mistral/image-20240805104006798.jpg" alt="image-20240805104006798" tabindex="0" loading="lazy"><figcaption>image-20240805104006798</figcaption></figure><ul><li>æ¶æ„ï¼šMixtral çš„ MOE æ¶æ„ç±»ä¼¼äºï¼Œåœ¨ MoE æ¨¡å‹ä¸­ï¼Œåªæœ‰ FFN å±‚è¢«è§†ä¸ºç‹¬ç«‹çš„ä¸“å®¶ï¼Œè€Œæ¨¡å‹çš„å…¶ä»–å‚æ•°æ˜¯å…±äº«çš„ã€‚å¤§è‡´å‚æ•°ä¸ºï¼š</li></ul><figure><img src="/assets/img/mistral/image-20240805104016338.jpg" alt="image-20240805104016338" tabindex="0" loading="lazy"><figcaption>image-20240805104016338</figcaption></figure><p>å¯¹ moe æ¶æ„ä¸å¤ªäº†è§£çš„æœ‹å‹ï¼Œå¯ä»¥å‚è€ƒè¿™ç¯‡åšå®¢ <a href="https://huggingface.co/blog/zh/moe" target="_blank" rel="noopener noreferrer">æ··åˆä¸“å®¶æ¨¡å‹åŸºç¡€ï¼ˆæ¨èï¼‰<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ã€‚</p><p>å‚è€ƒ huggingface ä¸­çš„ mixtral å’Œ mistral å®ç°å¯¹æ¯”ï¼Œå·®å¼‚åœ¨äº mixtral ä¸­å°†ä¼ ç»Ÿ transformer decoder layer ä¸­çš„ FFN æ›¿æ¢ä¸ºäº† <code>block_sparse_moe</code>ã€‚</p><figure><img src="/assets/img/mistral/image-20240723203836298.jpg" alt="https://github.com/open-compass/MixtralKit" tabindex="0" loading="lazy"><figcaption>https://github.com/open-compass/MixtralKit</figcaption></figure><p>ä¸»è¦é€»è¾‘ä¸ºï¼š</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><mi>T</mi><mi>o</mi><mi>p</mi><mi>K</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">â‹…</mo><msub><mi>W</mi><mrow><mi>g</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mtext>finalÂ hiddenÂ states</mtext><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>n</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo separator="true">â‹…</mo><msub><mi>E</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> G(x) = \text{Softmax}(TopK(x Â· W_{gate}))\\ \text{final hidden states} = \sum^{n-1}_{i=0} G(x)_iÂ·E_i(x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal">o</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">â‹…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">finalÂ hiddenÂ states</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0788em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8011em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">âˆ‘</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">âˆ’</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">â‹…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><p>å…¶ä¸­ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E_i(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> ä¸ºä¸“å®¶å¯¹åº”çš„ç½‘ç»œï¼Œå…·ä½“å±•ç¤ºä¸ºä¸‹é¢ huggingface å®ç°ä¸­çš„ <code>MixtralBlockSparseTop2MLP</code>ã€‚mixtral ä¸­é‡‡ç”¨äº† 8 ä¸ª expertï¼Œæ¯æ¬¡æ¨ç†ä½¿ç”¨é€‰å– top 2 çš„ expert è¿›è¡Œæ¨ç†ã€‚æ¯”å¦‚è¾“å…¥ä¸€å¥è¯ <code>ä½ å¥½ï¼Œä»Šå¤©</code>ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ¯ä¸ª token éƒ½ä¼šé€‰å‡º top 2 çš„ expert æ¥è´Ÿè´£è¿™ä¸ª token çš„é¢„æµ‹ï¼Œå› æ­¤åœ¨æ¨ç† <code>ä½ å¥½ï¼Œä»Šå¤©</code> æ—¶ï¼Œ <strong>æœ‰æ¦‚ç‡æ‰€æœ‰ expert éƒ½ä¼šå‚ä¸åˆ°è®¡ç®—å½“ä¸­</strong> ï¼Œå…·ä½“å¯ä»¥å‚è€ƒ <code>MixtralSparseMoeBlock</code> çš„å®ç°ã€‚</p><figure><img src="/assets/img/mistral/image-20240805104032194.jpg" alt="image-20240805104032194" tabindex="0" loading="lazy"><figcaption>image-20240805104032194</figcaption></figure><p>mixtral è®ºæ–‡ä¸­æåˆ°ä¸“å®¶åˆ†é…åœ¨ä¸åŒä¸»é¢˜ï¼ˆå¦‚ ArXiv è®ºæ–‡ã€ç”Ÿç‰©å­¦å’Œå“²å­¦æ–‡æ¡£ï¼‰ä¸­æ²¡æœ‰æ˜æ˜¾çš„æ¨¡å¼ï¼Œåªæœ‰åœ¨ DM æ•°å­¦ä¸­æ˜¾ç¤ºå‡ºè¾¹é™…ä¸Šçš„å·®å¼‚ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºå…¶æ•°æ®é›†çš„åˆæˆæ€§è´¨å’Œæœ‰é™çš„è‡ªç„¶è¯­è¨€è¦†ç›–èŒƒå›´æ‰€è‡´ã€‚router åœ¨æŸäº›å¥æ³•ç»“æ„ä¸Šè¡¨ç°å‡ºä¸€å®šçš„ç»“æ„åŒ–è¡Œä¸ºï¼ˆæ¯”å¦‚ python çš„ self ç­‰ï¼‰ï¼ŒåŒæ—¶è¿ç»­æ ‡è®°é€šå¸¸è¢«åˆ†é…ç»™ç›¸åŒçš„ä¸“å®¶ã€‚</p><ul><li>huggingface ä¸­çš„ mixtral æ ¸å¿ƒä»£ç ï¼š</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">MixtralDecoderLayer</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, config: MixtralConfig, layer_idx: </span><span style="color:#005CC5;">int</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_size </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.hidden_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.self_attn </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">MIXTRAL_ATTENTION_CLASSES</span><span style="color:#24292E;">[config._attn_implementation](config, layer_idx)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.block_sparse_moe </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MixtralSparseMoeBlock(config)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.input_layernorm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MixtralRMSNorm(config.hidden_size, </span><span style="color:#E36209;">eps</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">config.rms_norm_eps)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.post_attention_layernorm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> MixtralRMSNorm(config.hidden_size, </span><span style="color:#E36209;">eps</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">config.rms_norm_eps)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states: torch.Tensor,</span></span>
<span class="line"><span style="color:#24292E;">        attention_mask: Optional[torch.Tensor] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># æ­¤å¤„çœç•¥å‚æ•° ..</span></span>
<span class="line"><span style="color:#24292E;">    ) -&gt; Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        residual </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.input_layernorm(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states, self_attn_weights, present_key_value </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.self_attn(</span></span>
<span class="line"><span style="color:#24292E;">        	</span><span style="color:#6A737D;"># æ­¤å¤„çœç•¥å‚æ•° </span></span>
<span class="line"><span style="color:#24292E;">        )</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> residual </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        residual </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.post_attention_layernorm(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># Mixtral å°†åŸæœ¬çš„ hidden_states = self.FFN(hidden_states) æ›¿æ¢ä¸ºäº†ï¼š</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states, router_logits </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.block_sparse_moe(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> residual </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> hidden_states</span></span>
<span class="line"><span style="color:#24292E;">        outputs </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> (hidden_states,)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> outputs</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>huggingface ä¸­ <code>block_sparse_moe</code> çš„å®ç°ï¼ˆçœç•¥éƒ¨åˆ†æ¬¡è¦ä»£ç ï¼‰ï¼š</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">MixtralSparseMoeBlock</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, config):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.hidden_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.intermediate_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.num_local_experts</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.top_k </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.num_experts_per_tok</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.gate </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.experts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.ModuleList([MixtralBlockSparseTop2MLP(config) </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> _ </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts)])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.jitter_noise </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.router_jitter_noise</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:</span></span>
<span class="line"><span style="color:#24292E;">        batch_size, sequence_length, hidden_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states.shape</span></span>
<span class="line"><span style="color:#24292E;">        hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states.view(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">        router_logits </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.gate(hidden_states)  </span><span style="color:#6A737D;"># (batch * sequence_length, n_experts)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        routing_weights </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> F.softmax(router_logits, </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">torch.float)</span></span>
<span class="line"><span style="color:#24292E;">        routing_weights, selected_experts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.topk(routing_weights, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.top_k, </span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        routing_weights </span><span style="color:#D73A49;">/=</span><span style="color:#24292E;"> routing_weights.sum(</span><span style="color:#E36209;">dim</span><span style="color:#D73A49;">=-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#E36209;">keepdim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># we cast back to the input dtype</span></span>
<span class="line"><span style="color:#24292E;">        routing_weights </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> routing_weights.to(hidden_states.dtype)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        final_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.zeros(</span></span>
<span class="line"><span style="color:#24292E;">            (batch_size </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> sequence_length, hidden_dim), </span><span style="color:#E36209;">dtype</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">hidden_states.dtype, </span><span style="color:#E36209;">device</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">hidden_states.device</span></span>
<span class="line"><span style="color:#24292E;">        )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># One hot encode the selected experts to create an expert mask</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># this will be used to easily index which expert is going to be sollicitated</span></span>
<span class="line"><span style="color:#24292E;">        expert_mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.nn.functional.one_hot(selected_experts, </span><span style="color:#E36209;">num_classes</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts).permute(</span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># Loop over all available experts in the model and perform the computation on each expert</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> expert_idx </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.num_experts):</span></span>
<span class="line"><span style="color:#24292E;">            expert_layer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.experts[expert_idx]</span></span>
<span class="line"><span style="color:#24292E;">            idx, top_x </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.where(expert_mask[expert_idx])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># Index the correct hidden states and compute the expert hidden state for</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># the current expert. We need to make sure to multiply the output hidden</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># states by `routing_weights` on the corresponding tokens (top-1 and top-2)</span></span>
<span class="line"><span style="color:#24292E;">            </span></span>
<span class="line"><span style="color:#24292E;">            current_state </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> hidden_states[</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, top_x].reshape(</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># current_state: shape (n_i, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># æ‰€æœ‰ current_state çš„é•¿åº¦ n æ€»å’Œä¸º batch * sequence_length</span></span>
<span class="line"><span style="color:#24292E;">            current_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> expert_layer(current_state) </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> routing_weights[top_x, idx, </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># However `index_add_` only support torch tensors for indexing so we&#39;ll use</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># the `top_x` tensor here.</span></span>
<span class="line"><span style="color:#24292E;">            final_hidden_states.index_add_(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, top_x, current_hidden_states.to(hidden_states.dtype))</span></span>
<span class="line"><span style="color:#24292E;">        final_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> final_hidden_states, router_logits</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>å…¶ä¸­ï¼š <code>MixtralBlockSparseTop2MLP</code> é•¿è¿™æ ·ï¼š</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">MixtralBlockSparseTop2MLP</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, config: MixtralConfig):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.intermediate_size</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> config.hidden_size</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w1 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w2 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w3 </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.hidden_dim, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ffn_dim, </span><span style="color:#E36209;">bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.act_fn </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">ACT2FN</span><span style="color:#24292E;">[config.hidden_act]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, hidden_states):</span></span>
<span class="line"><span style="color:#24292E;">        current_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.act_fn(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w1(hidden_states)) </span><span style="color:#D73A49;">*</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w3(hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        current_hidden_states </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.w2(current_hidden_states)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> current_hidden_states</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="æ¨ç†" tabindex="-1"><a class="header-anchor" href="#æ¨ç†" aria-hidden="true">#</a> æ¨ç†</h4><p>æ ¹æ®æ¨¡å‹å‚æ•°é‡ 45B æ¥æ¨ç†çš„è¯ï¼Œå¦‚æœç”¨ fp16 çš„è¯æ¨ç†çš„è¯ï¼Œå¾—éœ€è¦è‡³å°‘ 90GB ä»¥ä¸Šçš„æ˜¾å­˜ï¼Œå¦‚æœç”¨ 4 bit çš„è¯ï¼Œ30GB æ˜¾å­˜å°±å¤Ÿäº†ã€‚é‡åŒ–çš„ç”Ÿæˆé€Ÿåº¦ï¼Œå¯ä»¥å‚è€ƒ<a href="https://www.reddit.com/r/LocalLLaMA/comments/18jslmf/tokens_per_second_mistral_8x7b_performance/" target="_blank" rel="noopener noreferrer">è¿™ä¸ª redis<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ä¸­çš„è¯„è®ºï¼Œå¤§è‡´ä¸º ï¼š</p><table><thead><tr><th>æ¨ç†ç²¾åº¦</th><th>è®¾å¤‡</th><th>é€Ÿåº¦ tokens/s</th></tr></thead><tbody><tr><td>Q4_K_M</td><td>å•å¡ 4090 + <strong>7950X3D</strong></td><td>20</td></tr><tr><td>Q4_K_M</td><td>2 x 3090</td><td>48.26</td></tr></tbody></table><p>å¦‚æœæœ‰ 100+GB ä»¥ä¸Šæ˜¾å­˜ï¼Œå¯ä»¥ç”¨ vllm å¿«é€Ÿæ­å»ºæµ‹è¯• apiï¼š</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="shiki github-light" style="background-color:#fff;" tabindex="0"><code><span class="line"><span style="color:#6F42C1;">docker</span><span style="color:#24292E;"> </span><span style="color:#032F62;">run</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">--gpus</span><span style="color:#24292E;"> </span><span style="color:#032F62;">all</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">-e</span><span style="color:#24292E;"> </span><span style="color:#032F62;">HF_TOKEN=</span><span style="color:#24292E;">$HF_TOKEN </span><span style="color:#005CC5;">-p</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">8000</span><span style="color:#032F62;">:8000</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">ghcr.io/mistralai/mistral-src/vllm:latest</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--host</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0.0</span><span style="color:#032F62;">.0.0</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--model</span><span style="color:#24292E;"> </span><span style="color:#032F62;">mistralai/Mixtral-8x7B-Instruct-v0.1</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">\</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#005CC5;">--tensor-parallel-size</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">2</span><span style="color:#24292E;"> </span><span style="color:#6A737D;"># 100+GB æ˜¾å­˜ \</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6F42C1;">--load-format</span><span style="color:#24292E;"> </span><span style="color:#032F62;">pt</span><span style="color:#24292E;"> </span><span style="color:#6A737D;"># needed since both `pt` and `safetensors` are available</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>NVIDIA çš„ <a href="https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm/?ncid=so-twit-928467/" target="_blank" rel="noopener noreferrer">TensorRT-LLM åšå®¢<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ä¸­å‘å‡ºäº†å¯¹ Mixtral 8*7B çš„ååé‡ benchmark ï¼ˆusing input and output sequence lengths of 128ï¼‰ï¼š</p><figure><img src="/assets/img/mistral/image-20240728094958591.jpg" alt="image-20240728094958591" tabindex="0" loading="lazy"><figcaption>image-20240728094958591</figcaption></figure><p>æ–‡ä¸­æ²¡æœ‰ç»™å‡ºå½“ sequence lengths æœ€å¤§æ—¶å€™çš„ååé‡ï¼Œä½†æ ¹æ®ä¸Šå›¾æ•°æ®ï¼Œå¯ä»¥çŒœæµ‹ 2 ä¸ª H100 éƒ¨ç½² 8*7B æ­£å¸¸æœåŠ¡ç”¨æˆ·æ—¶ï¼Œå¹³å‡ååé‡åº”è¯¥å¯ä»¥å¤§äº 7500Tokens/ç§’ï¼Œæ ¹æ® H100 çš„åŠŸè€—è®¡ç®—ç”µè´¹æˆæœ¬çš„è¯ï¼Œç”Ÿæˆ 1M token éœ€è¦è€—çº¦ä¸º 0.02 åº¦ç”µã€‚</p><h2 id="mixtral-8-22b" tabindex="-1"><a class="header-anchor" href="#mixtral-8-22b" aria-hidden="true">#</a> Mixtral 8*22B</h2><p><a href="https://mistral.ai/news/mixtral-8x22b/" target="_blank" rel="noopener noreferrer">å®˜æ–¹åšå®¢<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œ<a href="https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1" target="_blank" rel="noopener noreferrer">huggingface å¼€æºæ¨¡å‹<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œ</p><ul><li>æ¶æ„ï¼šæ¶æ„ä¸ mixtral 8*7B æ¶æ„ä¸€æ ·ï¼Œåœ¨ huggingface ä¸­ä½¿ç”¨çš„éƒ½æ˜¯<code>MixtralForCausalLM</code> ï¼Œä½† 22B çš„å„æ–¹é¢å‚æ•°å¤§ä¸€ç‚¹ï¼Œæ¯”è¾ƒç‰¹åˆ«çš„æ˜¯ context window ä» 32k å‡çº§åˆ°äº† 65kï¼Œ <code>vocab_size</code> ä¹Ÿæ›´å¤§ä¸€äº›ã€‚</li><li>æ”¯æŒ function callingï¼Œä¸è¿‡å¥½åƒæ²¡æœ‰é€éœ²å…·ä½“çš„ function calling è®­ç»ƒç»†èŠ‚ã€‚</li><li>æ•°å­¦å’Œ coding èƒ½åŠ›æ˜æ˜¾è¶…è¶Š llama2 70B</li><li>ä¼¼ä¹å¯¹ä¸­æ–‡çš„æ”¯æŒä¸æ˜¯å¾ˆå¥½ã€‚</li></ul><figure><img src="/assets/img/mistral/image-20240727152529176.jpg" alt="image-20240727152529176" tabindex="0" loading="lazy"><figcaption>image-20240727152529176</figcaption></figure><p>Mistral å›¢é˜Ÿå¼€æºçš„æ¨¡å‹ï¼Œéƒ½æ¯”è¾ƒæ³¨é‡ coding å’Œ math çš„èƒ½åŠ›ï¼ŒMixtral ç³»åˆ—çš„æ¨¡å‹åœ¨è¿™æ–¹ä¾¿è¡¨ç°ä¹Ÿæ˜¯æ¯”è¾ƒå¥½ï¼š</p><figure><img src="/assets/img/mistral/image-20240727152702974.jpg" alt="image-20240727152702974" tabindex="0" loading="lazy"><figcaption>image-20240727152702974</figcaption></figure><h2 id="mistral-nemo" tabindex="-1"><a class="header-anchor" href="#mistral-nemo" aria-hidden="true">#</a> Mistral Nemo</h2><p><a href="https://mistral.ai/news/mistral-nemo/" target="_blank" rel="noopener noreferrer">å®˜æ–¹åšå®¢<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œ<a href="https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407" target="_blank" rel="noopener noreferrer">huggingface æ¨¡å‹æƒé‡<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Mistral Nemo ä½¿ç”¨çš„ä¹Ÿæ˜¯ <code>MistralForCausalLM</code> æ¶æ„ï¼Œä¸ mistral 7B çš„å·®åˆ«ä¸ºï¼šMistral Nemo çš„ <code>hidden_size</code> ä» 4096 å˜ä¸º 5120ï¼›<code>max_position_embeddings</code> å˜ä¸º 1024000ï¼Œ<code>num_hidden_layers</code> å¢åŠ åˆ° 40ï¼Œ vocab_size å¢åŠ åˆ° 131072ï¼Œä¸ç”¨ sliding windowã€‚</p><ul><li>æ”¯æŒ function callingï¼</li><li>é‡‡ç”¨äº† Tekken ä½œä¸º tokenizerï¼Œæ¯” SentencePiece æ›´é«˜æ•ˆï¼ˆå‹ç¼©ç‡æ›´é«˜ï¼Œå®˜æ–¹æè¿°æ˜¯~30% more efficient at compressingï¼Œä¸ç¡®å®šæ˜¯å“ªä¸ªæ–¹é¢çš„ efficientï¼‰</li></ul><p>NVIDIA åœ¨<a href="https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/" target="_blank" rel="noopener noreferrer">è¿™ä¸ªåšå®¢<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ä¸­æåˆ°ï¼šMistral Nemo é‡‡ç”¨è¿™æ ·çš„è®¾è®¡ï¼Œæ˜¯ä¸ºäº†èƒ½å¤Ÿé€‚é…å•ä¸ª NVIDIA L40Sã€NVIDIA GeForce RTX 4090 æˆ– NVIDIA RTX 4500 GPUã€‚æ¨¡å‹é‡‡ç”¨ <a href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noopener noreferrer"> Megatron-LM<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> è®­ç»ƒï¼Œç”¨äº† 3,072 ä¸ª H100 80GB ã€‚</p><p>ä½†å…‰é‡‡ç”¨ FP16 åŠ è½½æ•´ä¸ª Mistral Nemo å°±éœ€è¦èŠ± 23 GB æ˜¾å­˜ï¼Œè¦æ˜¯è¦è·‘æ»¡æ•´ä¸ª context window sizeï¼Œé™¤äº†é‡åŒ–å¤–ï¼Œè¿˜æ˜¯å¾—éœ€è¦é‡‡ç”¨ offload æˆ–è€…å…¶ä»–æ–¹æ³•æ¥æ¨ç†</p><p>ä¸è¿‡ mistral å®˜æ–¹æŠŠ 12 B çš„æ¨¡å‹å’Œå…¶ä»– 8B çš„æ¨¡å‹å¯¹æ¯”ï¼Œæ„Ÿè§‰å¥½åƒä¸å¤ªå…¬å¹³ï¼š</p><figure><img src="/assets/img/mistral/image-20240727154936831.jpg" alt="image-20240727154936831" tabindex="0" loading="lazy"><figcaption>image-20240727154936831</figcaption></figure><h2 id="mistral-large-2" tabindex="-1"><a class="header-anchor" href="#mistral-large-2" aria-hidden="true">#</a> <strong>Mistral Large</strong> 2</h2><p><a href="https://mistral.ai/news/mistral-large-2407/" target="_blank" rel="noopener noreferrer">å®˜æ–¹åšå®¢<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼Œ <a href="https://huggingface.co/mistralai/Mistral-Large-Instruct-2407" target="_blank" rel="noopener noreferrer">huggingface æ¨¡å‹æƒé‡<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Mistral Large 2ï¼Œå‚æ•°é‡ 123Bï¼Œä¸»æ‰“å¤šè¯­è¨€ä»¥åŠ coding èƒ½åŠ›ã€‚é‡‡ç”¨ä¸ mistral 7B ä¸€æ ·çš„æ¶æ„ï¼Œhuggingface ä¸­åŒæ ·ä½¿ç”¨ <code>MistralForCausalLM</code>ï¼›æ¯”è¾ƒå€¼å¾—æ³¨æ„çš„æ˜¯ context window size ä¸º 131072ï¼Œä¸ç”¨ sliding windowã€‚</p><p>Llama 3.1 åˆšå‡ºä¸ä¹…ï¼Œå°±æ‹¿ <strong>Mistral Large</strong> 2 å’Œåˆ«äººæ¥å¯¹æ¯”ï¼š</p><figure><img src="/assets/img/mistral/image-20240805104145922.jpg" alt="image-20240805104145922" tabindex="0" loading="lazy"><figcaption>image-20240805104145922</figcaption></figure><p>åœ¨ä»£ç èƒ½åŠ›ä¸Šï¼ŒMistral large 2 æ¯” llama 3.1 å¹³å‡æ•ˆæœæ›´å¥½ã€‚</p><figure><img src="/assets/img/mistral/image-20240727201347583.jpg" alt="image-20240727201347583" tabindex="0" loading="lazy"><figcaption>image-20240727201347583</figcaption></figure><p>é™¤äº† coding å’Œæ•°å­¦å¤–ï¼Œåœ¨ MT Bench çš„è¯„åˆ†ä¹Ÿæ¯” llama 3.1 é«˜ï¼Œå¹³å‡ç”Ÿæˆçš„å›å¤é•¿åº¦æ¯” llama 3.1 è¦çŸ­</p><figure><img src="/assets/img/mistral/image-20240805104200493.jpg" alt="image-20240805104200493" tabindex="0" loading="lazy"><figcaption>image-20240805104200493</figcaption></figure><p>åŒæ—¶ï¼Œä¸­æ–‡èƒ½åŠ›ç›¸å¯¹ä¸Šä¸€ä»£ mistral large æœ‰å¤§æ­¥å¹…æå‡ï¼š</p><figure><img src="/assets/img/mistral/image-20240805104212135.jpg" alt="image-20240805104212135" tabindex="0" loading="lazy"><figcaption>image-20240805104212135</figcaption></figure></div><!----><footer class="page-meta"><!----><div class="meta-item git-info"><div class="update-time"><span class="label">ä¸Šæ¬¡ç¼–è¾‘äº: </span><!----></div><div class="contributors"><span class="label">è´¡çŒ®è€…: </span><!--[--><!--[--><span class="contributor" title="email: 417333277@qq.com">kevinng77</span><!--]--><!--]--></div></div></footer><!----><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer"><a href="https://beian.miit.gov.cn/" target="_blank">æ²ªICPå¤‡2023027904å·-1</a>&nbsp;<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=" target="_blank"><img src="/assets/gongan.png">å…¬å®‰å¤‡æ¡ˆå· </a></div><div class="vp-copyright">Copyright Â© 2025 Kevin å´å˜‰æ–‡</div></footer></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-80ce1db6.js" defer></script>
  </body>
</html>
