<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="http://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0nlu_pretrain.html"><meta property="og:site_name" content="记忆笔书"><meta property="og:title" content="NLP 预训练小综述"><meta property="og:description" content="预训练模型的前世今生 看完了 Pre-Trained Models: Past, Present and Future。对目前主流 NLP 预训练模型、预训练方式做个小结与梳理。 自从 ELMO，GPT，BERT 问世，基于大规模预料的预训练模型便开始流行起来。学者们的注意力渐渐从模型架构转移到了预训练上。预训练+微调的方式也创造了不少下游任务 SOTA。"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2023-03-26T07:48:04.000Z"><meta property="article:author" content="Kevin 吴嘉文"><meta property="article:tag" content="NLP"><meta property="article:published_time" content="2022-03-22T00:00:00.000Z"><meta property="article:modified_time" content="2023-03-26T07:48:04.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"NLP 预训练小综述","image":[""],"datePublished":"2022-03-22T00:00:00.000Z","dateModified":"2023-03-26T07:48:04.000Z","author":[{"@type":"Person","name":"Kevin 吴嘉文"}]}</script><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;700&display=swap" rel="stylesheet"><link rel="icon" href="/logo.svg"><title>NLP 预训练小综述 | 记忆笔书</title><meta name="description" content="预训练模型的前世今生 看完了 Pre-Trained Models: Past, Present and Future。对目前主流 NLP 预训练模型、预训练方式做个小结与梳理。 自从 ELMO，GPT，BERT 问世，基于大规模预料的预训练模型便开始流行起来。学者们的注意力渐渐从模型架构转移到了预训练上。预训练+微调的方式也创造了不少下游任务 SOTA。">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-99575b2e.css" as="style"><link rel="stylesheet" href="/assets/style-99575b2e.css">
    <link rel="modulepreload" href="/assets/app-82c5f5e3.js"><link rel="modulepreload" href="/assets/笔记nlu_pretrain.html-e3f43d32.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/assets/笔记nlu_pretrain.html-ecffedc1.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><!----><!----><span class="vp-site-name">记忆笔书</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="主页" class="vp-link nav-link nav-link" href="/"><span class="font-icon icon iconfont icon-home" style=""></span>主页<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="归档" class="vp-link nav-link nav-link" href="/timeline/"><span class="font-icon icon iconfont icon-categoryselected" style=""></span>归档<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/kevinng77" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!----><!--[--><button type="button" class="search-pro-button" role="search" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->NLP 预训练小综述</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Kevin 吴嘉文</span></span><span property="author" content="Kevin 吴嘉文"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2022-03-22T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 16 分钟</span><meta property="timeRequired" content="PT16M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category7 clickable" role="navigation">知识笔记</span><!--]--><meta property="articleSection" content="知识笔记"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag8 clickable" role="navigation">NLP</span><!--]--><meta property="keywords" content="NLP"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<!----></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#深度神经网络">深度神经网络</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#预训练模型的发展">预训练模型的发展</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#从-bert-的-mlm-开始">从 BERT 的 MLM 开始</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#更好的学习字符知识">更好的学习字符知识</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#字符之上">字符之上</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#预训练模型目前情况">预训练模型目前情况</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#未来的预训练模型">未来的预训练模型</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#预训练模型关键点总结">预训练模型关键点总结</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#论文">论文</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><h1 id="预训练模型的前世今生" tabindex="-1"><a class="header-anchor" href="#预训练模型的前世今生" aria-hidden="true">#</a> 预训练模型的前世今生</h1><blockquote><p>看完了 <a href="http://arxiv.org/abs/2106.07139" target="_blank" rel="noopener noreferrer">Pre-Trained Models: Past, Present and Future<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。对目前主流 NLP 预训练模型、预训练方式做个小结与梳理。</p></blockquote><p>自从 ELMO，GPT，BERT 问世，基于大规模预料的预训练模型便开始流行起来。学者们的注意力渐渐从模型架构转移到了预训练上。预训练+微调的方式也创造了不少下游任务 SOTA。</p><!--more--><h3 id="深度神经网络" tabindex="-1"><a class="header-anchor" href="#深度神经网络" aria-hidden="true">#</a> 深度神经网络</h3><p>深度神经网络早期主要面临数据少，模型规模受硬件限制等问题。回顾大部分 3 到 4 年前发布的 NLP 文章，大多研究关注与如何让模型更有效从数据集中获取知识。如发掘更优质的人工标注 <strong>数据集</strong> 、更好的 <strong>模型架构</strong> 或更完善的 <strong>特征工程</strong> 。</p><p>过去两年，NLP 的目光转移到了 <strong>预训练+微调</strong> 。类似 CV 领域的迁移学习 （Transfer Learning），预训练可提升在小规模训练集上的训练效果。早期基于词向量的预训练方案（如 Word2Vec、Glove ）有所效果，但无法解决一词多意等问题；在 BERT/GPT 等模型出现后，各式各样的预训练方案被提出，模型学习的质量得以提高。</p><h2 id="预训练模型的发展" tabindex="-1"><a class="header-anchor" href="#预训练模型的发展" aria-hidden="true">#</a> 预训练模型的发展</h2><figure><img src="/assets/img/nlu_pretrain/image-20220324220432793.png" alt="image-20220324220432793" tabindex="0" loading="lazy"><figcaption>image-20220324220432793</figcaption></figure><p><em>(图：预训练模型概述。来源：<a href="http://arxiv.org/abs/2106.07139" target="_blank" rel="noopener noreferrer">Pre-Trained Models: Past, Present and Future<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 截图)</em></p><p>文字语言由无数的字符组成，从字符、单词、句子、段落到一篇文章、一个书、一个论坛、一个网站等。每个层面都藏有深奥的语义知识。对预训练模型做一个小总结的话，可以感受到预训练任务正在从学习最基础的字符之间的关系拓展到句子、段落、话题等。</p><h3 id="从-bert-的-mlm-开始" tabindex="-1"><a class="header-anchor" href="#从-bert-的-mlm-开始" aria-hidden="true">#</a> 从 BERT 的 MLM 开始</h3><blockquote><p>拓展 - 分词：为了解决词汇问题，大规模预训练模型通常采用 subword（<a href="https://zhuanlan.zhihu.com/p/86965595" target="_blank" rel="noopener noreferrer">深入理解 NLP Subword 算法：BPE、WordPiece、ULM<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>）创建词表。常见的 subword 方式有 BPE（OpenAI <a href="https://link.zhihu.com/?target=https%3A//towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655" target="_blank" rel="noopener noreferrer">GPT-2 <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>与 Facebook <a href="https://link.zhihu.com/?target=https%3A//github.com/pytorch/fairseq/tree/master/examples/roberta" target="_blank" rel="noopener noreferrer">RoBERTa<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>）, WordPiece（BERT），Unigram Language Model 等。</p></blockquote><p>采用 Transformer Encoder 架构的 BERT 在预训练时，随机对部分分词（subword）进行了掩码（如 <code>dog, ##ing</code> 变 <code>[MASK] </code> ）。对于中文，上述对分词随机掩码的方式存在分割单词、破坏语义的情况，因此 <strong>BERT-WWM</strong> （Pre-Training with Whole Word Masking for Chinese BERT ）提出了 <strong>whole word mask</strong> 。针对整个 <strong>中文单词</strong> 进行掩码。</p><p>BERT 采用的是 Static Mask 的方式，即在训练前准备好 mask 了的预料。因此每轮 epoch 中，相同预料的 <code>[MASK]</code> 位置是一样的。 <strong>RoBERTa</strong> 提出 <strong>Dynamic Mask</strong> ，即在数据输入模型之前进行随机的 <code>[MASK]</code>。实验证明 Dynamic 在部分任务上略优于 BERT 初始方案。</p><p>BERT MLM 缺点之一是他是自编码结构，虽然有了位置编码，但是学习语言结构信息上还是没有自回归结构强。 <strong>XLNET</strong> 提出了 <strong>PLM</strong> ，增强了对单词结构顺序的学习。PLM 任务首先将单个输入语句的单词打乱重排，而后遮掩掉句子末尾的 N 个词（理论上是这样，但实际训练时只需要对 Attention Mask 进行处理就行，不需要真的排序）。此时使用自回归的方式来预测遮掩的词，就能够学习到不同位置的信息了。</p><h3 id="更好的学习字符知识" tabindex="-1"><a class="header-anchor" href="#更好的学习字符知识" aria-hidden="true">#</a> 更好的学习字符知识</h3><p>MLM 并非完美，过去两年，多种基于字符级别的预训练任务被提出。</p><p><strong>对于提升对语言架构的更高效学习：</strong></p><p>以加强段落中不同位置单词之间的联系， <strong>SpanBERT</strong> 添加了 SBO（ <strong>Span Boundary Objective</strong> ）。给定语句：<code>今天一起去踢足球吧</code>，进行掩码后变为 <code>今天一起 XXXX 吧</code> 。假设语句中的 <code>足</code> 词被遮掩且对应位置为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>7</mn></msub></mrow><annotation encoding="application/x-tex">x_7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">7</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。MLM 通过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>7</mn></msub></mrow><annotation encoding="application/x-tex">x_7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">7</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 对应的信息推测 <code>足</code> ，目标函数为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mrow><mo fence="true">(</mo><mtext> 足 </mtext><mo>∣</mo><msub><mi mathvariant="bold">x</mi><mn>7</mn></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\log P\left(\text { 足 } \mid \mathbf{x}_{7}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord text"><span class="mord"> </span><span class="mord cjk_fallback">足</span><span class="mord"> </span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">7</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span>；而 SBO 则是通过 Span 掩码两端的信息预测单词，目标函数为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mrow><mo fence="true">(</mo><mtext> 足 </mtext><mo>∣</mo><msub><mi mathvariant="bold">x</mi><mn>4</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mn>9</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">p</mi><mn>3</mn></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\log P\left(\text { 足 } \mid \mathbf{x}_{4}, \mathbf{x}_{9}, \mathbf{p}_{3}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord text"><span class="mord"> </span><span class="mord cjk_fallback">足</span><span class="mord"> </span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">9</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span> 其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>4</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>9</mn></msub></mrow><annotation encoding="application/x-tex">x_4,x_9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为 span 两端的 hidden state，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">p_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 为 <code>足</code> 的相对位置编码。</p><p>同时，为提高模型对与输入的鲁棒性， <strong>BART</strong> 提出 <strong>Token Deletion</strong> ， 删除某些字符，让模型预测删除字符的位置。</p><p>BERT MLM 存在的一个问题（直观上的，并无证明）是，由于语料中单词的分布是不均匀的，使得训练出来的表征分布在空间分布上不均匀。 <strong>ELECTR</strong> 提出了 RTD （ <strong>Replace Token Detection</strong> ），类似对比学习，一定程度上改善上述问题。训练时 RTD 需要配合 MLM，类似 GAN，构造生成器（BERT）与判别器（ELECTRA），生成器输入带有 <code>[MASKED]</code> 的语句，输出生成语句，判别器判断语句中哪个词生成错了；训练采用 Two-stage traning：先训练生成器，冻结后在训练判别器，依此循环。</p><p><strong>对于学习更多单词本身的信息：</strong></p><p><strong>ERINE</strong> （清华）提出了针对 <strong>命名体</strong> Entity Phrase 进行 Mask，在完成原先 MLM 任务的基础上，额外完成对命名体类别的分类 （ <strong>Entity Typing</strong> ），在这个任务中，预训练模型也融入了知识图谱相关的知识。</p><p>单词本身的特性也值得提取， <strong>ERINE-2.0</strong> 提出了 <strong>Capitalization Prediction</strong> ，大写的词比如 Apple 相比于其他词通常在句子当中有特定的含义，所以在 Erine 2.0 加入一个任务来判断一个词是否大写。此外，还提出了 <strong>Token-Document Relation</strong> 任务，预测段落中的某一单词，是否会在同一文章的其他段落出现 ，该任务可用于主题词预测，关键词抓取等。</p><h3 id="字符之上" tabindex="-1"><a class="header-anchor" href="#字符之上" aria-hidden="true">#</a> 字符之上</h3><p>从语言结构角度看，字符更上一层便是短语、句子。过去两年也有许多预训练任务为它们而生。</p><p><strong>短语级别来看：</strong></p><p><strong>SpanBERT</strong> 提出了对连续的片段 span 进行掩码。在实验中，基于 <strong>Span/N-gram</strong> （SpanBERT）掩码达到很好的效果（比 BERT 原始方案和其他几种掩码策略好）。做法是：从一个分布中随机选取 Span 的长度，而后算计选取 span 开始的位置，此位置必须是一个单词的开始，span 掩码包含的总是完整的单词。预训练的目标是预测被掩码的全部内容。</p><p>同样是对连续片段掩码，不同与 SpanBERT 的双向模型结构， <strong>MASS</strong> 采用了 encoder-decoder 架构，并使用自回归的方式对掩码部分进行了预测。与 XLNet 对 BERT 的修改有异曲同工之处。</p><p>MASS 是一次对一片掩码进行预测。 <strong>ERNIE-GEN</strong> 提出了 <strong>span-by-span generation</strong> ，遮掩随机的几个片段，在 decoder 部分统一对它们进行预测预测（ERNIE-GEN 模型为 ENCODER 结构，预测时参考了 UNILM 对 Attention Mask 进行修改）。</p><p><strong>句子级别来看：</strong></p><p>BERT 提出了二分类任务 <strong>NSP</strong> ，根据 CLS 位置的输出，预测 AB 两个输入语句是否是上下文关系。50%时候，B 是 A 的下一句，标注为 isNEXT。其他时间，B 是文本中的随机一个句子，标注为 NotNEXT。但 RoBERTa、SpanBERT、ALBERT 等提出该任务会拉底模型效果，并在预训练中去除了 NSP。</p><p>为了提升句子级别的预训练， <strong>ALBERT</strong> 提出了 SOP（ <strong>sentence order prediction</strong> ），同样为二分类任务，正样本与 NSP 相同，但负样本则将句子的位置调换，提高了训练难度。</p><p>相比于判断上下文关系，对几个句子进行重排难度更大。 <strong>ERNIE2.0</strong> 提出 <strong>Sentence Reordering</strong> 在训练当中，将段落随机分成 1 到 k 段并打乱它们的顺序，预训练模型的目标是判断这些句子的顺序（为每个句子预测一个顺序值，k 分类任务）。</p><p><strong>ERNIE-2.0</strong> 中还有其他针对句子提出的预训练任务，如判断句子距离的三分类任务 <strong>Sentence Distance</strong> ，针对两个句子 AB，输出：0 表示 AB 为文章中相邻句子；1 表示 AB 为同文章中不相邻句子；2 表示 AB 属于不同文章。还有 <strong>IR Relevance</strong> ，判断一个句子和一个标题的相关性（强关系/弱关系/无关系）。以及 <strong>Discourse Relation</strong> ，判断句对之间的修辞关系（semantic or rhetorical relation）。最后两个任务都为监督任务，需要在标注数据集上训练。</p><p><strong>ERINE 1.0</strong> 采用 DLM（ <strong>Dialogue Language Model</strong> ）建模 Query-Response 对话结构，将对话 Pair 对作为输入，引入 Dialogue Embedding 标识对话的角色，利用 Dialogue Response Loss 学习对话的隐式关系，通过该方法建模进一步提升模型语义表示能力。</p><h3 id="预训练模型目前情况" tabindex="-1"><a class="header-anchor" href="#预训练模型目前情况" aria-hidden="true">#</a> 预训练模型目前情况</h3><p>模型解释性主要探索的方向有：</p><p>Representation Probing：冻结 PTM 权重，链接额外的线性层。在不同的任务上进行测试与观察 Representation Analysis：对隐状态进行统计学的分析，如相似度，距离等等。 Attention analysis：对注意力矩阵进行分析。 Generation Analysis：直接评测生成不同句子和单词的概率、分布</p><p>目前来说，模型的鲁棒性似乎还是不好，主要采用对抗训练来提高模型鲁棒性。</p><p>高效训练模型方面，通常采用精度调整（如双精度训练）；分布式训练；探索更高效的模型架构来取代 Transformers；参数共享；模型剪枝、蒸馏等。</p><h3 id="未来的预训练模型" tabindex="-1"><a class="header-anchor" href="#未来的预训练模型" aria-hidden="true">#</a> 未来的预训练模型</h3><p>如何让模型更智慧？对于各种问题，主要的解决方向为优化模型架构，采用更多更好的数据，采用多样化的训练方案。</p><p>而目前，训练方式似乎成为了主流研究方向（个人感受）？包括更好的融入多模态、使用多重下游任务来提升预训练模型效果、融入知识图谱知识等。不确定 NLP 预训练模型是否会有大一统。</p><p>模型在应用层面的优化防线也有很多：</p><ul><li>提高基于少量资源微调的效果；</li><li>为领域预训练提供更好的数据集；</li><li>更高效的模型压缩；更好的硬件支持</li><li>提高鲁棒性、更正模型逻辑能力。（GPT-3 面对 &quot;How many eyes does my foot have?&quot; 问题，会回复 “Your foot has two eyes” ）</li></ul><p>似乎 Pretrain + Prompt 的方式可以更好的为模型融入更多下游任务的知识，提供模型质量，期待 NLP 技术在未来的发展。 <a href="https://zhuanlan.zhihu.com/p/465130047" target="_blank" rel="noopener noreferrer">后 Prompt 时代｜NLP 大一统建模范式：预训练+大规模多任务学习<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h2 id="预训练模型关键点总结" tabindex="-1"><a class="header-anchor" href="#预训练模型关键点总结" aria-hidden="true">#</a> 预训练模型关键点总结</h2><p><strong>ELMO</strong></p><ul><li>feature base；采用双向的 Bi-LSTM</li></ul><p><strong>OpenAI - GPT</strong></p><ul><li>parameter base；自回归模型，使用 transformer 模块（一个 Casual Masked 的 MHA 加上 FFN 层）；使用预训练 + 微调结构，微调通常使用全连接层处理最后一个位置的隐状态。</li></ul><p><strong>BERT</strong></p><ul><li><p>采用 transformer encoder 的双向结构</p></li><li><p>预训练任务：Masked LM：遮掩一部分的 token，并在输出层预测他们；NSP：二分类任务，判断两个句子是否为上下文关系（部分前后）。</p></li></ul><p><strong>升级版：</strong></p><p><strong>SpanBERT:</strong></p><ul><li>预训练任务：去除 NSP；采用 Span Mask 处理 MLM 任务；</li><li>目标函数添加 Span Boundary Objective</li></ul><p><strong>OpenAI - GPT2</strong></p><ul><li>自回归模型，结构与 GPT 相似，但使用 pre norm；Tokenizer 使用了特殊的 BPE；采用 query 来引导模型进行不同任务的训练（multi task learning）；zero shot 有效的一种解释是：训练的对话集中，包含有任务所需要的知识，如论文中展示的英法翻译。</li></ul><p><strong>ALBERT</strong></p><ul><li>架构方面，基于 BERT 的架构对 embedding 层进行了因式分解，共享层与层之间的模型参数（cross-layer parameter sharing）实现表明共享全部（FNN、Attention）权重下，模型表现也不会差太多；</li><li>预训练方便：去除 NSP，采用 SOP（sentence order prediction）正样本与 NSP 相同，但负样本则将句子的位置调换。</li></ul><p><strong>UNILM</strong></p><ul><li>BERT 与 GPT 主要是差了一个 Attention Mask。因此 UNILM 通过修改 Attention Mask 进行 NLU + NLG 的多任务学习，实现 BERT 处理 NLG 任务。</li></ul><p><strong>RoBERTa</strong></p><ul><li>预训练方便：去除了 NSP；使用了 Dynamic Mask 进行 MLM 任务；训练时间更长，batch size 更大。</li><li>结构方便：Tokenizer 参考 GPT2 使用了特殊的 BPE，句子长度加长</li></ul><p><strong>ERNIE(清华)</strong></p><ul><li>预训练方面：除了 MLM 外，添加新 entity typing 任务，对实体进行掩码以及类别预测，构造输入时， <strong>使用 TransE 模型的实体预训练 embedding 作为额外信息</strong> ，与原输入中标记的实体进行拼接；使用 <code>[ENT],[HD]</code> 等 token 进行任务提示，如<code>[ENT]</code> 用于标出实体位置，便于 entity typing 任务；采用 NSP。</li></ul><p><strong>ERNIE-2.0</strong></p><ul><li>预训练方面：尝试使用额外的预训练任务，学习处基础语法之外的信息，包括： Knowledge masking (ERNIE-1), Capitalization Prediction, Token-Document Relation Prediction, Sentence Reordering, Sentence Distance, Discourse Relation, IR Relevance；采用 Continual multi-task Learning 训练时逐步增加同时训练的任务数量，不同于 multi-task 和 continual learning。</li></ul><p><strong>MASS</strong></p><ul><li>4 层 transformer 结构。对句子中连续部分进行 mask，而后通过 decoder 进行自回归预测。decoder 只预测被 masked 的部分，未 masked 的部分使用 <code>[M]</code> 作为输入。</li></ul><p><strong>XLNET</strong></p><ul><li>预训练方面：提出 PLM，对 token 进行排序后在 mask，使用自回归的方式完成预训练；采用了双流注意力机制。</li></ul><p><strong>MPNet</strong></p><ul><li>解决 XLNET 微调时位置信息与预训练不匹配问题，即预训练时候模型知道语句长度，而微调时候不知道。</li></ul><p><strong>ELECTRA</strong></p><ul><li>预训练方面：采用 MLM + RTD（Replace Token Detection），类似 GAN，构造生成器（BERT）与判别器（ELECTRA），生成器输入带有 <code>[MASKED]</code> 的语句，输出生成语句，判别器判断语句中哪个词生成错了；训练采用 Two-stage traning：先训练生成器，冻结后在训练判别器，依次循环；采用 Adversarial Contrastive Estimation。</li></ul><p><strong>T5</strong></p><ul><li>论文很长（实验性研究）：encoder-decoder 的模型似乎效果更好；所有任务使用生成任务完成（Text-to-Text）; 预训练 MLM 时采用 Span Maksed。如原句 <code>今天天气真的非常好，是吧。</code> encoder 输入 <code>今天[x]真的[y]好，是吧</code> 。decoder 输出 <code>[x]天气[y]非常</code></li></ul><p><strong>BART</strong></p><ul><li>encoder-decoder 结构; 预训练方面：采用字词级别预训练：Token masking, Token deletion; Text infilling（spanBERT） ；加强句子间预训练：Sentence permutation（多分类）, document rotation</li></ul><p><strong>GPT-3</strong></p><ul><li>论文也很长，1700 亿参数，结构类似 GPT-2；zero-shot，无需微调，支持在输入中加入样例来实现 one-hot/few-shot；</li></ul><p><strong>NEZHA</strong></p><ul><li>Functional Relative Positional Encoding 相对位置编码，解决 bert 末端位置编码更新少的问题；采用 Whole Word Masking，使用 jieba 分词；采用 Mixed Precision Training，权重使用 FP32 储存与梯度更新，前向后向传导使用 FP16；采用 LAMB Optimizer 自适应调整学习率</li></ul><p><strong>ERNIE-GEN</strong></p><ul><li><p>处理由 teacher force 带来的 训练与预测时输入不一致问题。</p></li><li><p><strong>infilling generation</strong> ：每个需要预测的 token 位置上，额外添加一个<code>[attn]</code> token 来汇聚上文信息，模型输出的位置为 <code>[attn]</code>对应的位置，降低输出对上一个词的依赖。（类似于 decoder 的输入改成 <code>[attn]</code>序列）</p></li><li><p><strong>Noise-Aware generation</strong> ：随机进行单词替换，提高模型容错率</p></li><li><p><strong>span-by-span generation</strong> ：遮掩随机的几个片段，在 decoder 部分统一预测（更改 attention mask）。不同于 UNILM（mask 并预测末端）和 MASS（mask 随机片段，在 decoder 预测）。</p></li></ul><p><strong>VILBERT</strong></p><ul><li>双流多模态模型；FASTER-RCNN 做图像编码；提出 <strong>co-attention</strong> ：图像与文字各自经过编码后，使用不同的 transformer decoder 模块处理，图像/文字的输出作为文字/图像的 KV；预训练时预测部分遮掩的图片与文字，并判断图文是否匹配。</li></ul><p><strong>VisualBERT</strong></p><ul><li>单流多模态模型；图像 embedding 与文字 embedding 使用同一个 transformer 处理；同 VILBERT 采用图文 MLM 及二分类的图文匹配判断。</li></ul><p><strong>UNIMO</strong></p><ul><li><strong>多模态对比学习</strong> ：文字样本通过重写实现：句子级别重写，使用 back-translation（翻译成别的语言再翻译回来） 生成正样本，使用 tf-idf 相似句子作为负样本。单词级别重写，随机替换其中部分单词；图片正样本使用其本身，负样本使用图像相似度高的；对比学习的样本对为图文对（图片，文字），并未考虑单独的图片或文字对形式。</li><li><strong>预训练任务</strong> ：Span MASK（SpanBERT）双向预测掩码；span-by-span generation（同 ERNIE-GEN）单向预测掩码；图像遮掩后重建，损失为原图像表征与重建表征的欧式距离；</li></ul><p><strong>XLM</strong></p><ul><li>多语言模型；提出 TLM：使用多语言平行语料组成 <code>[A 语言][SEP][B 语言]</code>。的形式，同时对 A/B 中的单词做随机掩码。模型需要预测出掩码内容。A 、B 的位置掩码相同</li></ul><p><strong>XLM-R</strong></p><ul><li>XLM-RoBERTa；预训练任务为多语言的 MLM</li></ul><p><strong>mBART</strong></p><ul><li>多语言版，相对于 BART，encoder/decoder 添加了 <code>final_layernorm</code> 层</li><li>输入（encoder 结束与 decoder 开头）添加 <strong>特殊符号</strong> ，标记目前使用语言；预训练仍然采用 A 语言输入 A 语言输出；微调时通过 <strong>特殊符号</strong> 进行多语言任务微调；</li></ul><p><strong>Unicoder</strong></p><ul><li>提出了三个需要平行语言输入的预训练任务： <strong>CLWR</strong> （Cross-lingual Word Recovery）使用 B 语言的句子尝试构造 A 语言，使用了传统的注意力机制，而非 MHA； <strong>CLPC</strong> （Cross-lingual Paraphrase Classification）：二分类，判断中英句子对是否匹配； <strong>CLMLM</strong> （Cross-lingual Masked Language Model）：采用中英夹杂的句子进行 MLM（从实验上开起来没效果）</li></ul><h2 id="论文" tabindex="-1"><a class="header-anchor" href="#论文" aria-hidden="true">#</a> 论文</h2><p>[1] <a href="http://arxiv.org/abs/1611.01734" target="_blank" rel="noopener noreferrer">Deep Biaffine Attention for Neural Dependency Parsing<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [2] <a href="http://arxiv.org/abs/2002.04745" target="_blank" rel="noopener noreferrer">On Layer Normalization in the Transformer Architecture<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [3] <a href="http://arxiv.org/abs/2107.00440" target="_blank" rel="noopener noreferrer">CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [4] <a href="http://arxiv.org/abs/1909.00964" target="_blank" rel="noopener noreferrer">Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [5] <a href="http://arxiv.org/abs/1911.02116" target="_blank" rel="noopener noreferrer">Unsupervised Cross-lingual Representation Learning at Scale<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [6] UNITER: UNiversal Image-TExt Representation Learning [7] <a href="http://arxiv.org/abs/1908.03557" target="_blank" rel="noopener noreferrer">VisualBERT: A Simple and Performant Baseline for Vision and Language<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [8] <a href="http://arxiv.org/abs/1908.07490" target="_blank" rel="noopener noreferrer">LXMERT: Learning Cross-Modality Encoder Representations from Transformers<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [9] <a href="http://arxiv.org/abs/1908.02265" target="_blank" rel="noopener noreferrer">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [10] <a href="http://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer">Language Models are Few-Shot Learners<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [11] <a href="http://arxiv.org/abs/1909.00204" target="_blank" rel="noopener noreferrer">NEZHA: Neural Contextualized Representation for Chinese Language Understanding<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [12] ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS [13] <a href="http://arxiv.org/abs/1905.02450" target="_blank" rel="noopener noreferrer">MASS: Masked Sequence to Sequence Pre-training for Language Generation<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [14] <a href="http://arxiv.org/abs/2004.09297" target="_blank" rel="noopener noreferrer">MPNet: Masked and Permuted Pre-training for Language Understanding<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [15] <a href="http://arxiv.org/abs/1907.12412" target="_blank" rel="noopener noreferrer">ERNIE 2.0: A Continual Pre-training Framework for Language Understanding<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [16] <a href="http://arxiv.org/abs/1907.11692" target="_blank" rel="noopener noreferrer">RoBERTa: A Robustly Optimized BERT Pretraining Approach<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [17] <a href="http://arxiv.org/abs/1907.10529" target="_blank" rel="noopener noreferrer">SpanBERT: Improving Pre-training by Representing and Predicting Spans<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [18] <a href="http://arxiv.org/abs/1905.07129" target="_blank" rel="noopener noreferrer">ERNIE: Enhanced Language Representation with Informative Entities<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [19] <a href="http://arxiv.org/abs/2106.07139" target="_blank" rel="noopener noreferrer">Pre-Trained Models: Past, Present and Future<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [20] <a href="http://arxiv.org/abs/2001.11314" target="_blank" rel="noopener noreferrer">ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [21] <a href="http://arxiv.org/abs/2107.02137" target="_blank" rel="noopener noreferrer">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [22] <a href="http://arxiv.org/abs/1802.05365" target="_blank" rel="noopener noreferrer">Deep contextualized word representations<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [23] <a href="http://arxiv.org/abs/1901.07291" target="_blank" rel="noopener noreferrer">Cross-lingual Language Model Pretraining<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [24] <a href="http://arxiv.org/abs/1909.11942" target="_blank" rel="noopener noreferrer">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [25] <a href="https://www.aclweb.org/anthology/2020.acl-main.703" target="_blank" rel="noopener noreferrer">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [26] <a href="http://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [27] <a href="http://arxiv.org/abs/1910.10683" target="_blank" rel="noopener noreferrer">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [28] <a href="http://arxiv.org/abs/1906.08237" target="_blank" rel="noopener noreferrer">XLNet: Generalized Autoregressive Pretraining for Language Understanding<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [29] Improving Language Understanding by Generative Pre-Training [30] Language Models are Unsupervised Multitask Learners [31] <a href="http://arxiv.org/abs/2001.08210" target="_blank" rel="noopener noreferrer">Multilingual Denoising Pre-training for Neural Machine Translation<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [32] <a href="http://arxiv.org/abs/1905.03197" target="_blank" rel="noopener noreferrer">Unified Language Model Pre-training for Natural Language Understanding and Generation<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> [33] <a href="http://arxiv.org/abs/2012.15409" target="_blank" rel="noopener noreferrer">UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></div><!----><footer class="page-meta"><!----><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 417333277@qq.com">kevinng77</span><!--]--><!--]--></div></div></footer><!----><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer"><a href="https://beian.miit.gov.cn/" target="_blank">沪ICP备2023027904号-1</a>&nbsp;<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=" target="_blank"><img src="/assets/gongan.png">公安备案号 </a></div><div class="vp-copyright">Copyright © 2025 Kevin 吴嘉文</div></footer></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-82c5f5e3.js" defer></script>
  </body>
</html>
