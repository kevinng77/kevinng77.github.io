---
title: AI 基础回忆
date: 2020-06-17
author: Kevin 吴嘉文
categories:
- Notes|理论梳理
tags:
- 随笔
- Machine Learning
mathjax: true
toc: true
comments: 基础加密算法分析
---

# Machine Learning 随笔

>  算法和机械学系，他们的关系似乎正在逐渐的变远。在我印象中，当一个数据计入计算机后，一个算法工程师应该对计算机每个时刻的状态，对中间步骤的输入和输出都有致的了解。而今的AI课程，为了避免学生对AI的学习产生恐惧和厌恶，大量的数学推导被省略。SKlearn，tensorflow 或者pytorch种的library，也为我们的用户精心定制，不需要再去自己编写哪些枯燥且复杂的算法过程，如Back propagation。部分AI的初学者在使用这些算法时，难免会对这个黑盒子感到畏惧与陌生。

<!--more-->

## 前言

顶尖的数学家们为算法的应用做出了巨大的贡献，他们做了各种枯燥的证明，或发明了更优的解。而工程师们便是站在这些伟人的肩膀上，数学家毋庸置疑需要有非凡的数学能力，工程师们却不同，个人认为对于算法工程师，对数学的态度应该是**尽力**与**恰当**。

### 尽力

霍金在编写时间简史时，编著对他说，你的书里没多一个公式，你就会失去1百万个读者。（好像是一百万把还是多少不大记得了）群众似乎对数学或者公式有着天生的畏惧。在各种课程，或者论坛中，随处可见

如，在Andrew 的 Deep Learning.AI 课程中，Backpropagation被设为了 optional项目：

![image-20210207111458495](/img/ML思考/image-20210207111458495.png)



在 《Knuth D. - The art of computer programming.》中 作者特意标注了与数学有关的张洁与内容，并告知读者如果对数学没有兴趣，可以选择越过这些部分，但是推荐读者对数学推论的方法和最终结果要熟悉。



![image-20210207111653640](/img/ML思考/image-20210207111653640.png)

因此尽力去克服对数学的恐惧，也是本文推荐的。数学可能没那么你想的那么可怕，就似纸老虎。

### 恰当

数学的推算是相当花费时间的，工程师对数学的了解程度也应该是边际效用递减的。从了解算法的大致思路，到了解算法用到的每一个定理，公理遍需要花费大量的实践。而后再花上一两周去对定理进行证明是没有必要的了。**当然对于算法的数学证明了解程度取决于你对算法的需求**，若是在你的工作项目中，你需要确保整个工程的精度，你可能会考虑去熟悉掌握算法背后的每一条数学逻辑。但对于而今的初学者们则没有那么必要去这么做了。



## ML 基础

### ML的定义

59年，ML被定义为让计算机在不需要特殊编程的情况下进行针对特定任务的学习。到了98年，更具体地定义出来了。

“A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.”

有点类似培养一个孩子，你给他书，给他教材，或者给他讲故事。对于ML这些便是E，即data set。但你可能不会很清楚孩子是怎么把书和教材编程知识的。



## 机械 为什么能够学习？并达到我们想要的效果

阅读完此章你应该能够在脑海中清晰地知道Machine Learning的优化思路，当一个训练集被放入到一个machine learning模型中开始训练后，你的计算机将会经过哪些状态（此过程中都有哪些数据产生，计算机会分别调用哪些函数来对数据进行处理）？模型中的哪些参数将会变动？是什么驱使着他们朝着我们想要的方向变动？而当训练终止后，他们会处于什么状态。

### Objective function

为了验证是否达成目标，我们必须用某种方法来衡量，而这个被量化用于衡量的指标就是 objective。用来计算该指标的方程也就是objective function。

对于不同的目标我们有不同的objective function。如训练分类问题，我们的目标是MLE，即最小化Inverse log likelyhood，也称为CROSS ENTROPY。对于Linear regression，最小化的目标就是MSE。

于是我们只要找到一些方法，能够使得我们的目标函数最小化。那么我们就能够得到一个我们想要的模型，对吧。

那么什么方法可以找到这个目标函数的最小值呢？最常见的优化方案便是Gradient descent，请耐心往下看哈

### 导数的方向

**导数总是指向函数增长的方向**，举个例子 $y = x^2 + 2x + 1$， 在点$(1,4)$ 处，它的导数为 $y' = 2x + 2= 4$， 如果我们把 $x $ 加上导数的值 4，那么 $x$ 变成了 5，$y$ 的值就增加到了 36

### 用导数来求凹函数的最小值

有了这一特性，那么我们就可以进行骚操作了啊哈哈哈！

既然加上导数的值会会使函数值增加，那我减掉导数值不就可以缩小 $y$ 了嘛。那我们来试试，把 上例中的 $x = 1$ 减去 4， 得到了 -3。当 $x = -3$ 时，$y$ 的值变成了 4。

![image-20210207164556233](/img/ML思考/image-20210207164556233.png)

出来的结果和理论不一致呢，哪里出错了呢？

<img src="/img/ML思考/image-20210207131429291.png" alt="image-20210207131429291" style="zoom: 67%;" />

*(图中紫色的箭头也就是导数的方向了哈)*

### Gradient Descent

Gradient Descent 便是通过减去导数这种方法来求出函数的最小值的，但是我们需要在上面这种方法上进行几点补充来使得它能够求出最小值：

#### Learning Rate

我们继续上一小节的例子，根据下图不难看出，当我们减去 4 的时候，我们的 $x$ 已经越过了最优点，并往上爬了一段距离，说明这一步下坡我们走太大了。因此我们只要减小这一步的大小，我们就可以达到想要的效果啦。

<img src="/img/ML思考/image-20210207200317127.png" alt="image-20210207200317127" style="zoom:80%;" />

*（图：自己画的。版权所有）*

Learning rate 的设计便是为了解决这个问题。我们这次减去导数值与learning rate的乘积。假设 learning rate 我们取 0.1。那么结果是这样的：

+ 原 $x = 1, y = 4$。导数为 4
+ 减去 $learning rate \times 导数 = 0.4$，$x$ 变成 0.6。此时 $y$ 减小到了2.56

这样我们的 $y$ 就减小了，真是神奇呢。

#### 套娃来实现Gradient Descent

聪明的伙伴可能已经察觉到了，我们将 $x$ 减去 $learning  rate \times 导数$ 之后，将会得到一个新的 $x$。此时我们只要将新的到的 $x$ 再次进行同样的操作，就可以又得到一个新的更好的 $x$ 了！

![image-20210207170009279](/img/ML思考/image-20210207170009279.png)

如此循环我们就能够让 $x$ 像球一样滚到了函数的底部。这就是优化的大致思路，不过事实是我们有时候很难能够准确地求出最小值的大小，大部分情况我们得到的将会是一个近似值，或者说一种相对优化的解。小球可能会滚太慢，半天都走不到一小段；有些时候，小球不会混到最深的山谷中，而是会被卡在一些高处的小坑里面，（也就是我们常说的local optimal问题）；还有些时候learning rate太大，让小球不断地在最优点周围徘徊。这些问题都是曾经被众多地学者们讨论过的，针对他们做出的优化方案层出不穷，讨论他们的文章看的我头发光光。于是有了random seed，mini batch training，Adam，scaling 等大众所熟悉地操作来优化这套算法。



## 基础ML模型

### Linear Regression with multiple variables

#### 为什么用矩阵

+ 矩阵的特性能够代替for循环，大大提高训练的效率

其实不用矩阵，我们也能够实现ML算法，只需要使用多个for loop就行了。我们来做一个简单的例子。考虑下面的训练集：

```
x = [[2,3,4],[3,4,5],[4,5,6],[2,3,4]]
y = [2,4,5,6]
w = [1,-1,1]
```

那么我们求出当 `W = [1,-1,1]` 时候的 MSE很简单，使用几个for循环

```python
sum_yp = 0
for i in range(len(x)):
    temp = 0
    for j in range(len(x[i])):
        temp += (x[i][j] * w[j])
    sum_yp += (temp - y[i])**2
sum_yp /len(x)              
```

时间复杂度妥妥的 $O(N^3)$ 

但是用上矩阵就不一样了，两个矩阵的乘法就相当于我们上面的操作，他们能够求出同样的结果。而矩阵计算将时间复杂度大大减小，提高了计算的效率。

```python
def get_J(x,y,w):
    sum_yp2 = np.sum((np.dot(x,w.T) - y)**2)
    return sum_yp2/x.shape[0]
```

矩阵之所以能有更快的计算效率，一是归功于现在计算机的计算模式，相比于for循环一个一个遍历，一次只能处理一个$wx$，矩阵可以实现一次性对于一整个行的矩阵相乘，也就是一次解决 $\sum wx$。部分GPU厂商还会对此设计专门用来训练的GPU比如NVIDIA，使得计算机同一时间能够处理的数据数量大大提高。硬件方面外，尽管部分特殊的矩阵能够通过不同的算法来实现更好的时间复杂度，如Strassen' s Method。但这些技术有没有被用于当今的ML并不太清楚。

+ ML中矩阵是什么意思？

一般来说 x_train 矩阵的每一个行，都是一个sample。他的每一个列都是一个对应的feature。

## 把所有总结起来

那么现在让我们还原一道题目，以此来梳理我们的知识。

给定  $y = x_1w_1 + x_2w_2 + x_3w_3 + b$

```python
x = [[2,3,4],[3,4,5],[4,5,6]]
y = [2,4,5]
```

我们将采用gradient descent 求出最优的 $W$ 的解。

+ 首先我们随机地选择 $W$，作为我们地出发点。假设我们选了下面这样的 $W$。

```
W = [1,-1,1]
```

+ 第二我们通过减去 $W$ 导数的方法，来求出 MSE 的最小值。这边你可以把 $x$ 看成常数 ，因为 $x$ 训练集是不会变动的，整个模型训练的过程中，变动的只有这个 $W$。

第一步，计算我们的objective function：
$$
MSE =J= \frac{1}{2 m} \sum_{i=1}^{m}\left(h_{w}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
$$
第二部，计算导数，$W$ 的偏导就是：
$$
\frac {\partial W}{\partial J}=\frac{1}{m} \sum_{i=1}^{m}\left(h_{w}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
$$
然后我们让 $w$ 每次更新,这边的$\alpha$就是learning rate：
$$
w_{j}:=w_{j}-\alpha\left[\frac{1}{m} \sum_{i=1}^{m}\left(h_{w}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}\right]
$$
有了上面的这些想法，我们就可以写成函数啦，只要通过for 循环迭代多次，我们就可以得到优化后的w的值啦：如下

```python
x = np.array([[1,2,3],[3,4,5],[4,5,6],[2,4,6],[8,9,10]])
y = np.array([[4],[10],[13],[7],[25]])
w = np.array([[3.0,1.5,-1]])
def get_w(x,y,w,alpha,iter_num):
    m = x.shape[0]
    x = np.concatenate((np.ones(x.shape[0]).reshape(-1,1),x),axis = 1)
    w = np.concatenate((np.ones(w.shape[0]).reshape(-1,1),w),axis = 1)
    l_mse = []
    for _ in range(iter_num):
        MSE = get_J(x,y,w)
        l_mse.append(MSE.round(2))
        w -= alpha * (np.sum((np.dot(x,w.T) - y)*x,axis= 0))/m
    return w,l_mse
```

这个函数返回的w就是我们想要的系数，l_mse 是 计算过程中mse的记录值。

```python
answer,l_mse = get_w(x,y,w,0.0001,5000)
answer
```

上面这些步骤就是一个Machine learning模型，运行上面这个函数，你就相当于运行了。

```
from sklearn import LinearRegression
m = linear_model.LinearRegression()
m.fit(x,y)
```

**不过需要提醒的是！！！！** sklearn 库中的model 能够实现比上面函数更好的效果，上面函数不论是对常数项 $b$ 的处理，$w$ 初始值的处理等都做的非常粗糙，函数简陋，意在使读者能够对ML有一个大致的清晰的理解。

让我们来看看结果，打印 `np.dot(x,w.T)` 来输出我们的预测结果。能看到我们的预测结果和我们的训练集 `y`很相近了。

```
array([[ 4.05543213],
       [10.03547761],
       [13.02550035],
       [ 6.93755124],
       [24.98559132]])
```

再让我们看看训练过程中，我们的MSE都发生了什么

<img src="/img/ML思考/image-20210207203027819.png" alt="image-20210207203027819" style="zoom:80%;" />

如我们所料地，因为我们的 $w$ 一次又一次的变好，那么它对应的导数的方向也会变得越来越平坦，使得我们优化的脚步逐渐慢下来，最后接近0。

## 从一到无穷大

就像很多数学公理那样，当你证明了公理在2维空间成立的时候，很多时候它也能够在3，4等等更高维度上成立。

上一章针对multi linear regression的优化方案进行了介绍，它用了gradient descent来找到了最优解。**实际上，还有其他各种寻找最优解的方案。** 有了gradient descent 我们就能够把我们的数据喂给模型，让模型自己去学习，来告诉我们一个相对好的解决方案。

在上一章中，我们经历了

+ 选择 objective function
+ 建立模型   $y = x_1w_1 + x_2w_2 + x_3w_3 + b$
+ 选择一个优化方案，如果是gradient descent，我们就计算偏导
+ 调整 hyper parameters，我们这边只有 learning rate $\alpha$

最后我们找到了一个想要的解。

针对一些AI的其他模型，似乎我们也能采用同样的步骤。对于图像识别最基础的CNN，我们可以

+ 更换objective function， 如cross entropy。
+ 更换模型为CNN 框架
+ 根据CNN框架计算每一个Node的w的偏导
+ 调整 hyper parameters 使得模型能够达到最好的训练效果

最后我们一样的把 x_train 和 y_train 放入到模型中让模型自己学习，我们就可以得到一个较为优化的结果了。

除了CNN，RNN也是如此。因此也希望某些同学在面对完全陌生的AI时，喝口奶茶冷静一下。也许你并没有那么怕它。

![image-20210207204453557](/img/ML思考/image-20210207204453557.png)

*（张同学爱喝奶茶的青蛙）*

### 再问 为啥要了解AI中的数学

AI中的数学大部分都是偏导，矩阵

偏导我认为是了解并熟悉一个AI模型的最好方法。

对于复杂的AI模型，想要明白模型中的那些变量有着互相影响，理解模型的运作画面。一个好的方法便是通过模型结构，自己写出各个参数的偏导，当然**尽力**最重要。若是过了一两个小时还解不出来，那也是可以参考一下答案的。只要是思考过了，不论结果是否正确，你都将会对某个事件有着深刻的理解。[这里](http://wujiawen.xyz/2021/01/12/DLnote1/) 有着CNN，RNN一些基础模型的数学整理，包括曾经风云NLP的LSTM的偏导等。

